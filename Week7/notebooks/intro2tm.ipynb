{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Text Mining\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore more advanced machine learning\n",
    "techniques with text data. First, we introduce the concept of n-grams,\n",
    "which are combinations of one or more tokens. For example, bigrams are\n",
    "combinations of two tokens, while trigrams are combinations of three\n",
    "tokens. Next, we introduce the concept of stemming, which is used to\n",
    "convert tokens into their root forms so that token frequencies match the\n",
    "use of the root token rather than being spread across multiple similar\n",
    "tokens. Finally, we explore the application of clustering and feature\n",
    "selection to text data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "Formally, a [_n-gram_][ngd] is a contiguous sequence of **n** items from a\n",
    "parent sequence of items, such as characters or words in a text\n",
    "document. In general, we will focus solely on words in a document. Thus,\n",
    "our initial approach has simply been to look at unigrams or single\n",
    "words in a document when building a classification model. However,\n",
    "sometimes the combination of words can be more descriptive, for example,\n",
    "_unbelievably bad_ is generally viewed as a more powerful description\n",
    "than just _bad_. As a result, the concept of an _n-gram_ was created,\n",
    "where collections of words can be treated as features. In fact google\n",
    "allows a user to search for [specific n-gram][gnv] combinations in books that\n",
    "they have scanned.\n",
    "\n",
    "While this clearly can improve classification power, it also increases\n",
    "computational requirements. This is a result of the exponential rise in\n",
    "the number of possible features. For example, given $n$ words, we have\n",
    "$n \\times (n - 1)$ possible bigrams, and so on for higher order\n",
    "combinations. While this is not a problem for small vocabularies, for\n",
    "larger vocabularies (and corresponding documents) the number of possible\n",
    "features can quickly become very large. Thus, many text mining\n",
    "applications will make use of Hadoop or Spark clusters to leverage the\n",
    "inherent parallelism in these tasks.\n",
    "\n",
    "To demonstrate using n-grams, we first demonstrate the concept on a\n",
    "single sentence.\n",
    "\n",
    "-----\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[ngd]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'info490', 'introduces', 'many', 'concepts', 'in', 'data', 'science',\n",
      "  'info490 introduces', 'introduces many', 'many concepts', 'concepts in',\n",
      "  'in data', 'data science']\n"
     ]
    }
   ],
   "source": [
    "my_text = 'INFO490 introduces many concepts in data science.'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "pp.pprint(tk_func(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapping:\n",
      "----------------------------------------\n",
      "0 concepts\n",
      "1 concepts in\n",
      "2 data\n",
      "3 data science\n",
      "4 in\n",
      "5 in data\n",
      "6 info490\n",
      "7 info490 introduces\n",
      "8 introduces\n",
      "9 introduces many\n",
      "10 many\n",
      "11 many concepts\n",
      "12 science\n",
      "----------------------------------------\n",
      "['INFO490 is data science']\n",
      "----------------------------------------\n",
      "[[0 0 1 1 0 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "in_list = []\n",
    "in_list.append(my_text)\n",
    "\n",
    "cv = cv.fit(in_list)\n",
    "\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print('Token mapping:')\n",
    "print(40*'-')\n",
    "\n",
    "for tokens, rank in my_voc:\n",
    "    print(rank, tokens)\n",
    "\n",
    "print(40*'-')\n",
    "out_list = ['INFO490 is data science']\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used `CountVectorizer` to create n-gram\n",
    "tokens. Now that you have run the Notebook, go back and make the\n",
    "following changes to see how the results change.\n",
    "\n",
    "1. Change the `CountVectorizer` to use stop words and change the tokens\n",
    "to all lowercase. How does this change the tokens and mappings? \n",
    "2. Create your own sentence, do the tokens and n-grams make sense?\n",
    "3. Try changing the ngram range to different values (e.g.,\n",
    "`ngram_range=(1,4)` or `ngram_range=(2,3)`). Notice how the number of\n",
    "tokens quickly increase.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## N-gram Classification\n",
    "\n",
    "Having n-grams offers improved classification, since word or token\n",
    "combinations often include more information than single words or tokens.\n",
    "For example, _University Illinois_ means more than just _University_ and\n",
    "_Illinois_. We can build on our previous simple text classification\n",
    "pipeline to now develop a more complete code example that builds a\n",
    "feature vector containing both single words and b-grams from the\n",
    "documents. We use this new sparse matrix to classify the documents by\n",
    "using our simple Naive Bayes classifier, which obtains slightly better\n",
    "results. First, we load the movie review data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 2000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = '/home/data_scientist/data/nltk_data/corpora/movie_reviews'\n",
    "\n",
    "\n",
    "#data_dir = '/home/data_scientist/nltk_data/corpora/movie_reviews'\n",
    "mvr = load_files(data_dir, shuffle = False)\n",
    "print('Number of Reviews: {0}'.format(len(mvr.data)))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(\n",
    "    mvr.data, mvr.target, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.71      0.77       259\n",
      "        pos       0.73      0.85      0.79       241\n",
      "\n",
      "avg / total       0.79      0.78      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase and restrict ourselves to about half the available features\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 421010\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now we can repeat the results, but now use unigrams, bigrams, and\n",
    "trigrams. Since this will produce a document term matrix that likely\n",
    "exceeds the computational resources of our Docker containers, we impose\n",
    "two cuts on the tokens included in our DTM. First, we impose a minimum\n",
    "feature term that requires a term to be present in at least two\n",
    "documents. Second, we set a maximum frequency of 50%, such that any term\n",
    "occurring in more than fifty percent of all documents will be ignored\n",
    "(these are likely all stop words, but this value can be reduced to a\n",
    "lower value to trim frequently occurring words that add little to the\n",
    "accuracy. Notice that our overall number of features is much smaller,\n",
    "even though we are also using trigrams in this classification pipeline.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.78      0.81       259\n",
      "        pos       0.78      0.83      0.81       241\n",
      "\n",
      "avg / total       0.81      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__min_df=2, \\\n",
    "                cv__max_df=0.5)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 62735\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we included n-grams in the classification\n",
    "process by using a `CountVectorizer`.  Now that you have run the\n",
    "Notebook, go back and make the following changes to see how the results\n",
    "change.\n",
    "\n",
    "1. Try to determine a value of `max_df` that replicates the effects of\n",
    "using stop words. \n",
    "2. Change `CountVectorizer` to `TfidfVectorizer`. Do the results change?\n",
    "3. Try using a more powerful classification algorithm, as opposed to\n",
    "`MultinomialNB`. Do the results change?\n",
    "\n",
    "-----\n",
    "\n",
    "## Stemming\n",
    "\n",
    "So far, we have looked at several techniques to remove redundant or\n",
    "unimportant features. For example, we changed the case of all text to\n",
    "lowercase and we have applied stop words. However, there still is the\n",
    "issue of different forms of the same word, for example compute,\n",
    "computer, computed, and computing. The process of changing words back to\n",
    "their root, or basic form (by removing prefixes and suffixes) is known as\n",
    "stemming. \n",
    "\n",
    "The most widely used stemmer, or program/method that performs stemming,\n",
    "is the _Porter Stemmer_, which was originally published in 1980 by\n",
    "Martin Porter. An improved version was released in 2000, which fixed a\n",
    "number of errors. NLTK includes the Porter Stemmer, which can be used\n",
    "with scikit learn by creating a special function that tokenizes text\n",
    "documents and passing this function as an argument to the\n",
    "`CountVectorizer` via the `tokenizer` attribute. By performing stemming\n",
    "inside this tokenize method, we can return a set of tokens for a\n",
    "document that have been stemmed. In the following code cell, we use a\n",
    "custom `tokenize` method that first builds a list of tokens by using\n",
    "nltk, and then maps the Porter stemmer to the list of tokens to generate\n",
    "a stemmed list.\n",
    "\n",
    "\n",
    "-----\n",
    "[ws]: https://en.wikipedia.org/wiki/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.77      0.80       259\n",
      "        pos       0.77      0.83      0.80       241\n",
      "\n",
      "avg / total       0.80      0.80      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__tokenizer=tokenize)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 80529\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we incorporated the Porter Stemmer into the\n",
    "classification pipeline. Now that you have run the Notebook, go back and\n",
    "make the following changes to see how the results change.\n",
    "\n",
    "1. Did the Porter Stemmer improve the classification results (note you\n",
    "need to be sure you are comparing exactly the same pipeline (including\n",
    "the use of ngrams)? \n",
    "\n",
    "2. Can you compute the number of features that the Porter Stemmer\n",
    "generates? How does this compare the number of features without stemming?\n",
    "\n",
    "3. Try using a different stemming algorithm, such as [_snowball_][nsw].\n",
    "How do the classification results change? \n",
    "\n",
    "-----\n",
    "[nsw]: http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball\n",
    "\n",
    "## Clustering Analysis\n",
    "\n",
    "We can also apply clustering analysis to our feature matrix. While\n",
    "finding an unknown number of clusters in text documents can be\n",
    "difficult, we can learn about our data by identifying the clusters for\n",
    "our **known** labels. To demonstrate, in the following code cells, we\n",
    "employ k-means to find two clusters in our feature matrix (the moview reviews), after\n",
    "which we identify the most frequently used words in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=2, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 2\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', \\\n",
    "                     ngram_range=(1, 3), max_features=100000)\n",
    "\n",
    "train_counts = cv.fit_transform(mvr_train)\n",
    "test_data = cv.transform(mvr_test)\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: reese beau margaret mom lake deep end visnjic end body turn deep swinton deeper having story apparent early won gets year\n",
      "\n",
      "Cluster 1: film movie like just time good story character way characters make does really plot scene life people man little bad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = ['Neg', 'Pos']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can perform the same analysis on a more complex problem by analyzing\n",
    "the twenty newsgroup data set. First we load the data, after which we\n",
    "apply k-means and identify the most common tokens in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='train', shuffle=True, random_state=23)\n",
    "test = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='test', shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=20, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 20\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', max_features=100000)\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: class virtual object const edu rpi cs return colossus defined science wise os computer void ny rensselaer functions polytechnic troy\n",
      "\n",
      "Cluster 1: god people don like just know edu does think say way time said new right jesus did believe good armenian\n",
      "\n",
      "Cluster 2: dos windows microsoft tcp ms mouse amiga software pc graphics higher macintosh network mbytes version 00 ip memory support card\n",
      "\n",
      "Cluster 3: com subject writes lines edu organization article don like just people know think posting host time nntp does good use\n",
      "\n",
      "Cluster 4: jpeg image gif file color format images quality version files bit free programs available use jfif software don display edu\n",
      "\n",
      "Cluster 5: 00 25 50 10 20 adl 40 edu cancer information 1st new 11 use art gopher 15 server wolverine hiv\n",
      "\n",
      "Cluster 6: openwindows use sun xview usr look x11 lib open subject file openwinhome window olit news programs openwin manual fonts bin\n",
      "\n",
      "Cluster 7: jehovah elohim lord god christ father mcconkie unto son ps jesus said gods shall thou thee mormon thy earth stated\n",
      "\n",
      "Cluster 8: ftp free xview edu contact subject openwindows look notes com open commercial requirements applications description mit porter contrib export lcs\n",
      "\n",
      "Cluster 9: myers ms president think don dee ll know said going decision does house white today believe justice just board department\n",
      "\n",
      "Cluster 10: rx remote echo sh pwd shell shar display man host pl file fi command hostname sinclair test set makefile make\n",
      "\n",
      "Cluster 11: mb m4 ms ma mz mm mo m1 mc mu mw mh mf mp mj mt mx mk mr mn\n",
      "\n",
      "Cluster 12: 92 12 10 hiv 17 11 aids patients et 03 30 medical 25 milk cd4 31 tb 1993 number 04\n",
      "\n",
      "Cluster 13: planet earth spacecraft solar surface sun venus moon atmosphere planets jupiter kilometers miles space years moons degrees mars saturn mariner\n",
      "\n",
      "Cluster 14: mac files comp disk file software sys ftp macintosh faq questions stuffit hard available need apple use question edu application\n",
      "\n",
      "Cluster 15: edu subject lines organization university writes article posting host nntp just like don know cs com think ca does people\n",
      "\n",
      "Cluster 16: edu image graphics data pub ftp available mail software 128 images package send format 3d ray file files processing com\n",
      "\n",
      "Cluster 17: stephanopoulos mr president general did think know attorney just don going decision george said statement yesterday house white responsibility mean\n",
      "\n",
      "Cluster 18: ed israel people jews years arab jewish let palestinians palestine arabs country quote today begin gurion israeli ben new quoted\n",
      "\n",
      "Cluster 19: said people president think know going don door went started children didn armenians ll time say just came balcony ira\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = test['target']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used k-means clustering to find clusters in\n",
    "text data, and to identify the most important tokens in these clusters.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the vectorizer to use TF-IDF. Does this change the tokens in\n",
    "each cluster? \n",
    "2. Change the vectorizer to use ngrams. Does this change the tokens in\n",
    "each cluster?\n",
    "3. Include stemming in the vectorizer. Does this change the tokens in\n",
    "each cluster?\n",
    "4. Try using DBSCAN instead of k-means. How many clusters are found?\n",
    "What are the tokens in each cluster?\n",
    "\n",
    "Finally, the k-means algorithm finds clusters. Do these clusters map\n",
    "directly into the _real_ categories? Feel free to discuss this on the\n",
    "course forums.\n",
    "\n",
    "-----\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "The document term matrices we have constructed in these examples can\n",
    "become quite large. We have already reduced the number of features used\n",
    "in classification problems by using stop words, by using a consistent\n",
    "case, and by performing stemming. On the other hand, we have enabled\n",
    "exponential increases in the feature space by including n-grams\n",
    "(although we once again restricted the feature space by using the\n",
    "`max_features` or the `max_df` and `min_df` attributes). The\n",
    "traditional dimensional reduction technique we have used in the past is\n",
    "PCA. However, PCA can be difficult to use with text data given the large\n",
    "sizes of the matrices (since a matrix inversion can be required). We can\n",
    "employ alternative techniques, such as incremental PCA or Truncated SVD. \n",
    "\n",
    "But in reality, we are less interested in finding a reduced dimensional\n",
    "space than we are in removing features that contain little or no\n",
    "information (combining features is essentially increasing the ngram\n",
    "range). In this case, the problem of dimension reduction becomes one of\n",
    "optimal feature selection. For this, we can use the scikit learn\n",
    "`SelectKBest` method to identify the best $k$ features. In the following\n",
    "code cells, we first create a vectorizer, train it on data to\n",
    "demonstrate the accuracy and the number of features required to achieve\n",
    "that accuracy. After which, we employ `SelectKBest` to identify the best\n",
    "number of features, where number is predetermined, to achieve a similar\n",
    "accuracy. In the end, we find that less than ten percent of the original\n",
    "features are required to achieve the same level of accuracy.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Following Example was insipred by scikit learn demo\n",
    "# http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy =  82.0%\n",
      "Number of Features = 129792\n"
     ]
    }
   ],
   "source": [
    "# First, train on normal set of features, baseline performance.\n",
    "\n",
    "train_counts = tf.fit_transform(train['data'])\n",
    "test_data = tf.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_counts, train['target'])\n",
    "predicted = nb.predict(test_data)\n",
    "\n",
    "print(\"Prediction accuracy = {0:5.1f}%\".format(100.0 * nb.score(test_data, test['target'])))\n",
    "print('Number of Features = {}'.format(nb.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now employ feature selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Number of features to keep\n",
    "num_k = 10000\n",
    "\n",
    "# Employ select k best wiht chi2 since this works with sparse matrices.\n",
    "ch2 = SelectKBest(chi2, k=num_k)\n",
    "xtr = ch2.fit_transform(train_counts, train['target'])\n",
    "xt = ch2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  82.0%\n",
      "Number of Features = 10000\n"
     ]
    }
   ],
   "source": [
    "# Train simple model and compute accuracy\n",
    "nb = nb.fit(xtr, train['target'])\n",
    "predicted = nb.predict(xt)\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * nb.score(xt, test['target'])))\n",
    "print('Number of Features = {}'.format(nb.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can use the feature selection to identify the top features for each\n",
    "category. First we extract the feature names, after which we extract the\n",
    "importance (or support) for each feature. By sorting the array containing\n",
    "the important features, we can identify the top tokens.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "indices = ch2.get_support(indices=True)\n",
    "feature_names = np.array([feature_names[idx] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'god', 'caltech', 'atheists', 'livesey']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'image', 'thanks', '3d', 'files']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'dos', 'file', 'files', 'driver']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['drive', 'card', 'scsi', 'ide', 'bus']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'quadra', 'drive', 'centris']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'com', 'server']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'offer', 'shipping', 'distribution', 'condition']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'cars', 'com', 'article', 'engine']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'dod', 'com', 'ride', 'motorcycle']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['baseball', 'year', 'game', 'team', 'players']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'nhl']\n",
      "\n",
      "sci.crypt:\n",
      "['clipper', 'key', 'encryption', 'chip', 'com']\n",
      "\n",
      "sci.electronics:\n",
      "['com', 'use', 'circuit', 'host', 'power']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'geb', 'banks', 'gordon', 'com']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'moon', 'orbit', 'henry']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'rutgers', 'christian']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'guns', 'com', 'people', 'batf']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'arab']\n",
      "\n",
      "talk.politics.misc:\n",
      "['com', 'cramer', 'article', 'people', 'clinton']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'com', 'christian', 'jesus']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "top_count = 5\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_names = np.argsort(nb.coef_[idx])[-top_count:]\n",
    "    tn_lst = [name for name in feature_names[top_names]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print('\\n{0}:'.format(target))\n",
    "    pp.pprint(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used feature selection to identify the most\n",
    "important features in our simple classification pipeline. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change the vectorizer to change the case of all words an to employ\n",
    "stemming. How do the results (tokens) change?\n",
    "\n",
    "2. Change the classification algorithm to a more accurate method. How do\n",
    "the results change? How does the computational time change?\n",
    "\n",
    "Finally, what do the list of tokens say about the fact we did not remove\n",
    "headers or footers from the newsgroup postings? Feel free to comment on\n",
    "these questions in the course forum.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
