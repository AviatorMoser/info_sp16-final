{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Practical Concepts\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore \n",
    "\n",
    "1. Repeat unigram results (read in data/ set things up.)\n",
    "2. stemming\n",
    "3. Stemming results.\n",
    "4. n-grams\n",
    "5. bigram results\n",
    "6. tri gram results.\n",
    "7. results comparison (grid search?).\n",
    "\n",
    "Sentiment analysis. Movie reviews?\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 2000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = '/home/data_scientist/nltk_data/corpora/movie_reviews'\n",
    "mvr = load_files(data_dir, shuffle = False)\n",
    "print('Number of Reviews: {0}'.format(len(mvr.data)))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(\n",
    "    mvr.data, mvr.target, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "Formally, a [_n-gram_][ngd] is a contiguous sequence of **n** items from a\n",
    "parent sequence of items, such as characters or words in a text\n",
    "document. In general, we will focus solely on words in a document. Thus,\n",
    "our initial approach has simply been to look at unigrams or single\n",
    "words in a document when building a classification model. However,\n",
    "sometimes the combination of words can be more descriptive, for example,\n",
    "_unbelievably bad_ is generally viewed as a more powerful description\n",
    "than just _bad_. As a result, the concept of an _n-gram_ was created,\n",
    "where collections of words can be treated as features. In fact google\n",
    "allows a user to search for [specific n-gram][gnv] combinations in books that\n",
    "they have scanned.\n",
    "\n",
    "While this clearly can improve classification power, it also increases\n",
    "computational requirements. This is a result of the exponential rise in\n",
    "the number of possible features. For example, given $n$ words, we have\n",
    "$n \\times (n - 1)$ possible bigrams, and so on for higher order\n",
    "combinations. While this is not a problem for small vocabularies, for\n",
    "larger vocabularies (and corresponding documents) the number of possible\n",
    "features can quickly become very large. Thus, many text mining\n",
    "applications will make use of Hadoop or Spark clusters to leverage the\n",
    "inherent parallelism in these tasks.\n",
    "\n",
    "To demonstrate using n-grams, we first demonstrate the concept on a\n",
    "single sentence.\n",
    "\n",
    "-----\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[ngd]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'info490', 'introduces', 'concepts', 'data', 'science', 'info490 introduces',\n",
      "  'introduces concepts', 'concepts data', 'data science',\n",
      "  'info490 introduces concepts', 'introduces concepts data',\n",
      "  'concepts data science', 'info490 introduces concepts data',\n",
      "  'introduces concepts data science']\n"
     ]
    }
   ],
   "source": [
    "my_text = 'INFO490 introduces many concepts in data science.'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', ngram_range=(1,4), lowercase=True)\n",
    "\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "pp.pprint(tk_func(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapping:\n",
      "----------------------------------------\n",
      "0 concepts\n",
      "1 concepts data\n",
      "2 concepts data science\n",
      "3 data\n",
      "4 data science\n",
      "5 info490\n",
      "6 info490 introduces\n",
      "7 info490 introduces concepts\n",
      "8 info490 introduces concepts data\n",
      "9 introduces\n",
      "10 introduces concepts\n",
      "11 introduces concepts data\n",
      "12 introduces concepts data science\n",
      "13 science\n",
      "----------------------------------------\n",
      "['INFO490 is data science']\n",
      "----------------------------------------\n",
      "[[0 0 0 1 1 1 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "in_list = []\n",
    "in_list.append(my_text)\n",
    "\n",
    "cv = cv.fit(my_list)\n",
    "\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print('Token mapping:')\n",
    "print(40*'-')\n",
    "\n",
    "for tokens, rank in my_voc:\n",
    "    print(rank, tokens)\n",
    "\n",
    "print(40*'-')\n",
    "out_list = ['INFO490 is data science']\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used XXX. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change \n",
    "2. Change \n",
    "3. Try making \n",
    "\n",
    "Finally, try applying \n",
    "\n",
    "-----\n",
    "\n",
    "### N-gram classification\n",
    "\n",
    "Having n-grams offers improved classification, since word or token\n",
    "combinations often include more information than single words or tokens.\n",
    "For example, _University Illinois_ means more than just _University_ and\n",
    "_Illinois_. We can build on our previous simple text classification\n",
    "pipeline to now develop a more complete code example that builds a\n",
    "feature vector containing both single words and b-grams from the\n",
    "documents. We use this new sparse matrix to classify the documents by\n",
    "using our simple Naive Bayes classifier, which obtains slightly better\n",
    "results.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.71      0.77       259\n",
      "        pos       0.73      0.85      0.79       241\n",
      "\n",
      "avg / total       0.79      0.78      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase and restrict ourselves to about half the available features\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 421010\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.78      0.81       259\n",
      "        pos       0.78      0.83      0.81       241\n",
      "\n",
      "avg / total       0.81      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__min_df=2, \\\n",
    "                cv__max_df=0.5)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 62735\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used XXX. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change \n",
    "2. Change \n",
    "3. Try making \n",
    "\n",
    "Finally, try applying \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Stemming\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.77      0.80       259\n",
      "        pos       0.77      0.83      0.80       241\n",
      "\n",
      "avg / total       0.80      0.80      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__tokenizer=tokenize)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 80529\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used XXX. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change \n",
    "2. Change \n",
    "3. Try making \n",
    "\n",
    "Finally, try applying \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Analysis\n",
    "\n",
    "We can also apply clustering analysis to our feature matrix. While\n",
    "finding an unknown number of clusters in text documents can be\n",
    "difficult, we can learn about our data by identifying the clusters for\n",
    "our **known** labels. To demonstrate, in the following code cells, we\n",
    "employ k-means to find twenty clusters in our feature matrix, after\n",
    "which we identify the most frequently used words in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=2, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 2\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', \\\n",
    "                     ngram_range=(1, 3), max_features=100000)\n",
    "\n",
    "train_counts = cv.fit_transform(mvr_train)\n",
    "test_data = cv.transform(mvr_test)\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: film like movie just time good character story way characters scene really make films does plot life man people scenes\n",
      "\n",
      "Cluster 1: film movie like just good time story character way does plot characters make life little really man people bad movies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = ['Neg', 'Pos']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='train', shuffle=True, random_state=23)\n",
    "test = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='test', shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=20, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 20\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', max_features=100000)\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: edu subject com lines organization writes article university posting host nntp don just like know think people does ca time\n",
      "\n",
      "Cluster 1: jehovah elohim lord god christ father mcconkie unto son ps jesus said gods shall thou thee mormon thy earth stated\n",
      "\n",
      "Cluster 2: dos windows microsoft tcp ms mouse amiga software pc graphics higher macintosh network mbytes version 00 ip memory support card\n",
      "\n",
      "Cluster 3: people god edu don like just know think new does say time way 10 subject com said right did government\n",
      "\n",
      "Cluster 4: jpeg image gif file color format images quality version files bit free programs available use jfif software don display edu\n",
      "\n",
      "Cluster 5: gopher search edu client pub database software ftp information veronica macintosh data retrieve available unix world micro clients mail sites\n",
      "\n",
      "Cluster 6: 25 mac files comp disk file software sys macintosh ftp faq questions stuffit 75 54 hard need available 102 apple\n",
      "\n",
      "Cluster 7: edu image graphics data pub ftp available mail software 128 images package send format 3d ray file files processing com\n",
      "\n",
      "Cluster 8: openwindows use sun xview usr look x11 lib open subject file openwinhome window olit news programs openwin manual fonts bin\n",
      "\n",
      "Cluster 9: planet earth spacecraft solar surface sun venus moon atmosphere planets jupiter kilometers miles space years moons degrees mars saturn mariner\n",
      "\n",
      "Cluster 10: god psalms christ lord prayers jesus church kingdom people man enemies 10 earth wicked praying day judgement word cursed words\n",
      "\n",
      "Cluster 11: file image use server com edu ftp images rx program xfree86 bit gamma display slip free make files look windows\n",
      "\n",
      "Cluster 12: myers ms president think don dee ll know said going decision does house white today believe justice just board department\n",
      "\n",
      "Cluster 13: kuwait al sheikh sabah iraq british kuwaiti ottoman history arabia oil abu government gulf ruler relations arab time following expedition\n",
      "\n",
      "Cluster 14: said president mr think stephanopoulos know people don going did just general went didn door time started children say ll\n",
      "\n",
      "Cluster 15: phigs conference tel pex dec fax demonstrated graphics application group advanced corporation user support software figaro extensions center features based\n",
      "\n",
      "Cluster 16: adl bullock gerard information francisco san fbi anti police groups says american office files israel group south law report israeli\n",
      "\n",
      "Cluster 17: 00 20 appears 40 art 50 80 10 wolverine 60 1st ghost rider hobgoblin punisher man annual sabretooth appear cover\n",
      "\n",
      "Cluster 18: cancer hiv pages aids breast booklet information patients medical treatment patient april 1993 health 25 self people infected hicnet number\n",
      "\n",
      "Cluster 19: launch dec nov mars solar aug 1993 00 anniversary flyby meteor jul jun shower maximum ut observer sts feb 04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = test['target']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used XXX. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change \n",
    "2. Change \n",
    "3. Try making \n",
    "\n",
    "Finally, try applying \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## DImension Reduction\n",
    "\n",
    "The matrices are big. Lets reduce the number of features. PCA can be difficult given the size. Could use incremental PCA or Truncated SVD. But lets select the best k features.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Following Example was insipred by scikit learn demo\n",
    "# http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy =  82.0%\n",
      "Number of Features = 129792\n"
     ]
    }
   ],
   "source": [
    "# First, train on normal set of features, baseline performance.\n",
    "\n",
    "train_counts = tf.fit_transform(train['data'])\n",
    "test_data = tf.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_counts, train['target'])\n",
    "predicted = nb.predict(test_data)\n",
    "\n",
    "print(\"Prediction accuracy = {0:5.1f}%\".format(100.0 * nb.score(test_data, test['target'])))\n",
    "print('Number of Features = {}'.format(nb.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "num_k = 10000\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=num_k)\n",
    "xtr = ch2.fit_transform(train_counts, train['target'])\n",
    "xt = ch2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  82.0%\n",
      "Number of Features = 10000\n"
     ]
    }
   ],
   "source": [
    "nb = nb.fit(xtr, train['target'])\n",
    "predicted = nb.predict(xt)\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * nb.score(xt, test['target'])))\n",
    "print('Number of Features = {}'.format(nb.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "indices = ch2.get_support(indices=True)\n",
    "feature_names = np.array([feature_names[idx] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "[ 'keith', 'god', 'caltech', 'atheists', 'livesey', 'com', 'atheism', 'people',\n",
      "  'schneider', 'sgi', 'morality', 'don', 'solntze', 'article', 'cco', 'wpd',\n",
      "  'allan', 'say', 'islamic', 'islam']\n",
      "\n",
      "comp.graphics:\n",
      "[ 'graphics', 'image', 'thanks', '3d', 'files', 'host', 'nntp', 'com',\n",
      "  'program', 'file', 'help', 'need', 'looking', 'images', 'format', 'version',\n",
      "  'computer', 'polygon', 'does', 'software']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "[ 'windows', 'dos', 'file', 'files', 'driver', 'ms', 'drivers', 'use', 'thanks',\n",
      "  'win', 'com', 'using', 'card', 'program', 'problem', 'host', 'ftp', 'version',\n",
      "  'nntp', 'help']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "[ 'drive', 'card', 'scsi', 'ide', 'bus', 'controller', 'com', 'pc', 'thanks',\n",
      "  'isa', 'disk', 'dos', 'drives', 'help', 'host', 'nntp', 'motherboard',\n",
      "  'computer', 'monitor', 'does']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "[ 'mac', 'apple', 'quadra', 'drive', 'centris', 'nntp', 'host', 'thanks',\n",
      "  'simms', 'se', 'monitor', 'does', 'problem', 'com', 'lc', 'scsi', 'use',\n",
      "  'duo', 'computer', 'new']\n",
      "\n",
      "comp.windows.x:\n",
      "[ 'window', 'motif', 'mit', 'com', 'server', 'x11r5', 'widget', 'application',\n",
      "  'lcs', 'host', 'thanks', 'nntp', 'xterm', 'using', 'use', 'windows', 'expo',\n",
      "  'problem', 'display', 'uk']\n",
      "\n",
      "misc.forsale:\n",
      "[ 'sale', 'offer', 'shipping', 'distribution', 'condition', 'new', '00', 'mail',\n",
      "  'forsale', 'interested', 'price', 'sell', 'nntp', 'host', 'asking', 'usa',\n",
      "  'email', 'com', 'computer', 'drive']\n",
      "\n",
      "rec.autos:\n",
      "[ 'car', 'cars', 'com', 'article', 'engine', 'nntp', 'host', 'dealer', 'usa',\n",
      "  'distribution', 'good', 'don', 'new', 'oil', 'ford', 'think', 'price', 'ca',\n",
      "  'drive', 'hp']\n",
      "\n",
      "rec.motorcycles:\n",
      "[ 'bike', 'dod', 'com', 'ride', 'motorcycle', 'article', 'bikes', 'riding',\n",
      "  'ca', 'bmw', 'nntp', 'host', 'rider', 'helmet', 'motorcycles', 'uk', 'don',\n",
      "  'sun', 'honda', 'good']\n",
      "\n",
      "rec.sport.baseball:\n",
      "[ 'baseball', 'year', 'game', 'team', 'players', 'games', 'article', 'runs',\n",
      "  'pitching', 'season', 'braves', 'com', 'hit', 'phillies', 'cs', 'win', 'nntp',\n",
      "  'host', 'think', 'league']\n",
      "\n",
      "rec.sport.hockey:\n",
      "[ 'hockey', 'team', 'game', 'ca', 'nhl', 'play', 'players', 'season', 'toronto',\n",
      "  'games', 'cup', 'teams', 'win', 'leafs', 'playoffs', 'playoff', 'year',\n",
      "  'wings', 'detroit', 'pittsburgh']\n",
      "\n",
      "sci.crypt:\n",
      "[ 'clipper', 'key', 'encryption', 'chip', 'com', 'keys', 'escrow', 'government',\n",
      "  'crypto', 'nsa', 'security', 'secret', 'secure', 'algorithm', 'public',\n",
      "  'netcom', 'pgp', 'des', 'wiretap', 'gtoal']\n",
      "\n",
      "sci.electronics:\n",
      "[ 'com', 'use', 'circuit', 'host', 'power', 'nntp', 'does', 'need', 'ca',\n",
      "  'article', 'electronics', 'phone', 'don', 'voltage', 'thanks', 'line',\n",
      "  'radio', 'good', 'work', 'hp']\n",
      "\n",
      "sci.med:\n",
      "[ 'pitt', 'geb', 'banks', 'gordon', 'com', 'cs', 'msg', 'article', 'disease',\n",
      "  'doctor', 'science', 'medical', 'pittsburgh', 'food', 'n3jxp', 'chastity',\n",
      "  'skepticism', 'dsl', 'intellect', 'cadre']\n",
      "\n",
      "sci.space:\n",
      "[ 'space', 'nasa', 'moon', 'orbit', 'henry', 'digex', 'gov', 'alaska', 'pat',\n",
      "  'access', 'com', 'article', 'launch', 'shuttle', 'zoo', 'earth', 'toronto',\n",
      "  'aurora', 'prb', 'nsmca']\n",
      "\n",
      "soc.religion.christian:\n",
      "[ 'god', 'jesus', 'christians', 'rutgers', 'christian', 'christ', 'bible',\n",
      "  'church', 'people', 'faith', 'athos', 'believe', '1993', 'christianity',\n",
      "  'apr', 'think', 'say', 'does', 'don', 'question']\n",
      "\n",
      "talk.politics.guns:\n",
      "[ 'gun', 'guns', 'com', 'people', 'batf', 'fbi', 'weapons', 'waco', 'firearms',\n",
      "  'don', 'article', 'stratus', 'government', 'atf', 'control', 'law', 'think',\n",
      "  'cdt', 'nntp', 'host']\n",
      "\n",
      "talk.politics.mideast:\n",
      "[ 'israel', 'israeli', 'jews', 'turkish', 'arab', 'armenian', 'armenians',\n",
      "  'people', 'armenia', 'serdar', 'argic', 'article', 'turks', 'jewish', 'arabs',\n",
      "  'policy', 'org', 'turkey', 'israelis', 'com']\n",
      "\n",
      "talk.politics.misc:\n",
      "[ 'com', 'cramer', 'article', 'people', 'clinton', 'optilink', 'state',\n",
      "  'government', 'clayton', 'gay', 'don', 'kaldis', 'new', 'tax', 'think',\n",
      "  'president', 'news', 'law', 'homosexual', 'sexual']\n",
      "\n",
      "talk.religion.misc:\n",
      "[ 'god', 'sandvik', 'com', 'christian', 'jesus', 'kent', 'people', 'morality',\n",
      "  'article', 'newton', 'don', 'apple', 'christians', 'bible', 'koresh', 'say',\n",
      "  'objective', 'religion', 'christ', 'think']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "top_count = 20\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_names = np.argsort(nb.coef_[idx])[-top_count:]\n",
    "    tn_lst = [name for name in feature_names[top_names]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print('\\n{0}:'.format(target))\n",
    "    pp.pprint(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used XXX. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change \n",
    "2. Change \n",
    "3. Try making \n",
    "\n",
    "Finally, try applying \n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
