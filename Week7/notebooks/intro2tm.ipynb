{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Text Mining\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore more advanced machine learning\n",
    "techniques with text data. First, we introduce the concept of n-grams,\n",
    "which are combinations of one or more tokens. For example, bigrams are\n",
    "combinations of two tokens, while trigrams are combinations of three\n",
    "tokens. Next, we introduce the concept of stemming, which is used to\n",
    "convert tokens into their root forms so that token frequencies match the\n",
    "use of the root token rather than being spread across multiple similar\n",
    "tokens. Finally, we explore the application of clustering and feature\n",
    "selection to text data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "Formally, a [_n-gram_][ngd] is a contiguous sequence of **n** items from a\n",
    "parent sequence of items, such as characters or words in a text\n",
    "document. In general, we will focus solely on words in a document. Thus,\n",
    "our initial approach has simply been to look at unigrams or single\n",
    "words in a document when building a classification model. However,\n",
    "sometimes the combination of words can be more descriptive, for example,\n",
    "_unbelievably bad_ is generally viewed as a more powerful description\n",
    "than just _bad_. As a result, the concept of an _n-gram_ was created,\n",
    "where collections of words can be treated as features. In fact google\n",
    "allows a user to search for [specific n-gram][gnv] combinations in books that\n",
    "they have scanned.\n",
    "\n",
    "While this clearly can improve classification power, it also increases\n",
    "computational requirements. This is a result of the exponential rise in\n",
    "the number of possible features. For example, given $n$ words, we have\n",
    "$n \\times (n - 1)$ possible bigrams, and so on for higher order\n",
    "combinations. While this is not a problem for small vocabularies, for\n",
    "larger vocabularies (and corresponding documents) the number of possible\n",
    "features can quickly become very large. Thus, many text mining\n",
    "applications will make use of Hadoop or Spark clusters to leverage the\n",
    "inherent parallelism in these tasks.\n",
    "\n",
    "To demonstrate using n-grams, we first demonstrate the concept on a\n",
    "single sentence.\n",
    "\n",
    "-----\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[ngd]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'info490', 'introduces', 'concepts', 'data', 'science', 'info490 introduces',\n",
      "  'introduces concepts', 'concepts data', 'data science',\n",
      "  'info490 introduces concepts', 'introduces concepts data',\n",
      "  'concepts data science']\n"
     ]
    }
   ],
   "source": [
    "my_text = 'info490 introduces many concepts in data science.'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,3), stop_words = ['in','many'])\n",
    "\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "pp.pprint(tk_func(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapping:\n",
      "----------------------------------------\n",
      "0 concepts\n",
      "1 concepts data\n",
      "2 concepts data science\n",
      "3 data\n",
      "4 data science\n",
      "5 info490\n",
      "6 info490 introduces\n",
      "7 info490 introduces concepts\n",
      "8 introduces\n",
      "9 introduces concepts\n",
      "10 introduces concepts data\n",
      "11 science\n",
      "----------------------------------------\n",
      "['INFO490 is data science']\n",
      "----------------------------------------\n",
      "[[0 0 0 1 1 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "in_list = []\n",
    "in_list.append(my_text)\n",
    "\n",
    "cv = cv.fit(in_list)\n",
    "\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print('Token mapping:')\n",
    "print(40*'-')\n",
    "\n",
    "for tokens, rank in my_voc:\n",
    "    print(rank, tokens)\n",
    "\n",
    "print(40*'-')\n",
    "out_list = ['INFO490 is data science']\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used `CountVectorizer` to create n-gram\n",
    "tokens. Now that you have run the Notebook, go back and make the\n",
    "following changes to see how the results change.\n",
    "\n",
    "1. Change the `CountVectorizer` to use stop words and change the tokens\n",
    "to all lowercase. How does this change the tokens and mappings? \n",
    "2. Create your own sentence, do the tokens and n-grams make sense?\n",
    "3. Try changing the ngram range to different values (e.g.,\n",
    "`ngram_range=(1,4)` or `ngram_range=(2,3)`). Notice how the number of\n",
    "tokens quickly increase.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## N-gram Classification\n",
    "\n",
    "Having n-grams offers improved classification, since word or token\n",
    "combinations often include more information than single words or tokens.\n",
    "For example, _University Illinois_ means more than just _University_ and\n",
    "_Illinois_. We can build on our previous simple text classification\n",
    "pipeline to now develop a more complete code example that builds a\n",
    "feature vector containing both single words and b-grams from the\n",
    "documents. We use this new sparse matrix to classify the documents by\n",
    "using our simple Naive Bayes classifier, which obtains slightly better\n",
    "results. First, we load the movie review data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 2000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = '/home/data_scientist/data/nltk_data/corpora/movie_reviews'\n",
    "\n",
    "\n",
    "#data_dir = '/home/data_scientist/nltk_data/corpora/movie_reviews'\n",
    "mvr = load_files(data_dir, shuffle = False)\n",
    "print('Number of Reviews: {0}'.format(len(mvr.data)))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(\n",
    "    mvr.data, mvr.target, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.71      0.77       259\n",
      "        pos       0.73      0.85      0.79       241\n",
      "\n",
      "avg / total       0.79      0.78      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase and restrict ourselves to about half the available features\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 421010\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now we can repeat the results, but now use unigrams, bigrams, and\n",
    "trigrams. Since this will produce a document term matrix that likely\n",
    "exceeds the computational resources of our Docker containers, we impose\n",
    "two cuts on the tokens included in our DTM. First, we impose a minimum\n",
    "feature term that requires a term to be present in at least two\n",
    "documents. Second, we set a maximum frequency of 50%, such that any term\n",
    "occurring in more than fifty percent of all documents will be ignored\n",
    "(these are likely all stop words, but this value can be reduced to a\n",
    "lower value to trim frequently occurring words that add little to the\n",
    "accuracy. Notice that our overall number of features is much smaller,\n",
    "even though we are also using trigrams in this classification pipeline.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.78      0.81       259\n",
      "        pos       0.78      0.84      0.81       241\n",
      "\n",
      "avg / total       0.81      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__min_df=2, \\\n",
    "                cv__max_df=0.50)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 74925\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we included n-grams in the classification\n",
    "process by using a `CountVectorizer`.  Now that you have run the\n",
    "Notebook, go back and make the following changes to see how the results\n",
    "change.\n",
    "\n",
    "1. Try to determine a value of `max_df` that replicates the effects of\n",
    "using stop words. \n",
    "2. Change `CountVectorizer` to `TfidfVectorizer`. Do the results change?\n",
    "3. Try using a more powerful classification algorithm, as opposed to\n",
    "`MultinomialNB`. Do the results change?\n",
    "\n",
    "-----\n",
    "\n",
    "## Stemming\n",
    "\n",
    "So far, we have looked at several techniques to remove redundant or\n",
    "unimportant features. For example, we changed the case of all text to\n",
    "lowercase and we have applied stop words. However, there still is the\n",
    "issue of different forms of the same word, for example compute,\n",
    "computer, computed, and computing. The process of changing words back to\n",
    "their root, or basic form (by removing prefixes and suffixes) is known as\n",
    "stemming. \n",
    "\n",
    "The most widely used stemmer, or program/method that performs stemming,\n",
    "is the _Porter Stemmer_, which was originally published in 1980 by\n",
    "Martin Porter. An improved version was released in 2000, which fixed a\n",
    "number of errors. NLTK includes the Porter Stemmer, which can be used\n",
    "with scikit learn by creating a special function that tokenizes text\n",
    "documents and passing this function as an argument to the\n",
    "`CountVectorizer` via the `tokenizer` attribute. By performing stemming\n",
    "inside this tokenize method, we can return a set of tokens for a\n",
    "document that have been stemmed. In the following code cell, we use a\n",
    "custom `tokenize` method that first builds a list of tokens by using\n",
    "nltk, and then maps the Porter stemmer to the list of tokens to generate\n",
    "a stemmed list.\n",
    "\n",
    "\n",
    "-----\n",
    "[ws]: https://en.wikipedia.org/wiki/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.78      0.81       259\n",
      "        pos       0.78      0.84      0.81       241\n",
      "\n",
      "avg / total       0.81      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = EnglishStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__min_df=2, \\\n",
    "                cv__max_df=0.50, \\\n",
    "                cv__tokenizer=tokenize)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 74925\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we incorporated the Porter Stemmer into the\n",
    "classification pipeline. Now that you have run the Notebook, go back and\n",
    "make the following changes to see how the results change.\n",
    "\n",
    "1. Did the Porter Stemmer improve the classification results (note you\n",
    "need to be sure you are comparing exactly the same pipeline (including\n",
    "the use of ngrams)? \n",
    "\n",
    "2. Can you compute the number of features that the Porter Stemmer\n",
    "generates? How does this compare the number of features without stemming?\n",
    "\n",
    "3. Try using a different stemming algorithm, such as [_snowball_][nsw].\n",
    "How do the classification results change? \n",
    "\n",
    "-----\n",
    "[nsw]: http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball\n",
    "\n",
    "## Clustering Analysis\n",
    "\n",
    "We can also apply clustering analysis to our feature matrix. While\n",
    "finding an unknown number of clusters in text documents can be\n",
    "difficult, we can learn about our data by identifying the clusters for\n",
    "our **known** labels. To demonstrate, in the following code cells, we\n",
    "employ k-means to find two clusters in our feature matrix (the moview reviews), after\n",
    "which we identify the most frequently used words in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=2, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 2\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', \\\n",
    "                     ngram_range=(1, 3), max_features=100000)\n",
    "\n",
    "train_counts = cv.fit_transform(mvr_train)\n",
    "test_data = cv.transform(mvr_test)\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: film like just movie character time good story way characters scene really make does films life man scenes best little\n",
      "\n",
      "Cluster 1: film movie like just good time story character plot way characters make does bad people really movies little new director\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = ['Neg', 'Pos']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can perform the same analysis on a more complex problem by analyzing\n",
    "the twenty newsgroup data set. First we load the data, after which we\n",
    "apply k-means and identify the most common tokens in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='train', shuffle=True, random_state=23)\n",
    "test = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='test', shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=20, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 20\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', ngram_range=(1, 3), max_features=100000)\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: 25 file use ftp edu openwindows files server com xview gopher look mac available software faq subject sun make president\n",
      "\n",
      "Cluster 1: 92 12 12 92 10 hiv 17 10 12 11 aids patients et medical 03 30 11 92 25 31 milk 1993 tb\n",
      "\n",
      "Cluster 2: dos dos dos windows microsoft microsoft windows tcp ms mouse amiga software pc graphics higher macintosh network version ms windows 00 ip memory\n",
      "\n",
      "Cluster 3: jpeg image gif file color format images quality version files bit free programs available use jfif software edu don display\n",
      "\n",
      "Cluster 4: edu graphics pub mail ray 128 send ftp 3d com server objects amiga image archie rayshade file files images edu 128\n",
      "\n",
      "Cluster 5: mb m4 ms ma mz mm mo m1 mu mc mh mw mf mj mk mx mp mt m3 mr\n",
      "\n",
      "Cluster 6: edu com subject lines organization writes posting article host university nntp nntp posting nntp posting host posting host like know ca just don does\n",
      "\n",
      "Cluster 7: said started armenians didn went people diana children mamma apartment door don know azerbaijanis came ran balcony bus couldn going\n",
      "\n",
      "Cluster 8: myers ms ms myers think president don dee ll know said going decision dee dee does house ms myers don myers don white house white today\n",
      "\n",
      "Cluster 9: planet earth spacecraft solar surface venus sun moon atmosphere planets jupiter kilometers miles space years moons degrees mars saturn mariner\n",
      "\n",
      "Cluster 10: god people like armenian know just said don does edu say time right jesus think way new government did believe\n",
      "\n",
      "Cluster 11: det nyr tor bos mtl chi la pit edm phi min wsh hfd wpg stl cgy buf nyi van nj\n",
      "\n",
      "Cluster 12: image edu data available graphics ftp software pub processing images package format analysis display formats information file version program files\n",
      "\n",
      "Cluster 13: edu writes article subject lines organization don com just like people think university know posting host nntp nntp posting posting host nntp posting host\n",
      "\n",
      "Cluster 14: cancer hiv health 1993 medical number april 25 11 disease drug information volume newsletter medical newsletter volume number 25 1993 april 25 hicnet medical hicnet\n",
      "\n",
      "Cluster 15: said door ira ll children went law igor don didn home open started balcony people afraid know came let time\n",
      "\n",
      "Cluster 16: stephanopoulos mr mr stephanopoulos president general think did know attorney general attorney just don going decision george said statement yesterday stephanopoulos think mr stephanopoulos think\n",
      "\n",
      "Cluster 17: adl bullock gerard information francisco san francisco fbi san anti police groups says american office files israel group south israeli report\n",
      "\n",
      "Cluster 18: 00 20 appears 40 50 art 80 10 wolverine 60 1st ghost rider rider ghost hobgoblin punisher man annual sabretooth appear\n",
      "\n",
      "Cluster 19: jehovah lord god christ father mcconkie unto son ps jesus said gods shall thou thee mormon thy earth stated spirit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = test['target']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used k-means clustering to find clusters in\n",
    "text data, and to identify the most important tokens in these clusters.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the vectorizer to use TF-IDF. Does this change the tokens in\n",
    "each cluster? \n",
    "2. Change the vectorizer to use ngrams. Does this change the tokens in\n",
    "each cluster?\n",
    "3. Include stemming in the vectorizer. Does this change the tokens in\n",
    "each cluster?\n",
    "4. Try using DBSCAN instead of k-means. How many clusters are found?\n",
    "What are the tokens in each cluster?\n",
    "\n",
    "Finally, the k-means algorithm finds clusters. Do these clusters map\n",
    "directly into the _real_ categories? Feel free to discuss this on the\n",
    "course forums.\n",
    "\n",
    "-----\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "The document term matrices we have constructed in these examples can\n",
    "become quite large. We have already reduced the number of features used\n",
    "in classification problems by using stop words, by using a consistent\n",
    "case, and by performing stemming. On the other hand, we have enabled\n",
    "exponential increases in the feature space by including n-grams\n",
    "(although we once again restricted the feature space by using the\n",
    "`max_features` or the `max_df` and `min_df` attributes). The\n",
    "traditional dimensional reduction technique we have used in the past is\n",
    "PCA. However, PCA can be difficult to use with text data given the large\n",
    "sizes of the matrices (since a matrix inversion can be required). We can\n",
    "employ alternative techniques, such as incremental PCA or Truncated SVD. \n",
    "\n",
    "But in reality, we are less interested in finding a reduced dimensional\n",
    "space than we are in removing features that contain little or no\n",
    "information (combining features is essentially increasing the ngram\n",
    "range). In this case, the problem of dimension reduction becomes one of\n",
    "optimal feature selection. For this, we can use the scikit learn\n",
    "`SelectKBest` method to identify the best $k$ features. In the following\n",
    "code cells, we first create a vectorizer, train it on data to\n",
    "demonstrate the accuracy and the number of features required to achieve\n",
    "that accuracy. After which, we employ `SelectKBest` to identify the best\n",
    "number of features, where number is predetermined, to achieve a similar\n",
    "accuracy. In the end, we find that less than ten percent of the original\n",
    "features are required to achieve the same level of accuracy.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Following Example was insipred by scikit learn demo\n",
    "# http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy =  82.0%\n",
      "Number of Features = 129792\n"
     ]
    }
   ],
   "source": [
    "# First, train on normal set of features, baseline performance.\n",
    "\n",
    "train_counts = tf.fit_transform(train['data'])\n",
    "test_data = tf.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_counts, train['target'])\n",
    "predicted = nb.predict(test_data)\n",
    "\n",
    "print(\"Prediction accuracy = {0:5.1f}%\".format(100.0 * nb.score(test_data, test['target'])))\n",
    "print('Number of Features = {}'.format(nb.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now employ feature selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Number of features to keep\n",
    "num_k = 10000\n",
    "\n",
    "# Employ select k best wiht chi2 since this works with sparse matrices.\n",
    "ch2 = SelectKBest(chi2, k=num_k)\n",
    "xtr = ch2.fit_transform(train_counts, train['target'])\n",
    "xt = ch2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  82.0%\n",
      "Number of Features = 10000\n"
     ]
    }
   ],
   "source": [
    "# Train simple model and compute accuracy\n",
    "nb = nb.fit(xtr, train['target'])\n",
    "predicted = nb.predict(xt)\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * nb.score(xt, test['target'])))\n",
    "print('Number of Features = {}'.format(nb.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can use the feature selection to identify the top features for each\n",
    "category. First we extract the feature names, after which we extract the\n",
    "importance (or support) for each feature. By sorting the array containing\n",
    "the important features, we can identify the top tokens.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "indices = ch2.get_support(indices=True)\n",
    "feature_names = np.array([feature_names[idx] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'god', 'caltech', 'atheists', 'livesey']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'image', 'thanks', '3d', 'files']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'dos', 'file', 'files', 'driver']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['drive', 'card', 'scsi', 'ide', 'bus']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'quadra', 'drive', 'centris']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'com', 'server']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'offer', 'shipping', 'distribution', 'condition']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'cars', 'com', 'article', 'engine']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'dod', 'com', 'ride', 'motorcycle']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['baseball', 'year', 'game', 'team', 'players']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'nhl']\n",
      "\n",
      "sci.crypt:\n",
      "['clipper', 'key', 'encryption', 'chip', 'com']\n",
      "\n",
      "sci.electronics:\n",
      "['com', 'use', 'circuit', 'host', 'power']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'geb', 'banks', 'gordon', 'com']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'moon', 'orbit', 'henry']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'rutgers', 'christian']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'guns', 'com', 'people', 'batf']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'arab']\n",
      "\n",
      "talk.politics.misc:\n",
      "['com', 'cramer', 'article', 'people', 'clinton']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'com', 'christian', 'jesus']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "top_count = 5\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_names = np.argsort(nb.coef_[idx])[-top_count:]\n",
    "    tn_lst = [name for name in feature_names[top_names]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print('\\n{0}:'.format(target))\n",
    "    pp.pprint(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used feature selection to identify the most\n",
    "important features in our simple classification pipeline. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change the vectorizer to change the case of all words an to employ\n",
    "stemming. How do the results (tokens) change?\n",
    "\n",
    "2. Change the classification algorithm to a more accurate method. How do\n",
    "the results change? How does the computational time change?\n",
    "\n",
    "Finally, what do the list of tokens say about the fact we did not remove\n",
    "headers or footers from the newsgroup postings? Feel free to comment on\n",
    "these questions in the course forum.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
