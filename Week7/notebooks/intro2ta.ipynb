{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Text Analysis\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we introduce text analysis.\n",
    "\n",
    "1. fetch news group\n",
    "2. explore data\n",
    "3. one article.\n",
    "4. tokenize with python\n",
    "5. count.\n",
    "6. parse, header/footer\n",
    "7. lower case\n",
    "8. remove odd things\n",
    "9. Counts\n",
    "10. Bag of Words.\n",
    "11. NLTK.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "To get started with text analysis, we need **texts to analyze**. To get\n",
    "started, we will analyze the [twenty newsgroup][tng] data set. We first\n",
    "download this data (scikit learn has built in methods for doing this,\n",
    "however, we have cached a copy locally on our server). The data are made\n",
    "available via a custom object, but we can access the data of interest by\n",
    "using dictionary keys. Before delving into text analysis, we first\n",
    "explore this data over several code cells to understand more about the\n",
    "task at hand.\n",
    "\n",
    "-----\n",
    "\n",
    "[tng]: http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['target_names', 'target', 'DESCR', 'filenames', 'data'])\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "text = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm')\n",
    "\n",
    "# To learn more about these data, either browse the relevant \n",
    "# scikit learn documentation, or enter help(text) in an IPython code cell\n",
    "\n",
    "# The data can be accessed via Dictionary keys\n",
    "\n",
    "print(text.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0 = alt.atheism\n",
      "Class  1 = comp.graphics\n",
      "Class  2 = comp.os.ms-windows.misc\n",
      "Class  3 = comp.sys.ibm.pc.hardware\n",
      "Class  4 = comp.sys.mac.hardware\n",
      "Class  5 = comp.windows.x\n",
      "Class  6 = misc.forsale\n",
      "Class  7 = rec.autos\n",
      "Class  8 = rec.motorcycles\n",
      "Class  9 = rec.sport.baseball\n",
      "Class 10 = rec.sport.hockey\n",
      "Class 11 = sci.crypt\n",
      "Class 12 = sci.electronics\n",
      "Class 13 = sci.med\n",
      "Class 14 = sci.space\n",
      "Class 15 = soc.religion.christian\n",
      "Class 16 = talk.politics.guns\n",
      "Class 17 = talk.politics.mideast\n",
      "Class 18 = talk.politics.misc\n",
      "Class 19 = talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# Display target names, i.e., the names of the twenty news groups\n",
    "\n",
    "for idx, label in enumerate(text['target_names']):\n",
    "    print('Class {0:2d} = {1}'.format(idx, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Newsgroup: comp.sys.ibm.pc.hardware\n",
      "----------------------------------------------------------------------\n",
      "From: rnichols@cbnewsg.cb.att.com (robert.k.nichols)\n",
      "Subject: Re: how to search for bad memory chips.\n",
      "Organization: AT&T\n",
      "Distribution: na\n",
      "Lines: 29\n",
      "\n",
      "In article <N5s42B8w165w@c-cat.UUCP> david@c-cat.UUCP (Dave) writes:\n",
      ">i came upon this idea i would like to share with everyone.\n",
      ">\n",
      ">to check for bad memory chips\n",
      ">\n",
      ">1. create a boot disk with emm386 himem.sys and ramdrive.sys in the\n",
      ">   config/autoexec.bat.\n",
      ">\n",
      ">2. boot the PC to create a RAM drive as large as possible.\n",
      ">\n",
      ">3. use a disk repair utility ( I use NDD ). Run it on the RAM\n",
      ">        drive, yes it will run, its only a device driver\n",
      ">\n",
      ">4. run 1000 or so passes, they go very quick\n",
      ">\n",
      ">5. if your machine fails, there is a definate bad memory chip\n",
      ">\n",
      ">6. if your machine passes, there is a conflict with programs you\n",
      ">        are loading in memory.\n",
      "...\n",
      "\n",
      "It's an interesting idea, but the worst-case data patterns developed to\n",
      "test magnetic media are totally different than the patterns used to detect\n",
      "common faults in memory chips.\n",
      "\n",
      "--\n",
      "Bob Nichols\n",
      "AT&T Bell Laboratories\n",
      "rnichols@ihlpm.ih.att.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display single message\n",
    "messageID = 250\n",
    "\n",
    "print('Target Newsgroup: {0}'.format(text['target_names'][text['target'][messageID]]))\n",
    "print(70*'-')\n",
    "\n",
    "message = text['data'][messageID]\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression Count: 8\n",
      "Isolated Token Count: 4\n"
     ]
    }
   ],
   "source": [
    "print('Expression Count: {0}'.format(message.count('the')))\n",
    "print('Isolated Token Count: {0}'.format(message.count(' the ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('>', 10), ('a', 6), ('the', 5), ('to', 5), ('memory', 4), ('with', 3),\n",
      "  ('bad', 3), ('in', 3), ('create', 2), ('is', 2), ('passes,', 2), ('disk', 2),\n",
      "  ('if', 2), ('as', 2), ('chips.', 2), ('it', 2), ('patterns', 2), ('use', 2),\n",
      "  ('for', 2), ('AT&T', 2), ('RAM', 2), ('are', 2), ('your', 2), ('boot', 2),\n",
      "  ('machine', 2), ('there', 2), ('everyone.', 1), ('NDD', 1), ('Re:', 1),\n",
      "  ('drive', 1), ('>i', 1), ('check', 1), ('on', 1), ('...', 1), ('drive,', 1),\n",
      "  ('test', 1), ('large', 1), ('common', 1), ('different', 1), ('device', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections as cl\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "words = message.split()\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "# Number of tokens to display from message\n",
    "wc_display = 40\n",
    "\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('a', 6), ('to', 6), ('the', 5), ('memory', 5), ('with', 3), ('chips', 3),\n",
      "  ('in', 3), ('bad', 3), ('drive', 2), ('create', 2), ('is', 2), ('if', 2),\n",
      "  ('as', 2), ('it', 2), ('i', 2), ('UUCP', 2), ('c', 2), ('use', 2), ('cat', 2),\n",
      "  ('patterns', 2), ('rnichols', 2), ('for', 2), ('RAM', 2), ('are', 2),\n",
      "  ('AT', 2), ('your', 2), ('com', 2), ('run', 2), ('idea', 2), ('boot', 2),\n",
      "  ('sys', 2), ('disk', 2), ('machine', 2), ('T', 2), ('there', 2), ('att', 2),\n",
      "  ('passes', 2), ('david', 1), ('Organization', 1), ('check', 1)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "  \n",
    "words = re.sub(pattern, ' ', message).split()\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('to', 6), ('a', 6), ('the', 5), ('memory', 5), ('in', 4), ('with', 3),\n",
      "  ('chips', 3), ('it', 3), ('i', 3), ('bad', 3), ('run', 3), ('at', 2),\n",
      "  ('drive', 2), ('create', 2), ('nichols', 2), ('is', 2), ('if', 2), ('as', 2),\n",
      "  ('patterns', 2), ('c', 2), ('t', 2), ('for', 2), ('use', 2), ('cat', 2),\n",
      "  ('rnichols', 2), ('are', 2), ('uucp', 2), ('ram', 2), ('your', 2), ('com', 2),\n",
      "  ('idea', 2), ('boot', 2), ('sys', 2), ('disk', 2), ('machine', 2),\n",
      "  ('there', 2), ('att', 2), ('passes', 2), ('david', 1), ('check', 1)]\n"
     ]
    }
   ],
   "source": [
    "words = re.sub(pattern, ' ', message.lower()).split()\n",
    "\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term        : Frequency\n",
      "-------------------------\n",
      "to          : 0.032\n",
      "a           : 0.032\n",
      "the         : 0.027\n",
      "memory      : 0.027\n",
      "in          : 0.021\n",
      "with        : 0.016\n",
      "chips       : 0.016\n",
      "it          : 0.016\n",
      "i           : 0.016\n",
      "bad         : 0.016\n"
     ]
    }
   ],
   "source": [
    "print('{0:12s}: {1}'.format('Term', 'Frequency'))\n",
    "print(25*'-')\n",
    "\n",
    "t_wc = sum(wc.values())\n",
    "for wt in wc.most_common(10):\n",
    "    print('{0:12s}: {1:4.3f}'.format(wt[0], wt[1]/t_wc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used XXX. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change \n",
    "2. Change \n",
    "3. Try making \n",
    "\n",
    "Finally, try applying \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Bag of Words\n",
    "\n",
    "A simple question about text data mining that you might have is _How\n",
    "does one classify documents made up of words when machine learning\n",
    "algorithms work on numerical data?_ The simple answer is we need to\n",
    "build a numerical summary of a data set that our algorithms can\n",
    "manipulate. A conceptually easy approach  to implement this idea is to\n",
    "identify all possible words in the documents of interest and to track the\n",
    "number of times each words occurs in specific documents. This produces a\n",
    "(very) sparse matrix for our sample of documents, where the columns are\n",
    "the possible words (or tokens) and the rows are different documents. \n",
    "\n",
    "This concept, where one tokenizes documents to build these sparse\n",
    "matrices is more formally known as _bag of words_, because we effectively\n",
    "create the [bag of words][bwd] out of which are documents are\n",
    "constructed. In the bag of words model, each document can be mapped into\n",
    "a vector, where the individual elements correspond to the number of\n",
    "times the words (associated with the particular column) appears in the\n",
    "document.\n",
    "\n",
    "With scikit learn, we can use the [`CountVectorizer`][skcv] to break our\n",
    "document into tokens (in this case words), which are used to construct\n",
    "our _bag of words_ for the given set of documents. Given this tokenizer,\n",
    "we first need to construct the list of tokens, which we do with the\n",
    "`fit` method. Second, we need to transform our documents into this\n",
    "sparse matrix, which we do with the `transform` method. Since both steps\n",
    "use the same input dta, there is a convenience method to perform both\n",
    "operations at the same time, called `fit_transform`.\n",
    "\n",
    "-----\n",
    "[bwd]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "[skcv]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----\n",
    "\n",
    "Given the `CountVectorizer` we can see the number of words in our _bag_\n",
    "as well as the number of documents on which we train, which in this case\n",
    "agrees with the values we obtained when we read in the data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(text['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples = 1\n",
      "Number of Tokens = 130107\n",
      "---------------------------------------------------------------------------\n",
      "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "---------------------------------------------------------------------------\n",
      "Tuples from Document-Term Matrix[i, j] and c (Count)\n",
      "---------------------------------------------------------------------------\n",
      "[ (0, 2338, 1), (0, 10368, 1), (0, 28012, 1), (0, 28146, 1), (0, 29241, 2),\n",
      "  (0, 29573, 1), (0, 29620, 2), (0, 30044, 2), (0, 30193, 2), (0, 30526, 1),\n",
      "  (0, 31495, 3), (0, 32036, 1), (0, 32669, 1), (0, 34128, 1), (0, 34404, 2),\n",
      "  (0, 35805, 1), (0, 37509, 1), (0, 38019, 1), (0, 38082, 2), (0, 38243, 1),\n",
      "  (0, 38314, 1), (0, 39169, 1), (0, 39415, 1), (0, 39426, 3), (0, 41105, 2),\n",
      "  (0, 41288, 1), (0, 41850, 1), (0, 41880, 1), (0, 43311, 2), (0, 45143, 1),\n",
      "  (0, 45215, 1), (0, 45240, 1), (0, 45917, 1), (0, 46708, 1), (0, 46815, 1),\n",
      "  (0, 46838, 1), (0, 47201, 1), (0, 47721, 2), (0, 47982, 1), (0, 49047, 2),\n",
      "  (0, 49055, 1), (0, 51399, 1), (0, 52954, 1), (0, 54202, 1), (0, 54532, 1),\n",
      "  (0, 56283, 2), (0, 56979, 1), (0, 59590, 1), (0, 63250, 1), (0, 64186, 1),\n",
      "  (0, 65677, 2), (0, 65798, 2), (0, 65912, 1), (0, 65935, 1), (0, 66608, 4),\n",
      "  (0, 67798, 1), (0, 68532, 2), (0, 68766, 3), (0, 68857, 1), (0, 74314, 1),\n",
      "  (0, 74619, 1), (0, 75901, 1), (0, 76032, 1), (0, 76511, 1), (0, 78644, 2),\n",
      "  (0, 78878, 1), (0, 80798, 1), (0, 81066, 5), (0, 85739, 1), (0, 85884, 1),\n",
      "  (0, 86377, 1), (0, 87186, 2), (0, 89860, 1), (0, 89919, 1), (0, 90252, 1),\n",
      "  (0, 90379, 1), (0, 92260, 2), (0, 92408, 2), (0, 92544, 1), (0, 95130, 1),\n",
      "  (0, 96222, 1), (0, 98379, 1), (0, 99176, 2), (0, 99202, 1), (0, 99721, 1),\n",
      "  (0, 100949, 1), (0, 102566, 2), (0, 102656, 1), (0, 103506, 3),\n",
      "  (0, 105683, 1), (0, 106610, 1), (0, 108558, 1), (0, 111322, 1),\n",
      "  (0, 112650, 2), (0, 114239, 1), (0, 114418, 1), (0, 114455, 5),\n",
      "  (0, 114579, 2), (0, 114646, 1), (0, 114731, 1), (0, 115475, 6),\n",
      "  (0, 115825, 1), (0, 119519, 1), (0, 119737, 2), (0, 119740, 1),\n",
      "  (0, 119899, 1), (0, 119977, 2), (0, 121265, 1), (0, 124332, 1),\n",
      "  (0, 124616, 3), (0, 125094, 1), (0, 125110, 1), (0, 125271, 1),\n",
      "  (0, 128084, 1), (0, 128402, 1), (0, 128420, 2)]\n"
     ]
    }
   ],
   "source": [
    "# We needd an iteratable to appply cv.transform()\n",
    "msg = []\n",
    "msg.append(message)\n",
    "\n",
    "dtm = cv.transform(msg)\n",
    "\n",
    "print('Number of Samples = {0}'.format(dtm.shape[0]))\n",
    "print('Number of Tokens = {0}'.format(dtm.shape[1]))\n",
    "print(75*'-')\n",
    "\n",
    "# We can convert from sparse to dense to explore the document-term matrix\n",
    "print(dtm.todense()[:,45141:45240])\n",
    "print(75*'-')\n",
    "\n",
    "# We can also find the \n",
    "import scipy.sparse as sp\n",
    "\n",
    "print('Tuples from Document-Term Matrix[i, j] and c (Count)')\n",
    "print(75*'-')\n",
    "\n",
    "i, j, c = sp.find(dtm)\n",
    "dtm_list = list(zip(i, j, c))\n",
    "pp.pprint(dtm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can grab the words in our _bag of words_ by extracting the _vocubulary_. This allows us to see if words are present in the documents.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arbitrary Word = the: Column = 114455\n",
      "Max Word = to : Column = 115475\n",
      "Min Word = 1000 : Column = 2338\n"
     ]
    }
   ],
   "source": [
    "terms = cv.vocabulary_\n",
    "\n",
    "my_word = 'the'\n",
    "print(\"Arbitrary Word = {0}: Column = {1}\".format(my_word, terms[my_word]))\n",
    "\n",
    "from operator import itemgetter\n",
    "max_key = max(dtm_list, key=itemgetter(2))[1]\n",
    "min_key = min(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "x_max = [key for key in terms.keys() if terms[key] == max_key]\n",
    "x_min = [key for key in terms.keys() if terms[key] == min_key]\n",
    "\n",
    "print(\"Max Word = {0} : Column = {1}\".format(x_max[0], max_key))\n",
    "print(\"Min Word = {0} : Column = {1}\".format(x_min[0], min_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count = 6: terms[115475] = to\n",
      "Count = 5: terms[114455] = the\n",
      "Count = 5: terms[81066] = memory\n",
      "Count = 4: terms[66608] = in\n",
      "Count = 3: terms[124616] = with\n",
      "Count = 3: terms[103506] = run\n",
      "Count = 3: terms[68766] = it\n",
      "Count = 3: terms[39426] = chips\n",
      "Count = 3: terms[31495] = bad\n",
      "Count = 2: terms[29241] = are\n"
     ]
    }
   ],
   "source": [
    "# Number of terms to display\n",
    "top_display = 10\n",
    "\n",
    "# Sort our document term list, and unzip\n",
    "dtm_list.sort(key=itemgetter(2), reverse=True)\n",
    "i, j, c = zip(*dtm_list)\n",
    "\n",
    "# Grab out the keys and values for top terms\n",
    "x_keys = [(k, v) for k, v in terms.items() if terms[k] in j[:top_display]]\n",
    "x_keys.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Grab the data, including counts from DTM list\n",
    "x_counts = dtm_list[:top_display]\n",
    "x_counts.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Now we merge the two lists so we can sort to display terms in order\n",
    "x_merged = []\n",
    "for idx in range(len(x_keys)):\n",
    "    if x_keys[idx][1] != x_counts[idx][1]:\n",
    "        print('Error: column mismatch!')\n",
    "\n",
    "    x_merged.append((x_keys[idx][0], x_keys[idx][1], x_counts[idx][2]))\n",
    "\n",
    "x_merged.sort(key=itemgetter(2), reverse=True)\n",
    "\n",
    "# Print results\n",
    "for x in x_merged:\n",
    "    print(\"Count = {2}: terms[{0}] = {1}\".format(x[1], x[0], x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used scikit learn to ....\n",
    "\n",
    "Note how we do not have the single charctaer word `a` in the list of most common items. The reason is the default tokenizer for `CountVectorizer` removes single character words (as they are assumed to be uninformative). You can fix this by assigning the string `'(?u)\\\\b\\\\w+\\\\b'` as a value to the `token_pattern` parameter when creating the `CountVectorizer`.\n",
    "\n",
    "\n",
    "Rerun with this change and verify that the `a` token now appears.\n",
    "\n",
    "PCA to transform the combined digit data\n",
    "and to explore the distribution of the real data and outliers in\n",
    "two-dimensional plots by using different PCA components. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change the number of PCA components lower and higher. How does this\n",
    "affect the separation of the outliers?\n",
    "2. Change the PCA components used to make these plots, for example\n",
    "to the third and fourth component. How does this affect the separation\n",
    "of the outliers?\n",
    "3. Try making a new fake data point that combines the 'bar' image from\n",
    "the helper code with an existing number, such as a six. Using a\n",
    "different color ini the plots for this data point, where does this new,\n",
    "more realistic _fake_ data point lie in the previous plots?\n",
    "\n",
    "Finally, try applying a classifier on the new digit data. Can you\n",
    "explain (feel free to use the class forums) why the classification\n",
    "algorithm performed in the  manner it did for the new _fake_ digits?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('>', 19), ('.', 13), (':', 7), ('to', 6), (',', 6), ('a', 6), ('the', 5),\n",
      "  ('memory', 5), ('@', 4), ('in', 4)]\n"
     ]
    }
   ],
   "source": [
    "words = [word.lower() for word in nltk.word_tokenize(message)]\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('to', 6), ('a', 6), ('the', 5), ('memory', 5), ('in', 4), ('with', 3),\n",
      "  ('chips', 3), ('it', 3), ('i', 3), ('bad', 3)]\n"
     ]
    }
   ],
   "source": [
    "words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', message))]\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message has 128 tokens and 188 words for a lexical diversity of 1.469\n"
     ]
    }
   ],
   "source": [
    "# https://en.wikipedia.org/wiki/Lexical_diversity\n",
    "\n",
    "num_words = len(words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Message has %i tokens and %i words for a lexical diversity of %0.3f\" % (num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bins(tokens) = 128\n",
      "\n",
      "Number of sample outcomes = 188\n",
      "\n",
      "Maximum occuring token = to\n",
      "\n",
      "to           has frequency 0.032\n",
      "to           has frequency 0.032\n",
      "to           has frequency 0.032\n",
      "a            has frequency 0.032\n",
      "to           has frequency 0.032\n",
      "a            has frequency 0.032\n",
      "a            has frequency 0.032\n",
      "a            has frequency 0.032\n",
      "a            has frequency 0.032\n",
      "a            has frequency 0.032\n"
     ]
    }
   ],
   "source": [
    "# Display number of unique tokens (or bins)\n",
    "print('Number of unique bins(tokens) = {0}\\n'.format(counts.B()))\n",
    "print('Number of sample outcomes = {0}\\n'.format(counts.N()))\n",
    "print('Maximum occuring token = {0}\\n'.format(counts.max()))\n",
    "\n",
    "tokens = [(word, counts.freq(word)) for word in words]\n",
    "tokens.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "for token, freq in tokens[:top_display]:\n",
    "    print('{0:12s} has frequency {1:4.3f}'.format(token, freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'david', 'check', 'on', 'test', 'subject', 'common', 'cbnewsg', 'but', 'cb',\n",
      "  'bat']\n"
     ]
    }
   ],
   "source": [
    "# Hapaxes: https://en.wikipedia.org/wiki/Hapax_legomenon\n",
    "pp.pprint(counts.hapaxes()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  to    a  the memory   in with chips   it    i  bad \n",
      "   6    6    5    5    4    3    3    3    3    3 \n"
     ]
    }
   ],
   "source": [
    "counts.tabulate(top_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEuCAYAAABh+A5lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzY5tL3MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fed9480d7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts.plot(top_display, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## NLTK Corpus\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access an NLTK data set. \n",
    "#Reuter articles\n",
    "\n",
    "mvr = nltk.corpus.movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity Dataset Version 2.0\n",
      "Bo Pang and Lillian Lee\n",
      "\n",
      "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
      "\n",
      "Distributed with NLTK with permission from the authors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data set README,  remove array bounds to see entire file\n",
    "print(mvr.readme()[:178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review has 39768 tokens and 1583820 words for a lexical diversity of 39.826\n"
     ]
    }
   ],
   "source": [
    "mvr_words = mvr.words()\n",
    "counts  = nltk.FreqDist(mvr_words)\n",
    "num_words = len(mvr_words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Movie Review has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\".format(num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party',\n",
      "  ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an',\n",
      "  'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his',\n",
      "  'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',',\n",
      "  'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?',\n",
      "  'watch']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(mvr.words()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of reviews = 2000\n"
     ]
    }
   ],
   "source": [
    "# Each article is in a separate file\n",
    "\n",
    "print('Total Number of reviews = {0}'.format(len(mvr.fileids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example File: neg/cv000_29416.txt\n"
     ]
    }
   ],
   "source": [
    "a_filename = mvr.fileids()[0]\n",
    "print('Example File: {0}'.format(a_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('plot : two teen couples go to a church party , drink and then drive . \\n'\n",
      " 'they get into an accident . \\n'\n",
      " 'one of the guys dies , but his girlfriend continues to see him in her life '\n",
      " ', and has nightmares . \\n'\n",
      " \"what's the d\")\n"
     ]
    }
   ],
   "source": [
    "# Print part of the file\n",
    "pp.pprint(mvr.raw(a_filename)[:211])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# Display article assigned categories\n",
    "pp.pprint(mvr.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt',\n",
      "  'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt',\n",
      "  'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt',\n",
      "  'neg/cv009_29417.txt', 'neg/cv010_29063.txt', 'neg/cv011_13044.txt',\n",
      "  'neg/cv012_29411.txt', 'neg/cv013_10494.txt', 'neg/cv014_15600.txt',\n",
      "  'neg/cv015_29356.txt', 'neg/cv016_4348.txt', 'neg/cv017_23487.txt',\n",
      "  'neg/cv018_21672.txt', 'neg/cv019_16117.txt']\n"
     ]
    }
   ],
   "source": [
    "# Find articles that have specific category\n",
    "pp.pprint(mvr.fileids('neg')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '9', ':', 'its', 'pathetic', 'attempt', 'at', '\"', 'improving', '\"', 'on',\n",
      "  'a', 'shakespeare', 'classic', '.']\n",
      "['8', ':', 'its', 'just', 'another', 'piece', 'of', 'teen', 'fluff', '.']\n",
      "['7', ':', 'kids', 'in', 'high', 'school', 'are', 'not', 'that', 'witty', '.']\n",
      "['6', ':', 'the', 'wittiness', 'is', 'not', 'witty', 'enough', '.']\n",
      "['5', ':', 'the', 'comedy', 'is', 'not', 'funny', '.']\n",
      "['4', ':', 'the', 'acting', 'is', 'poor', '.']\n",
      "['3', ':', 'the', 'music', '.']\n",
      "['2', ':', 'the', 'poster', '.']\n",
      "['1', ':', 'its', 'worse', 'than', 'she', \"'\", 's', 'all', 'that', '!']\n",
      "[ '10', '=', 'a', 'classic', '9', '=', 'borderline', 'classic', '8', '=',\n",
      "  'excellent', '7', '=', 'good', '6', '=', 'better', 'than', 'average', '5',\n",
      "  '=', 'average', '4', '=', 'disappointing', '3', '=', 'poor', '2', '=',\n",
      "  'awful', '1', '=', 'a', 'crap', 'classic']\n"
     ]
    }
   ],
   "source": [
    "# Display sentances from an article\n",
    "\n",
    "a_filename = 'neg/cv779_18989.txt'\n",
    "for sent in mvr.sents(a_filename):\n",
    "    pp.pprint(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'uuuuuuggggggglllllllyyyyy', 's_funniest_home_videos_',\n",
      "  '_the_last_days_of_disco_', '_i_know_what_you_did_last_summer_',\n",
      "  '_fear_and_loathing_in_las_vegas_', '_breakfast_of_champions_',\n",
      "  '_breakfast_of_champions_', '_a_night_at_the_roxbury_',\n",
      "  '_a_night_at_the_roxbury_',\n",
      "  '__________________________________________________________',\n",
      "  '____________________________________________', '==========================',\n",
      "  '========================', '=======================',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------']\n"
     ]
    }
   ],
   "source": [
    "# We can process the words with normal Python\n",
    "# For example, print out really long words\n",
    "long_words = [word for word in mvr_words if len(word) > 22]\n",
    "long_words.sort(reverse=True)\n",
    "pp.pprint(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used NLTK to ....\n",
    "\n",
    "Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change the \n",
    "2. Change the \n",
    "3. Try making \n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
