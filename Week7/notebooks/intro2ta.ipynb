{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Text Analysis\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we introduce text analysis, which is one of\n",
    "the most exciting application areas for machine learning. Text analysis\n",
    "forms the basis for [natural language processing][nlp, and is explicitly\n",
    "used for [sentiment analysis][sa], for [language identification][lc],\n",
    "[spelling correction][sc], as well as mining text data from forms, such\n",
    "as [medical informatics][mi]. Before moving into text classification and\n",
    "text mining, in this notenook we focus on basic text analysis tasks such\n",
    "as accessing data, tokenizing a corpus, and computing token frequencies.\n",
    "We demonstrate these tasks by using basic Python concepts and by using\n",
    "functionaility from within the scikit learn library, Finally, we\n",
    "introduce the NLTK library, and use methods within this library to\n",
    "perform these tasks.\n",
    "\n",
    "-----\n",
    "[nlp]: https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "[sa]: https://en.wikipedia.org/wiki/Sentiment_analysis\n",
    "[lc]: https://en.wikipedia.org/wiki/Language_identification\n",
    "[sc]: http://norvig.com/spell-correct.html\n",
    "[mi]: https://en.wikipedia.org/wiki/Health_informatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display all plots inline\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "To get started with text analysis, we need **texts to analyze**. To get\n",
    "started, we will analyze the [twenty newsgroup][tng] data set. We first\n",
    "download this data (scikit learn has built in methods for doing this,\n",
    "however, we have cached a copy locally on our server). The data are made\n",
    "available via a custom object, but we can access the data of interest by\n",
    "using dictionary keys. Before delving into text analysis, we first\n",
    "explore this data over several code cells to understand more about the\n",
    "task at hand.\n",
    "\n",
    "In case you are unaware, one primary use of the early Internet (i.e.,\n",
    "pre-Web) was to share information among interested groups via\n",
    "newsgroups, or bulletin boards. Users could subscribe to these groups to\n",
    "send and receive postings of interest. This text classification problem\n",
    "uses postings to twenty newsgroups, thus the newsgroup is the\n",
    "classification target and the text in the posting is used to make the\n",
    "features. These postings were similar to emails, thus each posting will\n",
    "have a header, the article body, which might quote all or part of a\n",
    "previous message, and  possibly a footer (like an email signature).\n",
    "While we use the entire posting in these Notebooks, you can have the\n",
    "header, quoted text, and the footer removed by scikit learn by including\n",
    "the `remove` attribute, and indicating whether these sections should be\n",
    "removed. This attribute can take one or all of the values: `header`,\n",
    "`footer`, and `quotes`. For example, the following attribute would be \n",
    "used to remove both headers and footers.\n",
    "\n",
    "`remove =('headers', 'footers')`\n",
    "\n",
    "-----\n",
    "\n",
    "[tng]: http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['description', 'filenames', 'target_names', 'DESCR', 'target', 'data'])\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "text = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', remove =('headers', 'footers'))\n",
    "\n",
    "# To learn more about these data, either browse the relevant \n",
    "# scikit learn documentation, or enter help(text) in an IPython code cell\n",
    "\n",
    "# The data can be accessed via Dictionary keys\n",
    "\n",
    "print(text.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0 = alt.atheism\n",
      "Class  1 = comp.graphics\n",
      "Class  2 = comp.os.ms-windows.misc\n",
      "Class  3 = comp.sys.ibm.pc.hardware\n",
      "Class  4 = comp.sys.mac.hardware\n",
      "Class  5 = comp.windows.x\n",
      "Class  6 = misc.forsale\n",
      "Class  7 = rec.autos\n",
      "Class  8 = rec.motorcycles\n",
      "Class  9 = rec.sport.baseball\n",
      "Class 10 = rec.sport.hockey\n",
      "Class 11 = sci.crypt\n",
      "Class 12 = sci.electronics\n",
      "Class 13 = sci.med\n",
      "Class 14 = sci.space\n",
      "Class 15 = soc.religion.christian\n",
      "Class 16 = talk.politics.guns\n",
      "Class 17 = talk.politics.mideast\n",
      "Class 18 = talk.politics.misc\n",
      "Class 19 = talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# Display target names, i.e., the names of the twenty news groups\n",
    "\n",
    "for idx, label in enumerate(text['target_names']):\n",
    "    print('Class {0:2d} = {1}'.format(idx, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Newsgroup: sci.med\n",
      "----------------------------------------------------------------------\n",
      "In article <C5u5LG.C3G@gpu.utcc.utoronto.ca> molnar@Bisco.CAnet.CA (Tom Molnar) writes:\n",
      ">I experienced a sudden numbness in my left arm this morning.  Just after\n",
      ">I completed my 4th set of deep squats.  Today was my weight training\n",
      ">day and I was just beginning my routine.  All of a sudden at the end of\n",
      ">the 4th set my arm felt like it had gone to sleep.  It was cold, turned pale,\n",
      ">and lost 60% of its strength.  The weight I used for squats wasn't that\n",
      ">heavy, I was working hard but not at 100% effort.  I waited for a few \n",
      ">minutes, trying to shake the arm back to life and then continued with\n",
      ">chest exercises (flyes) with lighter dumbells than I normally use.  But\n",
      ">I dropped the left dumbell during the first set, and experienced continued\n",
      ">arm weakness into the second.  So I quit training and decided not to do my\n",
      ">usual hour on the ski machine either.  I'll take it easy for the rest of\n",
      ">the day.\n",
      ">\n",
      ">My arm is *still* somewhat numb and significantly weaker than normal --\n",
      ">my hand still tingles a bit down to the thumb. Color has returned to normal\n",
      ">and it is no longer cold. \n",
      ">\n",
      ">Horrid thoughts of chunks of plaque blocking a major artery course through\n",
      ">my brain.  I'm 34, vegetarian, and pretty fit from my daily exercise\n",
      ">regimen.  So that can't be it.  Could a pinched nerve from the bar\n",
      ">cause these symptoms (I hope)?\n",
      "\n",
      "It likely has nothing to do with \"chunks of plaque\" but it sounds like\n",
      "you may have a neurovascular compromise to your arm and you need medical\n",
      "attention *before* doing any more weight lifting.  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "----------------------------------------------------------------------------\n",
      "Gordon Banks  N3JXP      | \"Skepticism is the chastity of the intellect, and\n",
      "geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too soon.\" \n"
     ]
    }
   ],
   "source": [
    "# Display single message\n",
    "messageID = 251\n",
    "\n",
    "print('Target Newsgroup: {0}'.format(text['target_names'][text['target'][messageID]]))\n",
    "print(70*'-')\n",
    "\n",
    "message = text['data'][messageID]\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can analyze text messages by using basic Python commands. For\n",
    "example, we can find how many times a word appears in a text by using\n",
    "Python `string` functions. One important item to consider, however, is\n",
    "that, by default, Python will search for sequences of characters in a\n",
    "text message. Thus, if a word is also part of larger words, we will\n",
    "over-count the occurrences, as demonstrated in the following code cell.\n",
    "\n",
    "We overcome this limitation by explicitly splitting a text into tokens.\n",
    "By default, in Python this is done at whitespace, but this can be\n",
    "changed. The second code cell demonstrates tokenizing a text string, and\n",
    "using a [`Counter`][pc] to accumulate the number of unique occurrences\n",
    "of each token. Finally, we employ regular expressions to split a text\n",
    "string, and use the resulting tokens to create another `Counter` that\n",
    "can be used to compare the results of regular expression parsing with\n",
    "the default string tokenization. One benefit of using regular\n",
    "expressions is that we can specifically indicate of what a token should\n",
    "be composed, in this case, we state a token is a sequence of one or\n",
    "more alphanumeric characters surrounded by white space. Thus the '>'\n",
    "token is removed.\n",
    "\n",
    "-----\n",
    "[pc]: https://docs.python.org/3.5/library/collections.html#collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression Count: 14\n",
      "Isolated Token Count: 9\n"
     ]
    }
   ],
   "source": [
    "token = 'to'\n",
    "i_token = ' {0} '.format(token)\n",
    "\n",
    "print('Expression Count: {0}'.format(message.count(token)))\n",
    "print('Isolated Token Count: {0}'.format(message.count(i_token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens = 199\n",
      "---------------------------------------------------------------------------\n",
      "Top 40 tokens:\n",
      "---------------------------------------------------------------------------\n",
      "[ ('the', 11), ('of', 9), ('to', 9), ('and', 8), ('a', 7), ('my', 7), ('it', 6),\n",
      "  ('I', 6), ('arm', 5), ('was', 4), ('is', 4), ('>I', 3), ('for', 3),\n",
      "  ('with', 3), ('weight', 3), ('has', 2), ('at', 2), ('training', 2),\n",
      "  ('like', 2), ('continued', 2), ('not', 2), ('that', 2), ('sudden', 2),\n",
      "  ('It', 2), ('you', 2), ('So', 2), ('>my', 2), ('from', 2), ('but', 2),\n",
      "  ('|', 2), ('>and', 2), ('--', 2), ('>the', 2), ('set', 2), ('normal', 2),\n",
      "  ('left', 2), ('>', 2), ('experienced', 2), ('4th', 2), ('than', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Accumulate counts of tokens, using string functionality\n",
    "import collections as cl\n",
    "\n",
    "# Used to print out sequences\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "# Tokenize and create counter\n",
    "words = message.split()\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "# Number of tokens to display from message\n",
    "wc_display = 40\n",
    "\n",
    "# Display results\n",
    "print('Total number of tokens = {0:d}'.format(len(wc)))\n",
    "print(75*'-')\n",
    "print('Top {} tokens:'.format(wc_display))\n",
    "print(75*'-')\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'article', 'C5u5LG', 'C3G', 'gpu', 'utcc', 'utoronto', 'ca', 'molnar', 'Bisco', 'CAnet', 'CA', 'Tom', 'Molnar', 'writes', 'I', 'experienced', 'a', 'sudden', 'numbness', 'in', 'my', 'left', 'arm', 'this', 'morning', 'Just', 'after', 'I', 'completed', 'my', '4th', 'set', 'of', 'deep', 'squats', 'Today', 'was', 'my', 'weight', 'training', 'day', 'and', 'I', 'was', 'just', 'beginning', 'my', 'routine', 'All', 'of', 'a', 'sudden', 'at', 'the', 'end', 'of', 'the', '4th', 'set', 'my', 'arm', 'felt', 'like', 'it', 'had', 'gone', 'to', 'sleep', 'It', 'was', 'cold', 'turned', 'pale', 'and', 'lost', '60', 'of', 'its', 'strength', 'The', 'weight', 'I', 'used', 'for', 'squats', 'wasn', 't', 'that', 'heavy', 'I', 'was', 'working', 'hard', 'but', 'not', 'at', '100', 'effort', 'I', 'waited', 'for', 'a', 'few', 'minutes', 'trying', 'to', 'shake', 'the', 'arm', 'back', 'to', 'life', 'and', 'then', 'continued', 'with', 'chest', 'exercises', 'flyes', 'with', 'lighter', 'dumbells', 'than', 'I', 'normally', 'use', 'But', 'I', 'dropped', 'the', 'left', 'dumbell', 'during', 'the', 'first', 'set', 'and', 'experienced', 'continued', 'arm', 'weakness', 'into', 'the', 'second', 'So', 'I', 'quit', 'training', 'and', 'decided', 'not', 'to', 'do', 'my', 'usual', 'hour', 'on', 'the', 'ski', 'machine', 'either', 'I', 'll', 'take', 'it', 'easy', 'for', 'the', 'rest', 'of', 'the', 'day', 'My', 'arm', 'is', 'still', 'somewhat', 'numb', 'and', 'significantly', 'weaker', 'than', 'normal', 'my', 'hand', 'still', 'tingles', 'a', 'bit', 'down', 'to', 'the', 'thumb', 'Color', 'has', 'returned', 'to', 'normal', 'and', 'it', 'is', 'no', 'longer', 'cold', 'Horrid', 'thoughts', 'of', 'chunks', 'of', 'plaque', 'blocking', 'a', 'major', 'artery', 'course', 'through', 'my', 'brain', 'I', 'm', '34', 'vegetarian', 'and', 'pretty', 'fit', 'from', 'my', 'daily', 'exercise', 'regimen', 'So', 'that', 'can', 't', 'be', 'it', 'Could', 'a', 'pinched', 'nerve', 'from', 'the', 'bar', 'cause', 'these', 'symptoms', 'I', 'hope', 'It', 'likely', 'has', 'nothing', 'to', 'do', 'with', 'chunks', 'of', 'plaque', 'but', 'it', 'sounds', 'like', 'you', 'may', 'have', 'a', 'neurovascular', 'compromise', 'to', 'your', 'arm', 'and', 'you', 'need', 'medical', 'attention', 'before', 'doing', 'any', 'more', 'weight', 'lifting', 'Gordon', 'Banks', 'N3JXP', 'Skepticism', 'is', 'the', 'chastity', 'of', 'the', 'intellect', 'and', 'geb', 'cadre', 'dsl', 'pitt', 'edu', 'it', 'is', 'shameful', 'to', 'surrender', 'it', 'too', 'soon']\n",
      "Total number of tokens = 194\n",
      "---------------------------------------------------------------------------\n",
      "Top 40 tokens:\n",
      "---------------------------------------------------------------------------\n",
      "[ ('the', 13), ('I', 12), ('and', 10), ('of', 9), ('to', 9), ('my', 9),\n",
      "  ('a', 7), ('it', 7), ('arm', 6), ('was', 4), ('is', 4), ('with', 3),\n",
      "  ('for', 3), ('set', 3), ('weight', 3), ('has', 2), ('at', 2), ('training', 2),\n",
      "  ('t', 2), ('like', 2), ('continued', 2), ('not', 2), ('cold', 2),\n",
      "  ('still', 2), ('that', 2), ('sudden', 2), ('It', 2), ('you', 2), ('So', 2),\n",
      "  ('chunks', 2), ('from', 2), ('but', 2), ('squats', 2), ('normal', 2),\n",
      "  ('left', 2), ('experienced', 2), ('4th', 2), ('plaque', 2), ('than', 2),\n",
      "  ('day', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to tokenize\n",
    "import re\n",
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "# tokenize and accumulate token counts\n",
    "words = re.sub(pattern, ' ', message).split()\n",
    "print(words)\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "# Display results\n",
    "print('Total number of tokens = {0:d}'.format(len(wc)))\n",
    "print(75*'-')\n",
    "print('Top {} tokens:'.format(wc_display))\n",
    "print(75*'-')\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The previous cells tokenized a text document, but identical tokens with\n",
    "different case will be treated as distinct. In general, this is not a\n",
    "desirable result since it could undercount the occurrences of an\n",
    "otherwise important token. We can easily convert a text to all lowercase\n",
    "to prevent this, by using the string `lower` method. This is\n",
    "demonstrated int he following code cell, where the total number of\n",
    "tokens has changed, as well as the counts of specific tokens (such as\n",
    "`in`).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens = 186\n",
      "---------------------------------------------------------------------------\n",
      "Top 40 tokens:\n",
      "---------------------------------------------------------------------------\n",
      "[ ('the', 14), ('i', 12), ('and', 10), ('my', 10), ('of', 9), ('to', 9),\n",
      "  ('it', 9), ('a', 7), ('arm', 6), ('was', 4), ('is', 4), ('for', 3),\n",
      "  ('but', 3), ('set', 3), ('with', 3), ('weight', 3), ('in', 2), ('has', 2),\n",
      "  ('at', 2), ('training', 2), ('t', 2), ('so', 2), ('like', 2),\n",
      "  ('continued', 2), ('not', 2), ('ca', 2), ('cold', 2), ('still', 2),\n",
      "  ('that', 2), ('sudden', 2), ('you', 2), ('chunks', 2), ('from', 2),\n",
      "  ('squats', 2), ('normal', 2), ('just', 2), ('left', 2), ('molnar', 2),\n",
      "  ('experienced', 2), ('4th', 2)]\n"
     ]
    }
   ],
   "source": [
    "# We can convert message to lower-case\n",
    "words = re.sub(pattern, ' ', message.lower()).split()\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "# Display results\n",
    "print('Total number of tokens = {0:d}'.format(len(wc)))\n",
    "print(75*'-')\n",
    "print('Top {} tokens:'.format(wc_display))\n",
    "print(75*'-')\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Tokenizing a text document is interesting, but traditional machine\n",
    "learning algorithms operate directly on numerical data. One approach to\n",
    "analyze a text document is to generate a numerical representation of a\n",
    "text document by counting the number of times a word occurs (as we did\n",
    "with the `Counter` collection previously. Another approach is to\n",
    "normalize the token counts by the total number of tokens, which creates\n",
    "a term (or token) frequency. We demonstrate this in the following code\n",
    "cell, where we display the _top_ terms and their frequency in the\n",
    "message.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term        : Frequency\n",
      "-------------------------\n",
      "the         : 0.046\n",
      "i           : 0.039\n",
      "and         : 0.033\n",
      "my          : 0.033\n",
      "of          : 0.029\n",
      "to          : 0.029\n",
      "it          : 0.029\n",
      "a           : 0.023\n",
      "arm         : 0.020\n",
      "was         : 0.013\n",
      "is          : 0.013\n",
      "for         : 0.010\n",
      "but         : 0.010\n",
      "set         : 0.010\n",
      "with        : 0.010\n",
      "weight      : 0.010\n",
      "in          : 0.007\n",
      "has         : 0.007\n",
      "at          : 0.007\n",
      "training    : 0.007\n",
      "t           : 0.007\n",
      "so          : 0.007\n",
      "like        : 0.007\n",
      "continued   : 0.007\n",
      "not         : 0.007\n",
      "ca          : 0.007\n",
      "cold        : 0.007\n",
      "still       : 0.007\n",
      "that        : 0.007\n",
      "sudden      : 0.007\n",
      "you         : 0.007\n",
      "chunks      : 0.007\n",
      "from        : 0.007\n",
      "squats      : 0.007\n",
      "normal      : 0.007\n",
      "just        : 0.007\n",
      "left        : 0.007\n",
      "molnar      : 0.007\n",
      "experienced : 0.007\n",
      "4th         : 0.007\n"
     ]
    }
   ],
   "source": [
    "print('{0:12s}: {1}'.format('Term', 'Frequency'))\n",
    "print(25*'-')\n",
    "\n",
    "t_wc = sum(wc.values())\n",
    "for wt in wc.most_common(wc_display):\n",
    "    print('{0:12s}: {1:4.3f}'.format(wt[0], wt[1]/t_wc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used the twenty newsgroup data to perform\n",
    "some basic text analysis. Now that you have run the Notebook, go back\n",
    "and make the following changes to see how the results change.\n",
    "\n",
    "1. Remove the header, footer, and quoted material. How does this change\n",
    "the results? \n",
    "2. Change your regular expression so that tokens are sequences of two or\n",
    "more characters (i.e., no numbers). How does this change the results?\n",
    "3. Try timing the three technique (you might want a longer text\n",
    "document). Which is the fastest technique? Can you explain why?\n",
    "\n",
    "-----\n",
    "\n",
    "## Bag of Words\n",
    "\n",
    "A simple question about text data mining that you might have is _How\n",
    "does one classify documents made up of words when machine learning\n",
    "algorithms work on numerical data?_ The simple answer is we need to\n",
    "build a numerical summary of a data set that our algorithms can\n",
    "manipulate. A conceptually easy approach  to implement this idea is to\n",
    "identify all possible words in the documents of interest and to track the\n",
    "number of times each words occurs in specific documents. This produces a\n",
    "(very) sparse matrix for our sample of documents, where the columns are\n",
    "the possible words (or tokens) and the rows are different documents. \n",
    "\n",
    "This concept, where one tokenizes documents to build these sparse\n",
    "matrices is more formally known as _bag of words_, because we effectively\n",
    "create the [bag of words][bwd] out of which are documents are\n",
    "constructed. In the bag of words model, each document can be mapped into\n",
    "a vector, where the individual elements correspond to the number of\n",
    "times the words (associated with the particular column) appears in the\n",
    "document.\n",
    "\n",
    "With scikit learn, we can use the [`CountVectorizer`][skcv] to break our\n",
    "document into tokens (in this case words), which are used to construct\n",
    "our _bag of words_ for the given set of documents. Given this tokenizer,\n",
    "we first need to construct the list of tokens, which we do with the\n",
    "`fit` method. Second, we need to transform our documents into this\n",
    "sparse matrix, which we do with the `transform` method. Since both steps\n",
    "use the same input dta, there is a convenience method to perform both\n",
    "operations at the same time, called `fit_transform`.\n",
    "\n",
    "-----\n",
    "[bwd]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "[skcv]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----\n",
    "\n",
    "Given the `CountVectorizer` we can see the number of words in our _bag_\n",
    "as well as the number of documents on which we train, which in this case\n",
    "agrees with the values we obtained when we read in the data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary from our data\n",
    "cv.fit(text['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples = 1\n",
      "Number of Tokens = 114751\n",
      "---------------------------------------------------------------------------\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "---------------------------------------------------------------------------\n",
      "Tuples from Document-Term Matrix[i, j] and c (Count)\n",
      "---------------------------------------------------------------------------\n",
      "[ (0, 1946, 1), (0, 10226, 1), (0, 12557, 2), (0, 14013, 1), (0, 22580, 1),\n",
      "  (0, 23323, 1), (0, 23968, 10), (0, 24384, 1), (0, 25098, 6), (0, 25258, 1),\n",
      "  (0, 25275, 1), (0, 25702, 2), (0, 25884, 1), (0, 26927, 1), (0, 27243, 1),\n",
      "  (0, 27267, 1), (0, 27703, 1), (0, 27886, 1), (0, 27911, 1), (0, 28765, 1),\n",
      "  (0, 28784, 1), (0, 29089, 1), (0, 29888, 1), (0, 30814, 3), (0, 31147, 1),\n",
      "  (0, 31794, 1), (0, 32002, 2), (0, 32069, 1), (0, 32305, 1), (0, 32360, 1),\n",
      "  (0, 32867, 1), (0, 33664, 1), (0, 33807, 1), (0, 34173, 2), (0, 35304, 2),\n",
      "  (0, 35409, 1), (0, 35831, 1), (0, 35915, 1), (0, 36619, 2), (0, 37154, 1),\n",
      "  (0, 37238, 1), (0, 38930, 1), (0, 39275, 2), (0, 39585, 1), (0, 39751, 1),\n",
      "  (0, 42098, 2), (0, 42217, 1), (0, 42451, 1), (0, 42751, 1), (0, 42859, 1),\n",
      "  (0, 43035, 1), (0, 43036, 1), (0, 43115, 1), (0, 43701, 1), (0, 44017, 1),\n",
      "  (0, 44131, 1), (0, 44319, 1), (0, 44984, 1), (0, 46492, 1), (0, 46494, 1),\n",
      "  (0, 46631, 2), (0, 47957, 1), (0, 48089, 1), (0, 48544, 1), (0, 48581, 1),\n",
      "  (0, 48982, 1), (0, 49232, 3), (0, 49871, 2), (0, 51194, 1), (0, 52310, 1),\n",
      "  (0, 52377, 1), (0, 52484, 1), (0, 53819, 1), (0, 54053, 1), (0, 54204, 1),\n",
      "  (0, 54352, 2), (0, 54442, 1), (0, 54722, 1), (0, 55968, 1), (0, 56039, 1),\n",
      "  (0, 56143, 1), (0, 58321, 2), (0, 59350, 1), (0, 59631, 1), (0, 60116, 4),\n",
      "  (0, 60326, 9), (0, 60401, 1), (0, 62261, 2), (0, 65744, 2), (0, 66349, 1),\n",
      "  (0, 66370, 1), (0, 66385, 1), (0, 66416, 2), (0, 66421, 1), (0, 66851, 1),\n",
      "  (0, 67108, 1), (0, 67232, 1), (0, 68806, 1), (0, 69155, 1), (0, 70030, 1),\n",
      "  (0, 70721, 1), (0, 72064, 1), (0, 73079, 2), (0, 73305, 1), (0, 73339, 1),\n",
      "  (0, 74749, 10), (0, 75026, 1), (0, 75719, 1), (0, 75872, 1), (0, 76007, 1),\n",
      "  (0, 76721, 1), (0, 76947, 2), (0, 76953, 1), (0, 77013, 2), (0, 77038, 1),\n",
      "  (0, 77386, 1), (0, 77397, 1), (0, 78264, 9), (0, 78701, 1), (0, 80420, 1),\n",
      "  (0, 82357, 1), (0, 82469, 1), (0, 82677, 2), (0, 84114, 1), (0, 86471, 1),\n",
      "  (0, 88280, 1), (0, 89161, 1), (0, 89327, 1), (0, 90665, 1), (0, 92959, 1),\n",
      "  (0, 93452, 3), (0, 93670, 1), (0, 93696, 1), (0, 94416, 1), (0, 94829, 1),\n",
      "  (0, 94846, 1), (0, 95031, 1), (0, 95492, 2), (0, 95745, 1), (0, 95780, 1),\n",
      "  (0, 95871, 1), (0, 96623, 2), (0, 97441, 2), (0, 97726, 1), (0, 98247, 2),\n",
      "  (0, 98736, 1), (0, 99165, 1), (0, 99797, 1), (0, 100795, 2), (0, 100814, 2),\n",
      "  (0, 100827, 14), (0, 100887, 1), (0, 100985, 1), (0, 101084, 1),\n",
      "  (0, 101149, 1), (0, 101213, 1), (0, 101258, 1), (0, 101501, 1),\n",
      "  (0, 101732, 9), (0, 101761, 1), (0, 101822, 1), (0, 101893, 1),\n",
      "  (0, 102281, 2), (0, 102869, 1), (0, 103120, 1), (0, 105491, 1),\n",
      "  (0, 105494, 1), (0, 105578, 1), (0, 105616, 1), (0, 105664, 1),\n",
      "  (0, 106609, 1), (0, 108450, 1), (0, 108644, 4), (0, 108662, 1),\n",
      "  (0, 108895, 1), (0, 108901, 1), (0, 109003, 3), (0, 109823, 3),\n",
      "  (0, 110198, 1), (0, 110412, 1), (0, 113262, 2), (0, 113277, 1)]\n"
     ]
    }
   ],
   "source": [
    "# We can now process documents.\n",
    "\n",
    "# We needd an iteratable to appply cv.transform()\n",
    "msg = []\n",
    "msg.append(message)\n",
    "\n",
    "# Transform one message, which is easier to comprehend.\n",
    "# By default, scikit learn uses sparse matrices for text processing\n",
    "# What is returned is a Document Term Matrix (dtm)\n",
    "dtm = cv.transform(msg)\n",
    "\n",
    "print('Number of Samples = {0}'.format(dtm.shape[0]))\n",
    "print('Number of Tokens = {0}'.format(dtm.shape[1]))\n",
    "print(75*'-')\n",
    "\n",
    "# We can convert from sparse to dense to explore the document-term matrix\n",
    "# The range was manually chosen to have several non-zero elements\n",
    "print(dtm.todense()[:,45141:45240])\n",
    "print(75*'-')\n",
    "\n",
    "# We can also print only nonzero DTM matrix elements\n",
    "\n",
    "print('Tuples from Document-Term Matrix[i, j] and c (Count)')\n",
    "print(75*'-')\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Find non-zero elements\n",
    "i, j, c = sp.find(dtm)\n",
    "dtm_list = list(zip(i, j, c))\n",
    "pp.pprint(dtm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can grab the words in our _bag of words_ by extracting the\n",
    "_vocabulary_. This allows us to see if words are present in the\n",
    "documents. We can also find which term occurs most frequently, least\n",
    "frequently, as well as the overall top terms.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arbitrary Word (the): Column = 100827\n",
      "Max Word (the): Column = 100827\n",
      "Min Word (100): Column = 1946\n"
     ]
    }
   ],
   "source": [
    "# We can explore the terms in our vocabulary\n",
    "terms = cv.vocabulary_\n",
    "\n",
    "# Look at single term\n",
    "my_word = 'the'\n",
    "print(\"Arbitrary Word ({0}): Column = {1}\".format(my_word, terms[my_word]))\n",
    "\n",
    "from operator import itemgetter\n",
    "max_key = max(dtm_list, key=itemgetter(2))[1]\n",
    "min_key = min(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "x_max = [key for key in terms.keys() if terms[key] == max_key]\n",
    "x_min = [key for key in terms.keys() if terms[key] == min_key]\n",
    "\n",
    "print(\"Max Word ({0}): Column = {1}\".format(x_max[0], max_key))\n",
    "print(\"Min Word ({0}): Column = {1}\".format(x_min[0], min_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: Term in Vocabulary\n",
      "----------------------------------------\n",
      "   14: vocabulary[100827] = the\n",
      "   10: vocabulary[74749] = my\n",
      "   10: vocabulary[23968] = and\n",
      "    9: vocabulary[101732] = to\n",
      "    9: vocabulary[78264] = of\n",
      "    9: vocabulary[60326] = it\n",
      "    6: vocabulary[25098] = arm\n",
      "    4: vocabulary[108644] = was\n",
      "    4: vocabulary[60116] = is\n",
      "    3: vocabulary[30814] = but\n"
     ]
    }
   ],
   "source": [
    "# Number of terms to display\n",
    "top_display = 10\n",
    "\n",
    "# Sort our document term list, and unzip\n",
    "dtm_list.sort(key=itemgetter(2), reverse=True)\n",
    "i, j, c = zip(*dtm_list)\n",
    "\n",
    "# Grab out the keys and values for top terms\n",
    "x_keys = [(k, v) for k, v in terms.items() if terms[k] in j[:top_display]]\n",
    "x_keys.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Grab the data, including counts from DTM list\n",
    "x_counts = dtm_list[:top_display]\n",
    "x_counts.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Now we merge the two lists so we can sort to display terms in order\n",
    "x_merged = []\n",
    "for idx in range(len(x_keys)):\n",
    "    if x_keys[idx][1] != x_counts[idx][1]:\n",
    "        print('Error: column mismatch!')\n",
    "\n",
    "    x_merged.append((x_keys[idx][0], x_keys[idx][1], x_counts[idx][2]))\n",
    "\n",
    "x_merged.sort(key=itemgetter(2), reverse=True)\n",
    "\n",
    "# Print results\n",
    "print('Count: Term in Vocabulary')\n",
    "print(40*'-')\n",
    "for x in x_merged:\n",
    "    print(\"{2:5d}: vocabulary[{0}] = {1}\".format(x[1], x[0], x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used scikit learn to parse our sample text\n",
    "message by using `CountVectorizer`. If you look carefully, you will\n",
    "notice the default results are different than our standard Python text\n",
    "analysis. For example, note how we do not have the single character word\n",
    "`a` in the list of most common items. The reason is the default\n",
    "tokenizer for `CountVectorizer` removes single character words (as they\n",
    "are assumed to be uninformative). You can change this by providing a new\n",
    "regular expression to the `CountVectorizer` via the `token_pattern`\n",
    "parameter. \n",
    "\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the `CountVectorizer` regular expression to the string\n",
    "`'(?u)\\\\b\\\\w+\\\\b'`, how do the results change?\n",
    "2. Try vectorizing a different message (i.e., transform a different\n",
    "message, or messages). How do the results change?\n",
    "3. You can have `CountVectorizer` text processing object remove accented\n",
    "words by setting the `strip_accents` parameter to `True`. Does setting\n",
    "this parameter change the results?\n",
    "\n",
    "-----\n",
    "\n",
    "## NLTK\n",
    "\n",
    "The scikit learn library is a general purpose, Python machine learning\n",
    "library that does include some basic text analysis functionality. Text\n",
    "analysis, however is an extremely large and growing topic. As a result,\n",
    "we will want to explore an additional library for natural language\n",
    "processing. This library, known as [Natural Language ToolKit][nltk], or\n",
    "NLTK, enables a wide range of text analyses either on its own, or in\n",
    "conjunction with scikit learn. The NLTK library is extensive and\n",
    "includes [documentation][nltkd] covering many of the topics we have demonstrated\n",
    "previously.\n",
    "\n",
    "In the rest of this Notebook, we will explore how to use NLTK to perform\n",
    "basic text analysis, in a similar manner as demonstrated earlier via\n",
    "standard Python and the scikit learn library. First we import the\n",
    "library, and tokenize the message. We create an NLTK frequency\n",
    "distribution by passing a list of words to the NLTK `FreqDist` method.\n",
    "\n",
    "-----\n",
    "[nltk]: http://www.nltk.org\n",
    "[nltkd]: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('--', 40), ('>', 22), ('.', 17), ('the', 14), ('i', 12), ('and', 10),\n",
      "  ('my', 10), ('of', 9), ('to', 9), ('it', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a text document\n",
    "words = [word.lower() for word in nltk.word_tokenize(message)]\n",
    "\n",
    "# Count number of occurances for each token\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can clean up the list of tokens by using a regular expression with the\n",
    "`word_tokenize` method. We can reuse our previously defined regular\n",
    "expression `pattern = re.compile(r'[^\\w\\s]')` to identify tokens as one\n",
    "or more alphanumeric characters followed by a whitespace character.\n",
    "Doing this removes the punctuation tokens, as shown below.\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('the', 14), ('i', 12), ('and', 10), ('my', 10), ('of', 9), ('to', 9),\n",
      "  ('it', 9), ('a', 7), ('arm', 6), ('was', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Specify an RE to parse a text document\n",
    "words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', message))]\n",
    "\n",
    "# Count token occurances\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "One measure of the use of tokens in a document is [lexical\n",
    "diversity][ld], which is the fraction of unique tokens, or terms, in a\n",
    "document to the total tokens, or terms in a document. \n",
    "\n",
    "-----\n",
    "[ld]: https://en.wikipedia.org/wiki/Lexical_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message has 186 tokens and 307 words for a lexical diversity of 1.651\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_words = len(words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Message has %i tokens and %i words for a lexical diversity of %0.3f\" % (num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can compute the number of [unique sample values][b], [number of\n",
    "samples outcomes][n], and the [maximum occurring token][m] with simple\n",
    "NLTK statistical functions. We can also iterate through and display the\n",
    "most commonly occurring terms and their counts.\n",
    "\n",
    "-----\n",
    "[b]: http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.B\n",
    "[n]: http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.N\n",
    "[m]: http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bins(tokens) = 186\n",
      "\n",
      "Number of sample outcomes = 307\n",
      "\n",
      "Maximum occuring token = the\n",
      "\n",
      "Term        : Count\n",
      "-------------------------\n",
      "the         :  14.000\n",
      "i           :  12.000\n",
      "and         :  10.000\n",
      "my          :  10.000\n",
      "of          :  9.000\n",
      "to          :  9.000\n",
      "it          :  9.000\n",
      "a           :  7.000\n",
      "arm         :  6.000\n",
      "was         :  4.000\n"
     ]
    }
   ],
   "source": [
    "# Display number of unique tokens (or bins)\n",
    "print('Number of unique bins(tokens) = {0}\\n'.format(counts.B()))\n",
    "print('Number of sample outcomes = {0}\\n'.format(counts.N()))\n",
    "print('Maximum occuring token = {0}\\n'.format(counts.max()))\n",
    "\n",
    "print('{0:12s}: {1}'.format('Term', 'Count'))\n",
    "print(25*'-')\n",
    "\n",
    "for token, freq in counts.most_common(top_display):\n",
    "    print('{0:12s}:  {1:4.3f}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "For some machine learning applications, words that occur rarely are\n",
    "important. For example, in a classification process, words that are\n",
    "uniquely assigned to a particular message should carry considerable\n",
    "weight. Taken to the extreme, words that only occur once in an entire\n",
    "set of documents, or corpus, provide unique insight into the particular\n",
    "text document in which they occur. A word that only occurs once in an\n",
    "entire corpus is known as a [_hapax_][ha]. NLTK has a `hapaxes` method\n",
    "that can be used to quickly find _hapaxes_ in a corpus.\n",
    "\n",
    "-----\n",
    "[ha]: https://en.wikipedia.org/wiki/Hapax_legomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c6656ef2b54e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Hapaxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pp' is not defined"
     ]
    }
   ],
   "source": [
    "# Hapaxes\n",
    "pp.pprint(counts.hapaxes()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can also use NLTK to quickly see the most commonly used tokens.\n",
    "First, we can use the `tabulate` method to display the top tokens and\n",
    "their frequency (i.e., what we did in multiple lines of code earlier).\n",
    "Second, we can visually plot the counts of the top tokens by using the\n",
    "`plot` method. Note, we decorate the traditional plot by using seaborn,\n",
    "but the `plot` method will generate a simple visualization with one\n",
    "function call.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the    i  and   my   of   to   it    a  arm  was \n",
      "  14   12   10   10    9    9    9    7    6    4 \n"
     ]
    }
   ],
   "source": [
    "counts.tabulate(top_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnkAAAGlCAYAAACGHeBoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCGEEEKFJMkTQgghhFCh/wNeoaZdqoh3xQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9471e11f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "sns.set(style=\"white\", font_scale=1.5)\n",
    "sns.despine(offset=5)#, trim=True)\n",
    "counts.plot(top_display, cumulative=False)\n",
    "axs.set_title('Term Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## NLTK Corpus\n",
    "\n",
    "To this point, we have analyzed the twenty newsgroup data that are\n",
    "available from within scikit learn. The NLTK library includes a number\n",
    "of [data sets][nc] that can be downloaded and used directly from within\n",
    "NLTK. One data set that we will use repeatedly in this course will be\n",
    "the NLTK [movie review corpus][mrc]. These data should be available in\n",
    "your Docker container. If not, NLTK provides the `nltk.download()`\n",
    "method to download either all or one particular corpus.\n",
    "\n",
    "In the following code cells, we access the movie review data set,\n",
    "display (part) of the data set's README (or general documentation),\n",
    "before we begin to process the words or terms in the corpus.\n",
    "\n",
    "-----\n",
    "[nc]: http://www.nltk.org/nltk_data/\n",
    "[mrc]: http://www.cs.cornell.edu/people/pabo/movie-review-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access the NLTK movie review data set. \n",
    "\n",
    "mvr = nltk.corpus.movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity Dataset Version 2.0\n",
      "Bo Pang and Lillian Lee\n",
      "\n",
      "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
      "\n",
      "Distributed with NLTK with permission from the authors.\n",
      "\n",
      "=======\n",
      "\n",
      "Introduction\n",
      "\n",
      "This README v2.0 (June, 2004) for the v2.0 polarity dataset comes from\n",
      "the URL http://www.cs.cornell.edu/people/pabo/movie-review-data .\n",
      "\n",
      "=======\n",
      "\n",
      "What's New -- June, 2004\n",
      "\n",
      "This dataset represents an enhancement of the review corpus v1.0\n",
      "described in README v1.1: it contains more reviews, and labels were\n",
      "created with an improved rating-extraction system.\n",
      "\n",
      "=======\n",
      "\n",
      "Citation Info \n",
      "\n",
      "This data was first used in Bo Pang and Lillian Lee,\n",
      "``A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization \n",
      "Based on Minimum Cuts'',  Proceedings of the ACL, 2004.\n",
      "\n",
      "@InProceedings{Pang+Lee:04a,\n",
      "  author =       {Bo Pang and Lillian Lee},\n",
      "  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n",
      "  booktitle =    \"Proceedings of the ACL\",\n",
      "  year =         2004\n",
      "}\n",
      "\n",
      "=======\n",
      "\n",
      "Data Format Summary \n",
      "\n",
      "- review_polarity.tar.gz: contains this readme and  data used in\n",
      "  the experiments described in Pang/Lee ACL 2004.\n",
      "\n",
      "  Specifically:\n",
      "\n",
      "  Within the folder \"txt_sentoken\" are the 2000 processed down-cased\n",
      "  text files used in Pang/Lee ACL 2004; the names of the two\n",
      "  subdirectories in that folder, \"pos\" and \"neg\", indicate the true\n",
      "  classification (sentiment) of the component files according to our\n",
      "  automatic rating classifier (see section \"Rating Decision\" below).\n",
      "\n",
      "  File names consist of a cross-validation tag plus the name of the\n",
      "  original html file.  The ten folds used in the Pang/Lee ACL 2004 paper's\n",
      "  experiments were:\n",
      "\n",
      "     fold 1: files tagged cv000 through cv099, in numerical order\n",
      "     fold 2: files tagged cv100 through cv199, in numerical order     \n",
      "     ...\n",
      "     fold 10: files tagged cv900 through cv999, in numerical order\n",
      "\n",
      "  Hence, the file neg/cv114_19501.txt, for example, was labeled as\n",
      "  negative, served as a member of fold 2, and was extracted from the\n",
      "  file 19501.html in polarity_html.zip (see below).\n",
      "\n",
      "  Each line in each text file corresponds to a single sentence, as\n",
      "  determined by Adwait Ratnaparkhi's sentence boundary detector\n",
      "  MXTERMINATOR.\n",
      " \n",
      "  Preliminary steps were taken to remove rating information from the\n",
      "  text files, but only the rating information upon which the rating\n",
      "  decision was based is guaranteed to have been removed. Thus, if the\n",
      "  original review contains several instances of rating information,\n",
      "  potentially given in different forms, those not recognized as valid\n",
      "  ratings remain part of the review text.\n",
      "\t\n",
      "- polarity_html.zip: The original source files from which the\n",
      "  processed, labeled, and (randomly) selected data in\n",
      "  review_polarity.tar.gz was derived.\n",
      "\n",
      "  Specifically:  \n",
      "\n",
      "  This data consists of unprocessed, unlabeled html files from the\n",
      "  IMDb archive of the rec.arts.movies.reviews newsgroup,\n",
      "  http://reviews.imdb.com/Reviews. The files in review_polarity.tar.gz\n",
      "  represent a processed subset of these files. \n",
      "\n",
      "=======\n",
      "\n",
      "Rating Decision (Appendix A)\n",
      "\n",
      "This section describes how we determined whether a review was positive\n",
      "or negative.\n",
      "\n",
      "The original html files do not have consistent formats -- a review may\n",
      "not have the author's rating with it, and when it does, the rating can\n",
      "appear at different places in the file in different forms.  We only\n",
      "recognize some of the more explicit ratings, which are extracted via a\n",
      "set of ad-hoc rules.  In essence, a file's classification is determined\n",
      "based on the first rating we were able to identify.\n",
      "\n",
      "\n",
      "- In order to obtain more accurate rating decisions, the maximum\n",
      "\trating must be specified explicitly, both for numerical ratings\n",
      "\tand star ratings.  (\"8/10\", \"four out of five\", and \"OUT OF\n",
      "\t****: ***\" are examples of rating indications we recognize.)\n",
      "\n",
      "- With a five-star system (or compatible number systems):\n",
      "\tthree-and-a-half stars and up are considered positive, \n",
      "\ttwo stars and below are considered negative.\n",
      "- With a four-star system (or compatible number system):\n",
      "\tthree stars and up are considered positive, \n",
      "\tone-and-a-half stars and below are considered negative.  \n",
      "- With a letter grade system:\n",
      "\tB or above is considered positive,\n",
      "\tC- or below is considered negative.\n",
      "\n",
      "We attempted to recognize half stars, but they are specified in an\n",
      "especially free way, which makes them difficult to recognize.  Hence,\n",
      "we may lose a half star very occasionally; but this only results in 2.5\n",
      "stars in five star system being categorized as negative, which is \n",
      "still reasonable.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data set README,  remove array bounds to see entire file\n",
    "print(mvr.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review has 39768 tokens and 1583820 words for a lexical diversity of 39.826\n"
     ]
    }
   ],
   "source": [
    "mvr_words = mvr.words()\n",
    "counts  = nltk.FreqDist(mvr_words)\n",
    "num_words = len(mvr_words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Movie Review has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\".format(num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party',\n",
      "  ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an',\n",
      "  'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his',\n",
      "  'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',',\n",
      "  'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?',\n",
      "  'watch']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(mvr.words()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The data are organized into separate files for each movie review. Since\n",
    "these reviews have an associated sentiment:? negative and positive, the\n",
    "reviews are categorized (via a directory structure) into `neg` or `pos`\n",
    "respectively. We can directly access a single review, which can be\n",
    "treated as a single text document. In the next few code cells we\n",
    "directly access the number of files, which can be used to count the\n",
    "number of reviews (assuming one review per file). We also display the\n",
    "contents of a single file, before displaying a subset of the files in one\n",
    "particular category, in this case `neg`, or negative reviews.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of reviews = 2000\n"
     ]
    }
   ],
   "source": [
    "# Each article is in a separate file\n",
    "\n",
    "print('Total Number of reviews = {0}'.format(len(mvr.fileids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example File: neg/cv000_29416.txt\n"
     ]
    }
   ],
   "source": [
    "a_filename = mvr.fileids()[0]\n",
    "print('Example File: {0}'.format(a_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('plot : two teen couples go to a church party , drink and then drive . \\n'\n",
      " 'they get into an accident . \\n'\n",
      " 'one of the guys dies , but his girlfriend continues to see him in her life , '\n",
      " 'and has nightmares . \\n'\n",
      " \"what's the d\")\n"
     ]
    }
   ],
   "source": [
    "# Print part of the file\n",
    "pp.pprint(mvr.raw(a_filename)[:211])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# Display article assigned categories\n",
    "pp.pprint(mvr.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt',\n",
      "  'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt',\n",
      "  'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt',\n",
      "  'neg/cv009_29417.txt', 'neg/cv010_29063.txt', 'neg/cv011_13044.txt',\n",
      "  'neg/cv012_29411.txt', 'neg/cv013_10494.txt', 'neg/cv014_15600.txt',\n",
      "  'neg/cv015_29356.txt', 'neg/cv016_4348.txt', 'neg/cv017_23487.txt',\n",
      "  'neg/cv018_21672.txt', 'neg/cv019_16117.txt']\n"
     ]
    }
   ],
   "source": [
    "# Find articles that have specific category\n",
    "pp.pprint(mvr.fileids('neg')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given the contents of a file, we can process the associated text in the\n",
    "same manner as before. In this case, we tokenize one review into\n",
    "sentences as opposed to the traditional word tokens. After this, we\n",
    "create a list of words that are much longer than normal. As this simple\n",
    "example demonstrates, this can be a useful technique to search for\n",
    "potential problems, since in this case, none of the example shown are\n",
    "actual words.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '9', ':', 'its', 'pathetic', 'attempt', 'at', '\"', 'improving', '\"', 'on',\n",
      "  'a', 'shakespeare', 'classic', '.']\n",
      "['8', ':', 'its', 'just', 'another', 'piece', 'of', 'teen', 'fluff', '.']\n",
      "['7', ':', 'kids', 'in', 'high', 'school', 'are', 'not', 'that', 'witty', '.']\n",
      "['6', ':', 'the', 'wittiness', 'is', 'not', 'witty', 'enough', '.']\n",
      "['5', ':', 'the', 'comedy', 'is', 'not', 'funny', '.']\n",
      "['4', ':', 'the', 'acting', 'is', 'poor', '.']\n",
      "['3', ':', 'the', 'music', '.']\n",
      "['2', ':', 'the', 'poster', '.']\n",
      "['1', ':', 'its', 'worse', 'than', 'she', \"'\", 's', 'all', 'that', '!']\n",
      "[ '10', '=', 'a', 'classic', '9', '=', 'borderline', 'classic', '8', '=',\n",
      "  'excellent', '7', '=', 'good', '6', '=', 'better', 'than', 'average', '5',\n",
      "  '=', 'average', '4', '=', 'disappointing', '3', '=', 'poor', '2', '=',\n",
      "  'awful', '1', '=', 'a', 'crap', 'classic']\n"
     ]
    }
   ],
   "source": [
    "# Display sentances from an article\n",
    "\n",
    "a_filename = 'neg/cv779_18989.txt'\n",
    "for sent in mvr.sents(a_filename):\n",
    "    pp.pprint(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'uuuuuuggggggglllllllyyyyy', 's_funniest_home_videos_',\n",
      "  '_the_last_days_of_disco_', '_i_know_what_you_did_last_summer_',\n",
      "  '_fear_and_loathing_in_las_vegas_', '_breakfast_of_champions_',\n",
      "  '_breakfast_of_champions_', '_a_night_at_the_roxbury_',\n",
      "  '_a_night_at_the_roxbury_',\n",
      "  '__________________________________________________________',\n",
      "  '____________________________________________', '==========================',\n",
      "  '========================', '=======================',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------']\n"
     ]
    }
   ],
   "source": [
    "# We can process the words with normal Python\n",
    "# For example, print out really long words\n",
    "long_words = [word for word in mvr_words if len(word) > 22]\n",
    "long_words.sort(reverse=True)\n",
    "pp.pprint(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used NLTK to access the movie review corpus.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Tabulate the top tokens in the entire movie review corpus.\n",
    "2. Plot the top tokens in the entire movie review corpus.\n",
    "3. Search for words longer than eighteen characters. Did you find any\n",
    "real words?\n",
    "\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
