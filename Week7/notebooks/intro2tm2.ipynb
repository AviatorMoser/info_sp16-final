{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Practical Concepts\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore \n",
    "\n",
    "1. Repeat unigram results (read in data/ set things up.)\n",
    "2. stemming\n",
    "3. Stemming results.\n",
    "4. n-grams\n",
    "5. bigram results\n",
    "6. tri gram results.\n",
    "7. results comparison (grid search?).\n",
    "\n",
    "Sentiment analysis. Movie reviews?\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 2000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = '/home/data_scientist/nltk_data/corpora/movie_reviews'\n",
    "mvr = load_files(data_dir, shuffle = False)\n",
    "print('Number of Reviews: {0}'.format(len(mvr.data)))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(\n",
    "    mvr.data, mvr.target, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "Formally, a [_n-gram_][ngd] is a contiguous sequence of **n** items from a\n",
    "parent sequence of items, such as characters or words in a text\n",
    "document. In general, we will focus solely on words in a document. Thus,\n",
    "our initial approach has simply been to look at unigrams or single\n",
    "words in a document when building a classification model. However,\n",
    "sometimes the combination of words can be more descriptive, for example,\n",
    "_unbelievably bad_ is generally viewed as a more powerful description\n",
    "than just _bad_. As a result, the concept of an _n-gram_ was created,\n",
    "where collections of words can be treated as features. In fact google\n",
    "allows a user to search for [specific n-gram][gnv] combinations in books that\n",
    "they have scanned.\n",
    "\n",
    "While this clearly can improve classification power, it also increases\n",
    "computational requirements. This is a result of the exponential rise in\n",
    "the number of possible features. For example, given $n$ words, we have\n",
    "$n \\times (n - 1)$ possible bigrams, and so on for higher order\n",
    "combinations. While this is not a problem for small vocabularies, for\n",
    "larger vocabularies (and corresponding documents) the number of possible\n",
    "features can quickly become very large. Thus, many text mining\n",
    "applications will make use of Hadoop or Spark clusters to leverage the\n",
    "inherent parallelism in these tasks.\n",
    "\n",
    "To demonstrate using n-grams, the following code example builds a\n",
    "feature vector containing both ingle words and b-grams from the\n",
    "documents. We use this new sparse matrix to classify the documents by\n",
    "using our simple Naive Bayes classifier, which obtains slightly better\n",
    "results.\n",
    "\n",
    "-----\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[ngd]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.71      0.77       259\n",
      "        pos       0.73      0.85      0.79       241\n",
      "\n",
      "avg / total       0.79      0.78      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase and restrict ourselves to about half the available features\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 421010\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.78      0.81       259\n",
      "        pos       0.78      0.83      0.81       241\n",
      "\n",
      "avg / total       0.81      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__min_df=2, \\\n",
    "                cv__max_df=0.5)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 62735\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Student Activity\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.77      0.80       259\n",
      "        pos       0.77      0.83      0.80       241\n",
      "\n",
      "avg / total       0.80      0.80      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__tokenizer=tokenize)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 80529\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Analysis\n",
    "\n",
    "We can also apply clustering analysis to our feature matrix. While\n",
    "finding an unknown number of clusters in text documents can be\n",
    "difficult, we can learn about our data by identifying the clusters for\n",
    "our **known** labels. To demonstrate, in the following code cells, we\n",
    "employ k-means to find twenty clusters in our feature matrix, after\n",
    "which we identify the most frequently used words in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=2, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 2\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', \\\n",
    "                     ngram_range=(1, 3), max_features=100000)\n",
    "\n",
    "train_counts = cv.fit_transform(mvr_train)\n",
    "test_data = cv.transform(mvr_test)\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: movie vacation vegas like good music heart series vegas vacation film music heart doesn directed craven griswold national streep family roberta isn\n",
      "\n",
      "Cluster 1: film movie like just time good story character way characters make does plot really scene life people man little bad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = ['Neg', 'Pos']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='train', shuffle=True, random_state=23)\n",
    "test = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='test', shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=20, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 20\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', max_features=100000)\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: ftp edu cancer file use files server available gopher information mac hiv health number 1993 software faq com medical mail\n",
      "\n",
      "Cluster 1: 00 20 appears 40 art 50 80 10 wolverine 60 1st ghost rider hobgoblin punisher man annual sabretooth appear cover\n",
      "\n",
      "Cluster 2: god people like don know just said say does jesus edu right think time way new did believe christ lord\n",
      "\n",
      "Cluster 3: 92 12 10 hiv 17 11 aids patients et 03 30 medical 25 milk cd4 31 tb 1993 number 04\n",
      "\n",
      "Cluster 4: com subject lines writes organization edu article don just like people know think posting host nntp time does use good\n",
      "\n",
      "Cluster 5: stephanopoulos mr president general did think know attorney just don going decision george said statement yesterday house white responsibility mean\n",
      "\n",
      "Cluster 6: 03 04 02 05 won lost 06 07 idle 08 01 10 09 edu sox berkeley york new chicago san\n",
      "\n",
      "Cluster 7: edu graphics pub mail ray 128 send ftp 3d com server objects amiga rayshade archie image images file files archive\n",
      "\n",
      "Cluster 8: dos windows microsoft tcp ms mouse amiga software pc graphics higher macintosh network mbytes version 00 ip memory support card\n",
      "\n",
      "Cluster 9: said door started went children armenians didn people don balcony know came ira mamma apartment ll going time couldn diana\n",
      "\n",
      "Cluster 10: jpeg image gif file color format images quality version files bit free programs available use jfif software don display edu\n",
      "\n",
      "Cluster 11: image edu data available graphics ftp software pub processing images package format analysis display formats information file version files program\n",
      "\n",
      "Cluster 12: rx remote echo sh pwd shell shar display man host pl file fi command hostname sinclair test set makefile make\n",
      "\n",
      "Cluster 13: venus planet earth solar spacecraft surface kilometers space miles atmosphere sun moon planets years degrees mars jupiter soviet comet mission\n",
      "\n",
      "Cluster 14: mb ma m4 ms mm m1 mx mc mo mp ml mj mt mq m7 mi mg mr mh mu\n",
      "\n",
      "Cluster 15: openwindows use sun xview usr look x11 lib open subject file openwinhome window olit news programs openwin manual fonts bin\n",
      "\n",
      "Cluster 16: president think people going know say said united mr states things believe don good want just did american action administration\n",
      "\n",
      "Cluster 17: slip com driver use packet phone dos file ip cwru echo goto windows qvtnet edu dial pktmux connection net virtual\n",
      "\n",
      "Cluster 18: myers ms president think don dee ll know said going decision does house white today believe justice just board department\n",
      "\n",
      "Cluster 19: edu subject lines organization university writes article posting host nntp like just don know cs think does ca people new\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = test['target']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## DImension Reduction\n",
    "\n",
    "The matrices are big. Lets reduce the number of features. PCA can be difficult given the size. Could use incremental PCA or Truncated SVD. But lets select the best k features.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  82.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_counts = tf.fit_transform(train['data'])\n",
    "test_data = tf.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "clf = nb.fit(train_counts, train['target'])\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "num_k = 10000\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=num_k)\n",
    "xtr = ch2.fit_transform(train_counts, train['target'])\n",
    "xt = ch2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  82.0%\n"
     ]
    }
   ],
   "source": [
    "clf = nb.fit(xtr, train['target'])\n",
    "predicted = clf.predict(xt)\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(xt, test['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names = np.array([feature_names[i] for i in ch2.get_support(indices=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism:\n",
      "[ 'islam', 'islamic', 'say', 'allan', 'wpd', 'cco', 'article', 'solntze', 'don',\n",
      "  'morality', 'sgi', 'schneider', 'people', 'atheism', 'com', 'livesey',\n",
      "  'atheists', 'caltech', 'god', 'keith']\n",
      "comp.graphics:\n",
      "[ 'software', 'does', 'polygon', 'computer', 'version', 'format', 'images',\n",
      "  'looking', 'need', 'help', 'file', 'program', 'com', 'nntp', 'host', 'files',\n",
      "  '3d', 'thanks', 'image', 'graphics']\n",
      "comp.os.ms-windows.misc:\n",
      "[ 'help', 'nntp', 'version', 'ftp', 'host', 'problem', 'program', 'card',\n",
      "  'using', 'com', 'win', 'thanks', 'use', 'drivers', 'ms', 'driver', 'files',\n",
      "  'file', 'dos', 'windows']\n",
      "comp.sys.ibm.pc.hardware:\n",
      "[ 'does', 'monitor', 'computer', 'motherboard', 'nntp', 'host', 'help',\n",
      "  'drives', 'dos', 'disk', 'isa', 'thanks', 'pc', 'com', 'controller', 'bus',\n",
      "  'ide', 'scsi', 'card', 'drive']\n",
      "comp.sys.mac.hardware:\n",
      "[ 'new', 'computer', 'duo', 'use', 'scsi', 'lc', 'com', 'problem', 'does',\n",
      "  'monitor', 'se', 'simms', 'thanks', 'host', 'nntp', 'centris', 'drive',\n",
      "  'quadra', 'apple', 'mac']\n",
      "comp.windows.x:\n",
      "[ 'uk', 'display', 'problem', 'expo', 'windows', 'use', 'using', 'xterm',\n",
      "  'nntp', 'thanks', 'host', 'lcs', 'application', 'widget', 'x11r5', 'server',\n",
      "  'com', 'mit', 'motif', 'window']\n",
      "misc.forsale:\n",
      "[ 'drive', 'computer', 'com', 'email', 'usa', 'asking', 'host', 'nntp', 'sell',\n",
      "  'price', 'interested', 'forsale', 'mail', '00', 'new', 'condition',\n",
      "  'distribution', 'shipping', 'offer', 'sale']\n",
      "rec.autos:\n",
      "[ 'hp', 'drive', 'ca', 'price', 'think', 'ford', 'oil', 'new', 'don', 'good',\n",
      "  'distribution', 'usa', 'dealer', 'host', 'nntp', 'engine', 'article', 'com',\n",
      "  'cars', 'car']\n",
      "rec.motorcycles:\n",
      "[ 'good', 'honda', 'sun', 'don', 'uk', 'motorcycles', 'helmet', 'rider', 'host',\n",
      "  'nntp', 'bmw', 'ca', 'riding', 'bikes', 'article', 'motorcycle', 'ride',\n",
      "  'com', 'dod', 'bike']\n",
      "rec.sport.baseball:\n",
      "[ 'league', 'think', 'host', 'nntp', 'win', 'cs', 'phillies', 'hit', 'com',\n",
      "  'braves', 'season', 'pitching', 'runs', 'article', 'games', 'players', 'team',\n",
      "  'game', 'year', 'baseball']\n",
      "rec.sport.hockey:\n",
      "[ 'pittsburgh', 'detroit', 'wings', 'year', 'playoff', 'playoffs', 'leafs',\n",
      "  'win', 'teams', 'cup', 'games', 'toronto', 'season', 'players', 'play', 'nhl',\n",
      "  'ca', 'game', 'team', 'hockey']\n",
      "sci.crypt:\n",
      "[ 'gtoal', 'wiretap', 'des', 'pgp', 'netcom', 'public', 'algorithm', 'secure',\n",
      "  'secret', 'security', 'nsa', 'crypto', 'government', 'escrow', 'keys', 'com',\n",
      "  'chip', 'encryption', 'key', 'clipper']\n",
      "sci.electronics:\n",
      "[ 'hp', 'work', 'good', 'radio', 'line', 'thanks', 'voltage', 'don', 'phone',\n",
      "  'electronics', 'article', 'ca', 'need', 'does', 'nntp', 'power', 'host',\n",
      "  'circuit', 'use', 'com']\n",
      "sci.med:\n",
      "[ 'cadre', 'intellect', 'dsl', 'skepticism', 'chastity', 'n3jxp', 'food',\n",
      "  'pittsburgh', 'medical', 'science', 'doctor', 'disease', 'article', 'msg',\n",
      "  'cs', 'com', 'gordon', 'banks', 'geb', 'pitt']\n",
      "sci.space:\n",
      "[ 'nsmca', 'prb', 'aurora', 'toronto', 'earth', 'zoo', 'shuttle', 'launch',\n",
      "  'article', 'com', 'access', 'pat', 'alaska', 'gov', 'digex', 'henry', 'orbit',\n",
      "  'moon', 'nasa', 'space']\n",
      "soc.religion.christian:\n",
      "[ 'question', 'don', 'does', 'say', 'think', 'apr', 'christianity', '1993',\n",
      "  'believe', 'athos', 'faith', 'people', 'church', 'bible', 'christ',\n",
      "  'christian', 'rutgers', 'christians', 'jesus', 'god']\n",
      "talk.politics.guns:\n",
      "[ 'host', 'nntp', 'cdt', 'think', 'law', 'control', 'atf', 'government',\n",
      "  'stratus', 'article', 'don', 'firearms', 'waco', 'weapons', 'fbi', 'batf',\n",
      "  'people', 'com', 'guns', 'gun']\n",
      "talk.politics.mideast:\n",
      "[ 'com', 'israelis', 'turkey', 'org', 'policy', 'arabs', 'jewish', 'turks',\n",
      "  'article', 'argic', 'serdar', 'armenia', 'people', 'armenians', 'armenian',\n",
      "  'arab', 'turkish', 'jews', 'israeli', 'israel']\n",
      "talk.politics.misc:\n",
      "[ 'sexual', 'homosexual', 'law', 'news', 'president', 'think', 'tax', 'new',\n",
      "  'kaldis', 'don', 'gay', 'clayton', 'government', 'state', 'optilink',\n",
      "  'clinton', 'people', 'article', 'cramer', 'com']\n",
      "talk.religion.misc:\n",
      "[ 'think', 'christ', 'religion', 'objective', 'say', 'koresh', 'bible',\n",
      "  'christians', 'apple', 'don', 'newton', 'article', 'morality', 'people',\n",
      "  'kent', 'jesus', 'christian', 'com', 'sandvik', 'god']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "top_count = 20\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_names = np.argsort(nb.coef_[idx])[-top_count:]\n",
    "    print('{0}:'.format(target))\n",
    "    pp.pprint([name for name in feature_names[top_names]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
