{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Spark: Machine Learning\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform basic statistical analysis and machine learning. For part of this analysis, we will use the airline data, which has been stored in files that are accesible from within our Spark cluster. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "In this class, we have a dedicated Spark cluster running to allow\n",
    "students to explore Spark from within our IPython Notebook environment.\n",
    "Since our Spark cluster has limited resources, we need to carefully\n",
    "manage them, in particular we need to ensure that any SparkContext\n",
    "previously used by this Jupyter Server is properly released before\n",
    "starting a new one. After this, we will initialize a new SparkContext to\n",
    "properly interact from this dockerized IPython Notebook to the Spark\n",
    "cluster.\n",
    "\n",
    "----- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# We release the SparkContext if it exists.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass ;\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "# Now handle initial import statements\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create new Spark Configuration (port numbers might need to be adjusted from defaults.)\n",
    "myconf = SparkConf()\n",
    "myconf.setMaster('local[*]')\n",
    "myconf.setAppName(\"INFO490 SP16 W14-NB3: Professor Brunner\")\n",
    "myconf.set('spark.executor.memory', '1g')\n",
    "\n",
    "# Create and initialize a new Spark Context\n",
    "sc = SparkContext(conf=myconf)\n",
    "\n",
    "# Display Spark version information, which also verifies SparkContext is active\n",
    "print(\"\\nSpark version: {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "Previously in this Notebook, we have used Spark to create simple RDDs\n",
    "that demonstrated Spark transformations and actions on small data. Now\n",
    "we will change approaches and analyze the airline data, first starting\n",
    "with the single 2001 flight data file. We can create a new RDD by\n",
    "reading in the data as a textfile, after which we execute the RDD\n",
    "creation by counting the number of lines in the RDD. We subsequently\n",
    "apply several other RDD methods to display the first few rows of data by\n",
    "using the `take` method. Finally, we use the built-in `help` to se the\n",
    "list of supported RDD methods.\n",
    "\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/home/data_scientist/data/2001/2001-1.csv'\n",
    "\n",
    "text_file = sc.textFile(filename)\n",
    "\n",
    "col_data = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[4], p[14], p[15], p[16], p[17], p[18])) \\\n",
    "            .filter(lambda line: 'Year' not in line)\n",
    "\n",
    "cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "\n",
    "fields = cols.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), int(p[3]),\n",
    "                          int(p[4]), int(p[5]), p[6], p[7], int(p[8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480106"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be 480106 if everything works correctly\n",
    "fields.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2001, 1, 17, 1806, -3, -4, 'BWI', 'CLT', 361),\n",
       " (2001, 1, 18, 1805, 4, -5, 'BWI', 'CLT', 361),\n",
       " (2001, 1, 19, 1821, 23, 11, 'BWI', 'CLT', 361),\n",
       " (2001, 1, 20, 1807, 10, -3, 'BWI', 'CLT', 361),\n",
       " (2001, 1, 21, 1810, 20, 0, 'BWI', 'CLT', 361)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark Statistics\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "sdt = fields.map(lambda p: (p[2], p[3], p[4], p[5], p[8]))\n",
    "\n",
    "summary = Statistics.colStats(sdt)\n",
    "\n",
    "mus = summary.mean()\n",
    "mns = summary.min()\n",
    "mxs = summary.max()\n",
    "\n",
    "vrs = summary.variance()\n",
    "nnzs = summary.numNonzeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Mean    Variance     Min       Max  Non Zeroes\n",
      "-----------------------------------------------------------------\n",
      "Day            16.01       79.87    1.00     31.00      480106\n",
      "Dep. Time    1359.66   237399.85    1.00   2400.00      480106\n",
      "Arr. Delay      6.38      964.02  -80.00   1688.00      461157\n",
      "Dep. Delay      8.78      782.11  -59.00   1692.00      393503\n",
      "Distance      716.99   323369.33   21.00   4962.00      480106\n"
     ]
    }
   ],
   "source": [
    "cols = ['Day', 'Dep. Time', 'Arr. Delay', 'Dep. Delay', 'Distance']\n",
    "\n",
    "# Print out Header\n",
    "print('{0:>20s}{1:>12s}{2:>8s}{3:>10s}{4:>12s}'\\\n",
    "      .format('Mean', 'Variance', 'Min', 'Max', 'Non Zeroes'))\n",
    "print(65*'-')\n",
    "\n",
    "# Printout summary statistics\n",
    "for idx, (m, v, mn, mx, n) in enumerate(zip(mus, vrs, mns, mxs, nnzs)):\n",
    "    print('{5:10s}{0:10.2f}{1:12.2f}{2:8.2f}{3:10.2f}{4:12d}'\\\n",
    "          .format(m, v, mn, mx, int(n), cols[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Correlations\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [0, 1, 2]\n",
      "y =  [1, 2, 4]\n",
      "z =  [2, 1, 0]\n",
      "\n",
      "Pearson Correlation Tests\n",
      "-------------------------\n",
      "x corr x = +1.000\n",
      "x corr y = +0.982\n",
      "x corr z = -1.000\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Correlation Measurements\n",
    "\n",
    "# Sample Data\n",
    "x = sc.parallelize([0, 1, 2])\n",
    "y = sc.parallelize([1, 2, 4])\n",
    "z = sc.parallelize([2, 1, 0])\n",
    "\n",
    "print('x = ', x.collect())\n",
    "print('y = ', y.collect())\n",
    "print('z = ', z.collect())\n",
    "\n",
    "print('\\nPearson Correlation Tests')\n",
    "print(25*'-')\n",
    "print('x corr x = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, x, method='pearson')))\n",
    "\n",
    "print('x corr y = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, y, method='pearson')))\n",
    "\n",
    "print('x corr z = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, z, method='pearson')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dearture Time, Arrival Delay, Departure Delay\n",
      "\n",
      "Pearson Correlation Matrix:\n",
      "[[ 1.     0.134  0.167]\n",
      " [ 0.134  1.     0.904]\n",
      " [ 0.167  0.904  1.   ]]\n",
      "\n",
      "Spearman Correlation Matrix:\n",
      "[[ 1.     0.109  0.173]\n",
      " [ 0.109  1.     0.616]\n",
      " [ 0.173  0.616  1.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Set print precision of matrices\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Compute correlation of three columns in RDD\n",
    "cd = sdt.map(lambda p: (p[1], p[2], p[3]))\n",
    "\n",
    "print('Dearture Time, Arrival Delay, Departure Delay')\n",
    "\n",
    "print('\\nPearson Correlation Matrix:')\n",
    "print(Statistics.corr(cd, method='pearson'))\n",
    "\n",
    "print('\\nSpearman Correlation Matrix:')\n",
    "print(Statistics.corr(cd, method='spearman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Random Data and Sampling\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "ud = RandomRDDs.uniformRDD(sc, 1000, seed=23)\n",
    "\n",
    "nd = RandomRDDs.normalRDD(sc, 1000, seed=23)\n",
    "\n",
    "pd = RandomRDDs.poissonRDD(sc, mean=2.0, size=1000, seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform Distribution Statistics\n",
      " (count: 1000, mean: 0.495907509202282, stdev: 0.298581265498, max: 0.99957542053, min: 0.000220626980565)\n"
     ]
    }
   ],
   "source": [
    "print('Uniform Distribution Statistics\\n', ud.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Distribution Statistics\n",
      " (count: 1000, mean: -0.01951879687296531, stdev: 0.936332160006, max: 2.76048478382, min: -3.10768336984)\n"
     ]
    }
   ],
   "source": [
    "print('Normal Distribution Statistics\\n', nd.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisson Distribution Statistics\n",
      " (count: 1000, mean: 2.0089999999999995, stdev: 1.45771019068, max: 9.0, min: 0.0)\n"
     ]
    }
   ],
   "source": [
    "print('Poisson Distribution Statistics\\n', pd.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 280, mean: 0.015385485912264357, stdev: 1.00131906706, max: 2.76048478382, min: -2.98699042684)\n"
     ]
    }
   ],
   "source": [
    "# Sample without replacement\n",
    "\n",
    "frac = 0.25\n",
    "\n",
    "ds = nd.sample(False, frac)\n",
    "print(ds.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 254, mean: -0.11978375730478891, stdev: 0.884613297735, max: 2.76048478382, min: -3.10768336984)\n"
     ]
    }
   ],
   "source": [
    "# Sample with replacement\n",
    "ds = nd.sample(True, frac)\n",
    "print(ds.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "a = [0, 1, 2, 3, 4, 5]\n",
    "print(a[0])\n",
    "print(a[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "-----\n",
    "\n",
    "### Linear Modeling\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.regression import LinearRegressionModel\n",
    "\n",
    "# Minimum departure delay\n",
    "min_delay = 5.\n",
    "data = fields.filter(lambda p: p[5] > min_delay).map(lambda p: LabeledPoint(p[4], [p[5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(23.0, [11.0]),\n",
       " LabeledPoint(18.0, [20.0]),\n",
       " LabeledPoint(96.0, [100.0]),\n",
       " LabeledPoint(20.0, [17.0]),\n",
       " LabeledPoint(87.0, [97.0])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ln_model = LinearRegressionWithSGD.train(data, intercept=False, iterations=100, step=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vnp = data.map(lambda lp: (lp.label, float(ln_model.predict(lp.features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23.0, 10.730386646572505),\n",
       " (18.0, 19.5097939028591),\n",
       " (96.0, 97.54896951429551),\n",
       " (20.0, 16.583324817430235),\n",
       " (87.0, 94.62250042886664)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vnp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  15.4\n",
      "MSE = 238.1\n",
      "MAE =  10.8\n",
      "r2 =   0.9\n",
      "EV = 2014.8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "tm = RegressionMetrics(vnp)\n",
    "\n",
    "print('RMSE = {0:5.1f}'.format(tm.rootMeanSquaredError))\n",
    "print('MSE = {0:5.1f}'.format(tm.meanSquaredError))\n",
    "print('MAE = {0:5.1f}'.format(tm.meanAbsoluteError))\n",
    "print('r2 = {0:5.1f}'.format(tm.r2))\n",
    "print('EV = {0:5.1f}'.format(tm.explainedVariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(weights=[0.975489695143], intercept=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(ln_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest #, RandomForestModel\n",
    "\n",
    "rf_model = RandomForest.trainRegressor(data, categoricalFeaturesInfo={}, numTrees=1)\n",
    "\n",
    "#                                    numTrees=25, featureSubsetStrategy=\"auto\",\n",
    "#                                    impurity='variance', maxDepth=10, maxBins=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeEnsembleModel regressor with 1 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 0 <= 74.0)\n",
      "     If (feature 0 <= 30.0)\n",
      "      If (feature 0 <= 16.0)\n",
      "       If (feature 0 <= 11.0)\n",
      "        Predict: 5.726113904806455\n",
      "       Else (feature 0 > 11.0)\n",
      "        Predict: 11.548860895202358\n",
      "      Else (feature 0 > 16.0)\n",
      "       If (feature 0 <= 23.0)\n",
      "        Predict: 17.536144237834172\n",
      "       Else (feature 0 > 23.0)\n",
      "        Predict: 24.667233253496097\n",
      "     Else (feature 0 > 30.0)\n",
      "      If (feature 0 <= 50.0)\n",
      "       If (feature 0 <= 40.0)\n",
      "        Predict: 32.78938220887976\n",
      "       Else (feature 0 > 40.0)\n",
      "        Predict: 42.70787648243455\n",
      "      Else (feature 0 > 50.0)\n",
      "       If (feature 0 <= 64.0)\n",
      "        Predict: 54.16448238547553\n",
      "       Else (feature 0 > 64.0)\n",
      "        Predict: 66.7016348773842\n",
      "    Else (feature 0 > 74.0)\n",
      "     If (feature 0 <= 135.0)\n",
      "      If (feature 0 <= 103.0)\n",
      "       If (feature 0 <= 86.0)\n",
      "        Predict: 77.87184661957619\n",
      "       Else (feature 0 > 86.0)\n",
      "        Predict: 93.02047162477325\n",
      "      Else (feature 0 > 103.0)\n",
      "       Predict: 116.51830491474423\n",
      "     Else (feature 0 > 135.0)\n",
      "      Predict: 195.3330832708177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rf_model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr = rf_model.predict(data.map(lambda x: x.features))\n",
    "pnl = data.map(lambda lp: lp.label).zip(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  22.1\n",
      "MSE = 487.8\n",
      "MAE =  12.0\n",
      "r2 =   0.7\n",
      "EV = 2012.6\n"
     ]
    }
   ],
   "source": [
    "tm = RegressionMetrics(pnl)\n",
    "\n",
    "print('RMSE = {0:5.1f}'.format(tm.rootMeanSquaredError))\n",
    "print('MSE = {0:5.1f}'.format(tm.meanSquaredError))\n",
    "print('MAE = {0:5.1f}'.format(tm.meanAbsoluteError))\n",
    "print('r2 = {0:5.1f}'.format(tm.r2))\n",
    "print('EV = {0:5.1f}'.format(tm.explainedVariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced basic statistical analysis and machine learning with Spark. Now that you have run the Notebook, go back and make the\n",
    "following changes to see how the results change.\n",
    "\n",
    "1. Change the DataFrame to ...\n",
    "\n",
    "2. Use Intercept in Linear regression. Try more columns (extract them and add in after p[5]).\n",
    "\n",
    "3. Use more trees with RF.\n",
    "\n",
    "4. Try doing an LR like in Week 2 Logisitic Regression.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Ending the Spark Session\n",
    "\n",
    "We must stop the `SparkContext` in order to release resources on the\n",
    "instructional cluster before existing this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
