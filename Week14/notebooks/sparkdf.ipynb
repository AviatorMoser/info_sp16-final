{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Spark\n",
    "## DataFrames, SQL, and Basic Data Analysis\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform data\n",
    "processing in a similar maner to our previous efforts with Pandas. For\n",
    "this we will use the airline data, which has been stored in an HDFS\n",
    "system that is accesible from within our Spark cluster. [Other][dw]\n",
    "tutorials exist, although they often focus on Scala examples since Spark\n",
    "is written for that language.\n",
    "\n",
    "-----\n",
    "[sp]: http://spark.apache.org\n",
    "[sh]: http://hadoop.apache.org\n",
    "[sy]: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\n",
    "[shdfs]: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html\n",
    "[sce]: http://techcrunch.com/2015/07/12/spark-and-hadoop-are-friends-not-foes/\n",
    "[dw]: https://github.com/deanwampler/spark-workshop/tree/master/tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# We release the SparkContext if it exists.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass ;\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "# Now handle initial import statements\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create new Spark Configuration (port numbers might need to be adjusted from defaults.)\n",
    "myconf = SparkConf()\n",
    "myconf.setMaster('local[*]')\n",
    "myconf.setAppName(\"INFO490 SP16 W14-NB2: Professor Brunner\")\n",
    "myconf.set('spark.executor.memory', '1g')\n",
    "\n",
    "# Create and initialize a new Spark Context\n",
    "sc = SparkContext(conf=myconf)\n",
    "\n",
    "# Display Spark version information, which also verifies SparkContext is active\n",
    "print(\"\\nSpark version: {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "Previously in this Notebook, we have used Spark to create simple RDDs\n",
    "that demonstrated Spark transformations and actions on small data. Now\n",
    "we will change approaches and analyze the airline data, first starting\n",
    "with the single 2001 flight data file. We can create a new RDD by\n",
    "reading in the data as a textfile, after which we execute the RDD\n",
    "creation by counting the number of lines in the RDD. We subsequently\n",
    "apply several other RDD methods to display the first few rows of data by\n",
    "using the `take` method. Finally, we use the built-in `help` to se the\n",
    "list of supported RDD methods.\n",
    "\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"hdfs://10.0.3.113:9000/home/ubuntu/data/2001.csv\")\n",
    "\n",
    "col_data = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[4], p[14], p[15], p[16], p[17], p[18])) \\\n",
    "            .filter(lambda line: 'Year' not in line)\n",
    "\n",
    "cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "\n",
    "fields = cols.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), int(p[3]),\n",
    "                          int(p[4]), int(p[5]), p[6], p[7], int(p[8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5723673"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Spark DataFrame\n",
    "\n",
    "Spark supports a simplified [Data Frame][spdf] as part of the [Spark\n",
    "SQL][spsql] library. We can create a Data Frame from an existing RDD by\n",
    "also specifying the column labels and data types. The data types must\n",
    "be one of the pre-defined [Spark SQL types][spdt]. After creating the\n",
    "new DataFrame (which is backed by an RDD), we can perform many of the\n",
    "same tasks with Spark that we performed with Pandas (but not all, and\n",
    "not in as simple of an approach). The following code cells show how we\n",
    "can take our 2001 flight data RDD and create a new Data Frame, which we\n",
    "subsequently use in several subsequent code cells.\n",
    "\n",
    "-----\n",
    "[spdf]: https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\n",
    "[spsql]: https://spark.apache.org/sql/\n",
    "[spdt]: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "schemaString = \"Year Month DayOfMonth DepTime ArrDelay DepDelay Origin Destination Distance\"\n",
    "\n",
    "fieldTypes = [IntegerType(), IntegerType(), IntegerType(), IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              StringType(), StringType(), IntegerType()]\n",
    "\n",
    "f_data = [StructField(field_name, field_type, True) \\\n",
    "          for field_name, field_type in zip(schemaString.split(), fieldTypes)]\n",
    "\n",
    "schema = StructType(f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Month: int, DayOfMonth: int, DepTime: int, ArrDelay: int, DepDelay: int, Origin: string, Destination: string, Distance: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "In the following three code cells, we `show` the first few lines of the\n",
    "DataFrame, then use the `head` method, which displays more syntactic\n",
    "information for each row, and finally use the `describe` method, which\n",
    "doesn't execute until the `show` action is invoked. While the output is\n",
    "less visually attractive than the Pandas result, we still obtain the\n",
    "necessary information.\n",
    "\n",
    "After these code cells, we access the DataFrame schema, first by using\n",
    "the `printSchema` method to nicely output the schema, and next access a\n",
    "column directly, which we can now do since we have named our DataFrame\n",
    "columns.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|Year|Month|DayOfMonth|DepTime|ArrDelay|DepDelay|Origin|Destination|Distance|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|2001|    1|        17|   1806|      -3|      -4|   BWI|        CLT|     361|\n",
      "|2001|    1|        18|   1805|       4|      -5|   BWI|        CLT|     361|\n",
      "|2001|    1|        19|   1821|      23|      11|   BWI|        CLT|     361|\n",
      "|2001|    1|        20|   1807|      10|      -3|   BWI|        CLT|     361|\n",
      "|2001|    1|        21|   1810|      20|       0|   BWI|        CLT|     361|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year=2001, Month=1, DayOfMonth=17, DepTime=1806, ArrDelay=-3, DepDelay=-4, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=18, DepTime=1805, ArrDelay=4, DepDelay=-5, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=19, DepTime=1821, ArrDelay=23, DepDelay=11, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=20, DepTime=1807, ArrDelay=10, DepDelay=-3, Origin='BWI', Destination='CLT', Distance=361)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|   Year|            Month|       DayOfMonth|           DepTime|          ArrDelay|          DepDelay|         Distance|\n",
      "+-------+-------+-----------------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|5723673|          5723673|          5723673|           5723673|           5723673|           5723673|          5723673|\n",
      "|   mean| 2001.0|6.291580773394986|15.71320251873229|1348.6880443729053| 5.528248731190619| 8.115271609681406| 735.173682004545|\n",
      "| stddev|    0.0|3.381754330822876|8.827993155975875|   482.63871515896|31.429288422846703|28.234080794004345|574.8151318384248|\n",
      "|    min|   2001|                1|                1|                 1|             -1116|              -204|               21|\n",
      "|    max|   2001|               12|               31|              2400|              1688|              1692|             4962|\n",
      "+-------+-------+-----------------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'Year'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can extract data from the DataFrame by using similar techniques to\n",
    "what we used with Pandas. One difference is that we need to `filter` the\n",
    "DataFrame, as opposed to directly access rows. However, we can filter\n",
    "rows to extract flights that left O'Hare, and secondly those flights\n",
    "that left O'Hare more than two hours late. In the second case, we also\n",
    "tranform the output to `select` the _Destination_ column and a new\n",
    "column that is the _Distance_ in kilometers.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321784"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Destination| (Distance * 1.6)|\n",
      "+-----------+-----------------+\n",
      "|        PHL|           1084.8|\n",
      "|        CLT|958.4000000000001|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        STL|            412.8|\n",
      "|        STL|            412.8|\n",
      "|        PVD|           1358.4|\n",
      "|        LAX|           2792.0|\n",
      "|        LAX|           2792.0|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').filter(df['DepDelay'] > 120).select(df['Destination'], df['Distance'] * 1.6).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "Given a Spark DataFrame, we can apply SQL statements directly against\n",
    "the DataFrame by registering the DataFrame as a Spark temporary SQL\n",
    "table. The following code cells demonstrates this, as we register our\n",
    "DataFrame as a `flights` table, and execute a SQL statement to select\n",
    "the same data we obtained from our previous DataFrame filter.Since the\n",
    "data are unordered, we have different results displayed via the `show`\n",
    "method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|Destination|Distance|\n",
      "+-----------+--------+\n",
      "|        PHL|     678|\n",
      "|        CLT|     599|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        STL|     258|\n",
      "|        STL|     258|\n",
      "|        PVD|     849|\n",
      "|        LAX|    1745|\n",
      "|        LAX|    1745|\n",
      "+-----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "\n",
    "df.registerTempTable(\"flights\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = sqlContext.sql(\"SELECT Destination, Distance FROM flights WHERE Origin = 'ORD' AND DepDelay > 120\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Breakout Session\n",
    "\n",
    "During this breakout, you should work with the previous Spark examples\n",
    "in order to better learn how Spark works, and how it is different than\n",
    "pure Python approaches like Pandas. Specific problems you can attempt\n",
    "include the following:\n",
    "\n",
    "1. Change the `myRDD` example to start with all integers from 0 to 199.\n",
    "Use an appropriate lambda function to convert this RDD to a new RDD that\n",
    "has all odd integers from 1 to 399.\n",
    "\n",
    "2. Filter the previous RDD to contain only entries that are divisible by\n",
    "9.\n",
    "\n",
    "3. Convert this RDD to a Spark DataFrame, specify the column name as\n",
    "`Numbers`.\n",
    "\n",
    "4. Add an index column to this Spark DataFrame, which sequentially\n",
    "increases.\n",
    "\n",
    "Additional, more advanced problems:\n",
    "\n",
    "1. Create an RDD containing the 'Year', 'Month', 'DayofMonth', 'dDelay',\n",
    "and 'Origin' columns for the airline data for all years 1990-2005.\n",
    "\n",
    "2. Filter this RDD to contain only flight data for flights leaving O'Hare\n",
    "airport.\n",
    "\n",
    "3. Implement a linear fit to the airline flight data in this RDD.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Ending the Spark Session\n",
    "\n",
    "We must stop the `SparkContext` in order to release resources on the\n",
    "instructional cluster before existing this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional References\n",
    "\n",
    "2. [Official Spark Documentation][osd] .\n",
    "5. [Spark][sn] for Data Science Notebook.\n",
    "3. [Pandas and Spark][psd1] Comparison.\n",
    "3. Another [Pandas & Spark ][psd1] Comparison.\n",
    "8. [IPython Spark][ipys] Docker image to simplify learning.\n",
    "-----\n",
    "[osd]: https://spark.apache.org/docs/latest/index.html\n",
    "[sn]: https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/spark/spark.ipynb\n",
    "[psd1]: https://github.com/christophebourguignat/notebooks/blob/master/Spark-Pandas-Differences.ipynb\n",
    "[psd2]: https://lab.getbase.com/pandarize-spark-dataframes/\n",
    "[ipys]: https://github.com/Lab41/ipython-spark-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return to the [Week Two](index.ipynb) index.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
