{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Spark: DataFrames\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform data\n",
    "processing in a similar maner to our previous efforts with Pandas. For\n",
    "this we will use the airline data, which has been stored in an HDFS\n",
    "system that is accesible from within our Spark cluster. [Other][dw]\n",
    "tutorials exist, although they often focus on Scala examples since Spark\n",
    "is written for that language.\n",
    "\n",
    "-----\n",
    "[sp]: http://spark.apache.org\n",
    "[sh]: http://hadoop.apache.org\n",
    "[sy]: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\n",
    "[shdfs]: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html\n",
    "[sce]: http://techcrunch.com/2015/07/12/spark-and-hadoop-are-friends-not-foes/\n",
    "[dw]: https://github.com/deanwampler/spark-workshop/tree/master/tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup pySpark to be able to work with Cassandra\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages TargetHolding:pyspark-cassandra:0.3.5 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# We release the SparkContext if it exists.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass ;\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "# Now handle initial import statements\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create new Spark Configuration (port numbers might need to be adjusted from defaults.)\n",
    "myconf = SparkConf()\n",
    "myconf.setMaster('local[*]')\n",
    "myconf.setAppName(\"INFO490 SP16 W14-NB2: Professor Brunner\")\n",
    "myconf.set('spark.executor.memory', '1g')\n",
    "myconf.set('spark.cassandra.connection.host', '40.124.12.119')\n",
    "\n",
    "# Create and initialize a new Spark Context\n",
    "sc = SparkContext(conf=myconf)\n",
    "\n",
    "# Display Spark version information, which also verifies SparkContext is active\n",
    "print(\"\\nSpark version: {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "In this Notebook, we will need sample data. To simplify acquiring data\n",
    "to demonstrate using Spark DataFrames, we include the RDD code from the\n",
    "[Introduction to Spark](intro2spark.ipynb) Notebook in the following\n",
    "cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/home/data_scientist/data/2001/2001-1.csv'\n",
    "\n",
    "text_file = sc.textFile(filename)\n",
    "\n",
    "col_data = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[4], p[14], p[15], p[16], p[17], p[18])) \\\n",
    "            .filter(lambda line: 'Year' not in line)\n",
    "\n",
    "cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "\n",
    "fields = cols.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), int(p[3]),\n",
    "                          int(p[4]), int(p[5]), p[6], p[7], int(p[8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480106"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be 480106 if everything works correctly\n",
    "fields.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark DataFrame\n",
    "\n",
    "Spark supports a simplified [Data Frame][spdf] as part of the [Spark\n",
    "SQL][spsql] library. We can create a Data Frame from an existing RDD by\n",
    "also specifying the column labels and data types. The data types must\n",
    "be one of the pre-defined [Spark SQL types][spdt]. After creating the\n",
    "new DataFrame (which is backed by an RDD), we can perform many of the\n",
    "same tasks with Spark that we performed with Pandas (but not all, and\n",
    "not in as simple of an approach). The following code cells show how we\n",
    "can take our 2001 flight data RDD and create a new Data Frame, which we\n",
    "subsequently use in several subsequent code cells.\n",
    "\n",
    "-----\n",
    "[spdf]: https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\n",
    "[spsql]: https://spark.apache.org/sql/\n",
    "[spdt]: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "schemaString = \"Year Month DayOfMonth DepTime ArrDelay DepDelay Origin Destination Distance\"\n",
    "\n",
    "fieldTypes = [IntegerType(), IntegerType(), IntegerType(), IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              StringType(), StringType(), IntegerType()]\n",
    "\n",
    "f_data = [StructField(field_name, field_type, True) \\\n",
    "          for field_name, field_type in zip(schemaString.split(), fieldTypes)]\n",
    "\n",
    "schema = StructType(f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Month: int, DayOfMonth: int, DepTime: int, ArrDelay: int, DepDelay: int, Origin: string, Destination: string, Distance: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "In the following three code cells, we `show` the first few lines of the\n",
    "DataFrame, then use the `head` method, which displays more syntactic\n",
    "information for each row, and finally use the `describe` method, which\n",
    "doesn't execute until the `show` action is invoked. While the output is\n",
    "less visually attractive than the Pandas result, we still obtain the\n",
    "necessary information.\n",
    "\n",
    "After these code cells, we access the DataFrame schema, first by using\n",
    "the `printSchema` method to nicely output the schema, and next access a\n",
    "column directly, which we can now do since we have named our DataFrame\n",
    "columns.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|Year|Month|DayOfMonth|DepTime|ArrDelay|DepDelay|Origin|Destination|Distance|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|2001|    1|        17|   1806|      -3|      -4|   BWI|        CLT|     361|\n",
      "|2001|    1|        18|   1805|       4|      -5|   BWI|        CLT|     361|\n",
      "|2001|    1|        19|   1821|      23|      11|   BWI|        CLT|     361|\n",
      "|2001|    1|        20|   1807|      10|      -3|   BWI|        CLT|     361|\n",
      "|2001|    1|        21|   1810|      20|       0|   BWI|        CLT|     361|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year=2001, Month=1, DayOfMonth=17, DepTime=1806, ArrDelay=-3, DepDelay=-4, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=18, DepTime=1805, ArrDelay=4, DepDelay=-5, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=19, DepTime=1821, ArrDelay=23, DepDelay=11, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=20, DepTime=1807, ArrDelay=10, DepDelay=-3, Origin='BWI', Destination='CLT', Distance=361)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
      "|summary|                Year| Month|       DayOfMonth|           DepTime|         ArrDelay|         DepDelay|         Distance|\n",
      "+-------+--------------------+------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
      "|  count|              480106|480106|           480106|            480106|           480106|           480106|           480106|\n",
      "|   mean|              2001.0|   1.0|16.01370530674476| 1359.660206287778|6.382288494624103|8.781523246949632|716.9933556339641|\n",
      "| stddev|1.065161224168148...|   0.0|  8.9369643824565|487.23695943583846|31.04865060768917|27.96630068676185|568.6557196351711|\n",
      "|    min|                2001|     1|                1|                 1|              -80|              -59|               21|\n",
      "|    max|                2001|     1|               31|              2400|             1688|             1692|             4962|\n",
      "+-------+--------------------+------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'Year'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can extract data from the DataFrame by using similar techniques to\n",
    "what we used with Pandas. One difference is that we need to `filter` the\n",
    "DataFrame, as opposed to directly access rows. However, we can filter\n",
    "rows to extract flights that left O'Hare, and secondly those flights\n",
    "that left O'Hare more than two hours late. In the second case, we also\n",
    "tranform the output to `select` the _Destination_ column and a new\n",
    "column that is the _Distance_ in kilometers.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27455"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Destination| (Distance * 1.6)|\n",
      "+-----------+-----------------+\n",
      "|        PHL|           1084.8|\n",
      "|        CLT|958.4000000000001|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        STL|            412.8|\n",
      "|        STL|            412.8|\n",
      "|        PVD|           1358.4|\n",
      "|        LAX|           2792.0|\n",
      "|        LAX|           2792.0|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').filter(df['DepDelay'] > 120).select(df['Destination'], df['Distance'] * 1.6).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "Given a Spark DataFrame, we can apply SQL statements directly against\n",
    "the DataFrame by registering the DataFrame as a Spark temporary SQL\n",
    "table. The following code cells demonstrates this, as we register our\n",
    "DataFrame as a `flights` table, and execute a SQL statement to select\n",
    "the same data we obtained from our previous DataFrame filter.Since the\n",
    "data are unordered, we have different results displayed via the `show`\n",
    "method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|Destination|Distance|\n",
      "+-----------+--------+\n",
      "|        PHL|     678|\n",
      "|        CLT|     599|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        STL|     258|\n",
      "|        STL|     258|\n",
      "|        PVD|     849|\n",
      "|        LAX|    1745|\n",
      "|        LAX|    1745|\n",
      "+-----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "\n",
    "df.registerTempTable(\"flights\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = sqlContext.sql(\"SELECT Destination, Distance FROM flights WHERE Origin = 'ORD' AND DepDelay > 120\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Cassandra Query\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"bigdog\", table=\"airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "|month|dayofmonth|deptime|origin|arrdelay|depdelay|destination|distance|year|\n",
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "|    1|         1|      1|   ANC|     -13|      -4|        LAX|    2345|2001|\n",
      "|    1|         1|      1|   LAS|      33|      41|        PHL|    2176|2001|\n",
      "|    1|         1|      1|   MCI|      14|      11|        MDW|     405|2001|\n",
      "|    1|         1|      1|   RIC|       1|       2|        ORF|      75|2001|\n",
      "|    1|         1|      2|   DTW|      22|      52|        LAX|    1979|2001|\n",
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('month', 'int'),\n",
       " ('dayofmonth', 'int'),\n",
       " ('deptime', 'int'),\n",
       " ('origin', 'string'),\n",
       " ('arrdelay', 'int'),\n",
       " ('depdelay', 'int'),\n",
       " ('destination', 'string'),\n",
       " ('distance', 'int'),\n",
       " ('year', 'int')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bwi = flights.filter(flights.origin == 'BWI').filter(flights.distance > 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "|month|dayofmonth|deptime|origin|arrdelay|depdelay|destination|distance|year|\n",
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "|    1|         1|    655|   BWI|     -37|      -5|        SFO|    2457|2001|\n",
      "|    1|         1|    658|   BWI|     -40|      -2|        SFO|    2457|2001|\n",
      "|    1|         1|    715|   BWI|     -25|       0|        LAS|    2106|2001|\n",
      "|    1|         1|    731|   BWI|     -36|      -4|        LAX|    2329|2001|\n",
      "|    1|         1|    751|   BWI|      12|      19|        PHX|    1999|2001|\n",
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bwi.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Spark DataFrames, Spark SQL, and Basic Statistics with Spark. Now that you have run the Notebook, go back and make the\n",
    "following changes to see how the results change.\n",
    "\n",
    "1. Change the DataFrame to ...\n",
    "\n",
    "2. New SQL query\n",
    "\n",
    "3. Compute Statistics for Poisson. Now swithc to lognormal and calculate statistics.\n",
    "`Numbers`.\n",
    "\n",
    "4. Add an index column to this Spark DataFrame, which sequentially\n",
    "increases.\n",
    "\n",
    "Additional, more advanced problems:\n",
    "\n",
    "1. Create a DataFrame containing the 'Year', 'Month', 'DayofMonth', 'dDelay',\n",
    "and 'Origin' columns for the airline data.\n",
    "\n",
    "2. Filter this DataFrame to contain only flight data for flights leaving Willard airport.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Ending the Spark Session\n",
    "\n",
    "We must stop the `SparkContext` in order to release resources on the\n",
    "instructional cluster before existing this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
