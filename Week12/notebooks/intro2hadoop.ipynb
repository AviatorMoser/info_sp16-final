{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Hadoop\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "----- \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this Notebook, we will demonstrate how to run a Hadoop Streaming\n",
    "Map/Reduce job in a docker container. Our setup will be using a single\n",
    "Hadoop node, which will not be very fast, especially when compared to\n",
    "simply running the map/reduce Python code directly. However, the full\n",
    "Hadoop process will be demonstrated, including the use of the Hadoop\n",
    "file system (HDFS) and the Hadoop Streaming process model. Before\n",
    "proceeding with this Notebook, be sure to (at least start to) download\n",
    "the SequenceIQ Hadoop Docker container.\n",
    "\n",
    "Typically, basic Hadoop is operated on a large cluster that runs both\n",
    "Hadoop and HDFS, although with the development of Yarn, more diverse\n",
    "workflows are now possible. In this Notebook, we only explore the basic\n",
    "Hadoop components of Hadoop and HDFS, which work together to run code on\n",
    "the nodes that hold the relevant data in order to maximize throughput.\n",
    "[Other resources][hort] exist to learn more about Yarn and other Hadoop\n",
    "workflows. The basic Hadoop task is a map/reduce process, where a map\n",
    "process analyzes data and creates a sequential list of key-value pairs\n",
    "(like a Python dictionary). The Hadoop process model sorts the output of\n",
    "the mappers before passing the results to a reduce process. The reduce\n",
    "process combines the key-value pairs to generate final output. The\n",
    "prototype map/reduce example is the [word-count problem][wcp], where a\n",
    "large corpus is analyzed to quantify how many times each word appears\n",
    "(one can quickly see how this model can be extended to analyze website\n",
    "as opposed to texts).\n",
    "\n",
    "Thus to complete a map/reduce task in Hadoop we need to complete the\n",
    "following tasks:\n",
    "\n",
    "1. create a Map program\n",
    "2. create a Reduce program\n",
    "3. obtain a data set to analyze\n",
    "4. load our data into HDFS\n",
    "5. execute our map/reduce program by using Hadoop\n",
    "\n",
    "The rest of this Notebook will demonstrate how to perform each of these\n",
    "tasks. We first will create the two Python programs, download a sample\n",
    "text, and also download the hadoop-streaming jar file into a shared\n",
    "local directory from within our course4 Docker container. Once these\n",
    "steps are complete, we will start our Hadoop Docker container to\n",
    "complete the rest of the process.\n",
    "\n",
    "In the next code cell, we start the process by running a shell script\n",
    "that creates (and deletes first if it exists) the shared directory that\n",
    "will hold the Python codes and data for our Map/Reduce Hadoop project.\n",
    "\n",
    "-----\n",
    "[hort]: http://hortonworks.com/hadoop-tutorial/introducing-apache-hadoop-developers/\n",
    "[wcp]: https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\n",
    "\n",
    "-----\n",
    "\n",
    "https://github.com/sequenceiq/hadoop-docker\n",
    "\n",
    "https://github.com/UI-DataScience/docker-info490/blob/master/hadoop/example.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "# A Bash Shell Script to delete the Hadoop diorectory if it exists, afterwhich\n",
    "# make a new Hadoop directory\n",
    "\n",
    "# Our directory name\n",
    "DIR=/home/data_scientist/rppdm/hadoop\n",
    "\n",
    "# Delete if exists\n",
    "if [ -d \"$DIR\" ]; then\n",
    "    rm -rf \"$DIR\"\n",
    "fi\n",
    "\n",
    "# Now make the directory\n",
    "mkdir \"$DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data to process\n",
    "\n",
    "Our simple map/reduce programs require text data to operate. While there\n",
    "are a number of possible options, for this example we can grab a free\n",
    "book from [Project Gutenberg][pg]:\n",
    "\n",
    "    wget --directory-prefix=/notebooks/rppds/hadoop/ --output-document=book.txt \\\n",
    "        http://www.gutenberg.org/cache/epub/4300/pg4300.txt`\n",
    "\n",
    "In this case, we have grabbed the full text of the novel _Ulysses_, by\n",
    "James Joyce.\n",
    "\n",
    "-----\n",
    "[pg]: http://www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-04-05 23:46:45--  http://www.gutenberg.org/cache/epub/4300/pg4300.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1573151 (1.5M) [text/plain]\n",
      "Saving to: ‘/home/data_scientist/rppdm/hadoop/book.txt’\n",
      "\n",
      "100%[======================================>] 1,573,151   1.11MB/s   in 1.3s   \n",
      "\n",
      "2016-04-05 23:46:47 (1.11 MB/s) - ‘/home/data_scientist/rppdm/hadoop/book.txt’ saved [1573151/1573151]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grab a book to process\n",
    "!wget --output-document=/home/data_scientist/rppdm/hadoop/book.txt \\\n",
    "http://www.gutenberg.org/cache/epub/4300/pg4300.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Bayesian Hierarchical modeling.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the model parameters, rerun the Notebook and see how the\n",
    "different (unpooled versus pooled) modeling approaches perform.\n",
    "2. Change the number of points in each bin, rerun the Notebook and see\n",
    "how the different (unpooled versus pooled) modeling approaches perform.\n",
    "3. Change the model for the prior distributions, e.g., try a uniform\n",
    "distribution for the intercept. How do the different (unpooled versus\n",
    "pooled) modeling approaches perform?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop\r\n"
     ]
    }
   ],
   "source": [
    "!echo $HADOOP_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping datanode\n",
      "stopping namenode\n",
      "starting datanode, logging to /usr/local/hadoop/logs/hadoop--datanode-83a8dc7241b5.out\n",
      "starting namenode, logging to /usr/local/hadoop/logs/hadoop--namenode-83a8dc7241b5.out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# This script is necessary to work around a bug in the distribution.\n",
    "\n",
    "# Simplify command line, cd to root HADOOP directory\n",
    "cd $HADOOP_PREFIX\n",
    "\n",
    "# Stop nodes in case they are already running.\n",
    "sbin/hadoop-daemon.sh stop datanode\n",
    "sbin/hadoop-daemon.sh stop namenode\n",
    "\n",
    "# Start HADOOP datanode and namenode\n",
    "sbin/hadoop-daemon.sh start datanode\n",
    "sbin/hadoop-daemon.sh start namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max memory size         (kbytes, -m) unlimited\r\n",
      "open files                      (-n) 1048576\r\n",
      "pipe size            (512 bytes, -p) 8\r\n",
      "POSIX message queues     (bytes, -q) 819200\r\n",
      "real-time priority              (-r) 0\r\n",
      "stack size              (kbytes, -s) 8192\r\n",
      "cpu time               (seconds, -t) unlimited\r\n",
      "max user processes              (-u) 1048576\r\n",
      "virtual memory          (kbytes, -v) unlimited\r\n",
      "file locks                      (-x) unlimited\r\n"
     ]
    }
   ],
   "source": [
    "# This will need to be modified to correct filename from previous code output\n",
    "!tail /usr/local/hadoop/logs/hadoop--namenode-83a8dc7241b5.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HDFS\n",
    "\n",
    "At this point, we need to move our data to process into the Hadoop\n",
    "Distributed File system, or HDFS. HDFS is a a file system that is designed\n",
    "to work effectively with the Hadoop environment. In a typical Hadoop\n",
    "cluster, files would be broken up and distributed to different Hadoop\n",
    "nodes. The processing is moved to the data in this model, which can\n",
    "produce high throughput, especially for map/reduce programming tasks.\n",
    "However, this means you can not simply move around the HDFS file system\n",
    "in the same manner as a traditional Unix file system, since the\n",
    "components of a particular file are not all col-located. Instead, we\n",
    "must use the [HDFS file system interface][hdfs], which is invoked by\n",
    "using `$HADOOP_PREFIX/bin/hdfs`. Running this command by itself in your\n",
    "Hadoop Docker container will list the available commands, as shown in\n",
    "the following code cell.\n",
    "\n",
    "-----\n",
    "\n",
    "[hdfs]: https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [--config confdir] COMMAND\r\n",
      "       where COMMAND is one of:\r\n",
      "  dfs                  run a filesystem command on the file systems supported in Hadoop.\r\n",
      "  namenode -format     format the DFS filesystem\r\n",
      "  secondarynamenode    run the DFS secondary namenode\r\n",
      "  namenode             run the DFS namenode\r\n",
      "  journalnode          run the DFS journalnode\r\n",
      "  zkfc                 run the ZK Failover Controller daemon\r\n",
      "  datanode             run a DFS datanode\r\n",
      "  dfsadmin             run a DFS admin client\r\n",
      "  haadmin              run a DFS HA admin client\r\n",
      "  fsck                 run a DFS filesystem checking utility\r\n",
      "  balancer             run a cluster balancing utility\r\n",
      "  jmxget               get JMX exported values from NameNode or DataNode.\r\n",
      "  mover                run a utility to move block replicas across\r\n",
      "                       storage types\r\n",
      "  oiv                  apply the offline fsimage viewer to an fsimage\r\n",
      "  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage\r\n",
      "  oev                  apply the offline edits viewer to an edits file\r\n",
      "  fetchdt              fetch a delegation token from the NameNode\r\n",
      "  getconf              get config values from configuration\r\n",
      "  groups               get the groups which users belong to\r\n",
      "  snapshotDiff         diff two snapshots of a directory or diff the\r\n",
      "                       current directory contents with a snapshot\r\n",
      "  lsSnapshottableDir   list all snapshottable dirs owned by the current user\r\n",
      "\t\t\t\t\t\tUse -help to see options\r\n",
      "  portmap              run a portmap service\r\n",
      "  nfs3                 run an NFS version 3 gateway\r\n",
      "  cacheadmin           configure the HDFS cache\r\n",
      "  crypto               configure HDFS encryption zones\r\n",
      "  storagepolicies      get all the existing block storage policies\r\n",
      "  version              print the version\r\n",
      "\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The standard command we will use is `dfs` which runs a filesystem\n",
    "command on the HDFS file system that is supported by Hadoop. The [list\n",
    "of supported `dfs` commands][dfsl] is extensive, and mirrors many of the\n",
    "traditional Unix file systems commands. The full listing can be obtained\n",
    "by entering `$HADOOP_PREFIX/bin/hdfs dfs` at the prompt in our Hadoop\n",
    "Docker container.\n",
    "\n",
    "-----\n",
    "\n",
    "[dfsl]: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    " Some of the more useful commands for this class\n",
    "include:\n",
    "\n",
    "- `-cat`: copies the source path to STDOUT.\n",
    "\n",
    "- `-count -h`: counts the number of directories, files and byts under the\n",
    "path specified. With the `-h` flag, the output is displayed in a\n",
    "human-readable format.\n",
    "\n",
    "- `-expunge`: empties the trash. By default, files and directories are\n",
    "not removed from HDFS with the `rm` command, they are simply moved to the\n",
    "trash. This can be useful when HDFS supplies a `Name node is in safe\n",
    "mode.` message. \n",
    "\n",
    "- `-ls`: lists the contents of the indicated directory in HDFS.\n",
    "\n",
    "- `-mkdir -p`: creates a new directory in HDFS at the specified\n",
    "location. With the `-p` flag any parent directory specified in the full\n",
    "path will also be created as necessary.\n",
    "\n",
    "- `-put`: copies indicated file(s) from local host file system into the\n",
    "specified path in HDFS.\n",
    "\n",
    "- `-rm -f -r`: delete the indicated file or directory. With the `-r -f`\n",
    "flags, the command will not display any message and any will delete any\n",
    "files or directories under the indicated directory. The `-skipTrash`\n",
    "flag should be used to delete the indicated resource immediately.\n",
    "\n",
    "- `-tail`: display the last kilobyte of the indicated file to STDOUT.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-05 20:24 /home\r\n",
      "drwx------   - data_scientist supergroup          0 2016-04-05 20:25 /tmp\r\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-05 20:26 /user\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-05 20:46 /user/data_scientist\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls /user/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /user: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                  Size   Used  Available  Use%\r\n",
      "hdfs://83a8dc7241b5:9000  18.2 G  1.6 M      8.2 G    0%\r\n"
     ]
    }
   ],
   "source": [
    "# Free Space\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.2 K  hadoop\r\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -du -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Running shell script\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: `hadoop/input': File exists\n",
      "16/04/05 22:46:25 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/05 22:46:26 WARN mapreduce.JobSubmitter: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "16/04/05 22:46:26 INFO input.FileInputFormat: Total input paths to process : 32\n",
      "16/04/05 22:46:26 INFO mapreduce.JobSubmitter: number of splits:32\n",
      "16/04/05 22:46:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1459886174643_0008\n",
      "16/04/05 22:46:27 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.\n",
      "16/04/05 22:46:27 INFO impl.YarnClientImpl: Submitted application application_1459886174643_0008\n",
      "16/04/05 22:46:27 INFO mapreduce.Job: The url to track the job: http://83a8dc7241b5:8088/proxy/application_1459886174643_0008/\n",
      "16/04/05 22:46:27 INFO mapreduce.Job: Running job: job_1459886174643_0008\n",
      "16/04/05 22:46:34 INFO mapreduce.Job: Job job_1459886174643_0008 running in uber mode : false\n",
      "16/04/05 22:46:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/05 22:46:59 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "16/04/05 22:47:00 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "16/04/05 22:47:01 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "16/04/05 22:47:25 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "16/04/05 22:47:26 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "16/04/05 22:47:48 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "16/04/05 22:47:49 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/04/05 22:47:53 INFO mapreduce.Job:  map 53% reduce 18%\n",
      "16/04/05 22:48:09 INFO mapreduce.Job:  map 56% reduce 18%\n",
      "16/04/05 22:48:10 INFO mapreduce.Job:  map 69% reduce 18%\n",
      "16/04/05 22:48:11 INFO mapreduce.Job:  map 69% reduce 23%\n",
      "16/04/05 22:48:30 INFO mapreduce.Job:  map 72% reduce 23%\n",
      "16/04/05 22:48:31 INFO mapreduce.Job:  map 84% reduce 23%\n",
      "16/04/05 22:48:32 INFO mapreduce.Job:  map 84% reduce 28%\n",
      "16/04/05 22:48:51 INFO mapreduce.Job:  map 91% reduce 28%\n",
      "16/04/05 22:48:51 INFO mapreduce.Job: Task Id : attempt_1459886174643_0008_m_000031_0, Status : FAILED\n",
      "Error: java.io.FileNotFoundException: Path is not a file: /user/data_scientist/hadoop/input/hadoop\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:70)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1891)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1812)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1784)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1222)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:545)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:783)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path is not a file: /user/data_scientist/hadoop/input/hadoop\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:70)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1891)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1812)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1784)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1468)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1399)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n",
      "\tat com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)\n",
      "\t... 20 more\n",
      "\n",
      "16/04/05 22:48:52 INFO mapreduce.Job:  map 97% reduce 28%\n",
      "16/04/05 22:48:54 INFO mapreduce.Job:  map 97% reduce 32%\n",
      "16/04/05 22:48:56 INFO mapreduce.Job: Task Id : attempt_1459886174643_0008_m_000031_1, Status : FAILED\n",
      "Error: java.io.FileNotFoundException: Path is not a file: /user/data_scientist/hadoop/input/hadoop\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:70)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1891)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1812)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1784)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1222)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:545)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:783)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path is not a file: /user/data_scientist/hadoop/input/hadoop\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:70)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1891)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1812)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1784)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1468)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1399)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n",
      "\tat com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)\n",
      "\t... 20 more\n",
      "\n",
      "16/04/05 22:49:01 INFO mapreduce.Job: Task Id : attempt_1459886174643_0008_m_000031_2, Status : FAILED\n",
      "Error: java.io.FileNotFoundException: Path is not a file: /user/data_scientist/hadoop/input/hadoop\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:70)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1891)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1812)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1784)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1222)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:545)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:783)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path is not a file: /user/data_scientist/hadoop/input/hadoop\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:70)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1891)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1812)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1784)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1468)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1399)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n",
      "\tat com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)\n",
      "\t... 20 more\n",
      "\n",
      "16/04/05 22:49:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/05 22:49:09 INFO mapreduce.Job: Job job_1459886174643_0008 failed with state FAILED due to: Task failed task_1459886174643_0008_m_000031\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "16/04/05 22:49:09 INFO mapreduce.Job: Counters: 40\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=3306850\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=81118\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=93\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=4\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=35\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=4\n",
      "\t\tData-local map tasks=31\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=714709\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=103363\n",
      "\t\tTotal time spent by all map tasks (ms)=714709\n",
      "\t\tTotal time spent by all reduce tasks (ms)=103363\n",
      "\t\tTotal vcore-seconds taken by all map tasks=714709\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=103363\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=731862016\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=105843712\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2065\n",
      "\t\tMap output records=26\n",
      "\t\tMap output bytes=647\n",
      "\t\tMap output materialized bytes=586\n",
      "\t\tInput split bytes=4339\n",
      "\t\tCombine input records=26\n",
      "\t\tCombine output records=15\n",
      "\t\tSpilled Records=15\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=7027\n",
      "\t\tCPU time spent (ms)=9130\n",
      "\t\tPhysical memory (bytes) snapshot=6491193344\n",
      "\t\tVirtual memory (bytes) snapshot=21794807808\n",
      "\t\tTotal committed heap usage (bytes)=4238077952\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76779\n",
      "16/04/05 22:49:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://83a8dc7241b5:9000/user/data_scientist/hadoop/output already exists\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:562)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:432)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)\n",
      "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1314)\n",
      "\tat org.apache.hadoop.examples.Grep.run(Grep.java:92)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.hadoop.examples.Grep.main(Grep.java:101)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n",
      "\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n",
      "\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n",
      "tail: `hadoop/output/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HADOOP_PREFIX\n",
    "bin/hdfs dfs -mkdir -p hadoop\n",
    "#bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop/ hadoop/input\n",
    "bin/hdfs dfs -mkdir -p hadoop/input\n",
    "\n",
    "# run the mapreduce\n",
    "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep hadoop/input hadoop/output 'dfs[a-z.]+'\n",
    "\n",
    "# check the output\n",
    "bin/hdfs dfs -tail hadoop/output/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/04/05 20:25:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/05 20:25:59 WARN mapreduce.JobSubmitter: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "16/04/05 20:25:59 INFO input.FileInputFormat: Total input paths to process : 31\n",
      "16/04/05 20:25:59 INFO mapreduce.JobSubmitter: number of splits:31\n",
      "16/04/05 20:25:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1459886174643_0002\n",
      "16/04/05 20:26:00 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.\n",
      "16/04/05 20:26:00 INFO impl.YarnClientImpl: Submitted application application_1459886174643_0002\n",
      "16/04/05 20:26:00 INFO mapreduce.Job: The url to track the job: http://83a8dc7241b5:8088/proxy/application_1459886174643_0002/\n",
      "16/04/05 20:26:00 INFO mapreduce.Job: Running job: job_1459886174643_0002\n",
      "16/04/05 20:26:10 INFO mapreduce.Job: Job job_1459886174643_0002 running in uber mode : false\n",
      "16/04/05 20:26:10 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/05 20:26:41 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "16/04/05 20:27:09 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "16/04/05 20:27:34 INFO mapreduce.Job:  map 39% reduce 13%\n",
      "16/04/05 20:27:35 INFO mapreduce.Job:  map 48% reduce 13%\n",
      "16/04/05 20:27:36 INFO mapreduce.Job:  map 55% reduce 13%\n",
      "16/04/05 20:27:37 INFO mapreduce.Job:  map 55% reduce 18%\n",
      "16/04/05 20:27:58 INFO mapreduce.Job:  map 65% reduce 18%\n",
      "16/04/05 20:27:59 INFO mapreduce.Job:  map 71% reduce 22%\n",
      "16/04/05 20:28:02 INFO mapreduce.Job:  map 71% reduce 24%\n",
      "16/04/05 20:28:18 INFO mapreduce.Job:  map 74% reduce 24%\n",
      "16/04/05 20:28:20 INFO mapreduce.Job:  map 74% reduce 25%\n",
      "16/04/05 20:28:22 INFO mapreduce.Job:  map 84% reduce 25%\n",
      "16/04/05 20:28:23 INFO mapreduce.Job:  map 87% reduce 27%\n",
      "16/04/05 20:28:26 INFO mapreduce.Job:  map 87% reduce 29%\n",
      "16/04/05 20:28:40 INFO mapreduce.Job:  map 90% reduce 29%\n",
      "16/04/05 20:28:41 INFO mapreduce.Job:  map 94% reduce 30%\n",
      "16/04/05 20:28:42 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "16/04/05 20:28:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/05 20:28:44 INFO mapreduce.Job: Job job_1459886174643_0002 completed successfully\n",
      "16/04/05 20:28:45 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=406\n",
      "\t\tFILE: Number of bytes written=3413822\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=81118\n",
      "\t\tHDFS: Number of bytes written=510\n",
      "\t\tHDFS: Number of read operations=96\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=32\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=32\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=749326\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=94612\n",
      "\t\tTotal time spent by all map tasks (ms)=749326\n",
      "\t\tTotal time spent by all reduce tasks (ms)=94612\n",
      "\t\tTotal vcore-seconds taken by all map tasks=749326\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=94612\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=767309824\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=96882688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2065\n",
      "\t\tMap output records=26\n",
      "\t\tMap output bytes=647\n",
      "\t\tMap output materialized bytes=586\n",
      "\t\tInput split bytes=4339\n",
      "\t\tCombine input records=26\n",
      "\t\tCombine output records=15\n",
      "\t\tReduce input groups=13\n",
      "\t\tReduce shuffle bytes=586\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=13\n",
      "\t\tSpilled Records=30\n",
      "\t\tShuffled Maps =31\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=31\n",
      "\t\tGC time elapsed (ms)=7409\n",
      "\t\tCPU time spent (ms)=11790\n",
      "\t\tPhysical memory (bytes) snapshot=6508314624\n",
      "\t\tVirtual memory (bytes) snapshot=22504054784\n",
      "\t\tTotal committed heap usage (bytes)=4269862912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76779\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=510\n",
      "16/04/05 20:28:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/05 20:28:45 WARN mapreduce.JobSubmitter: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "16/04/05 20:28:45 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "16/04/05 20:28:45 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/04/05 20:28:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1459886174643_0003\n",
      "16/04/05 20:28:45 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.\n",
      "16/04/05 20:28:45 INFO impl.YarnClientImpl: Submitted application application_1459886174643_0003\n",
      "16/04/05 20:28:45 INFO mapreduce.Job: The url to track the job: http://83a8dc7241b5:8088/proxy/application_1459886174643_0003/\n",
      "16/04/05 20:28:45 INFO mapreduce.Job: Running job: job_1459886174643_0003\n",
      "16/04/05 20:28:57 INFO mapreduce.Job: Job job_1459886174643_0003 running in uber mode : false\n",
      "16/04/05 20:28:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/05 20:29:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/04/05 20:29:11 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/05 20:29:11 INFO mapreduce.Job: Job job_1459886174643_0003 completed successfully\n",
      "16/04/05 20:29:11 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=352\n",
      "\t\tFILE: Number of bytes written=212927\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=652\n",
      "\t\tHDFS: Number of bytes written=242\n",
      "\t\tHDFS: Number of read operations=7\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3919\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4648\n",
      "\t\tTotal time spent by all map tasks (ms)=3919\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4648\n",
      "\t\tTotal vcore-seconds taken by all map tasks=3919\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4648\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4013056\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4759552\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=13\n",
      "\t\tMap output records=13\n",
      "\t\tMap output bytes=320\n",
      "\t\tMap output materialized bytes=352\n",
      "\t\tInput split bytes=142\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=352\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=13\n",
      "\t\tSpilled Records=26\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=99\n",
      "\t\tCPU time spent (ms)=860\n",
      "\t\tPhysical memory (bytes) snapshot=319045632\n",
      "\t\tVirtual memory (bytes) snapshot=1412440064\n",
      "\t\tTotal committed heap usage (bytes)=168497152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=510\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=242\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hadoop \\\n",
    "    jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar \\\n",
    "    grep /home/data_scientist/hadoop/input /home/data_scientist/hadoop/output 'dfs[a-z.]+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "At this point, we first need to create an directory to hold the input\n",
    "and output of our Hadoop task. We will create a new directory called\n",
    "`wc` with a subdirectory called `in` to hold the input data for our\n",
    "Hadoop task. Second, we will need to copy the book text file into this\n",
    "new HDFS directory. This means we will need to run the following two\n",
    "commands at the prompt in our Hadoop Docker container:\n",
    "\n",
    "1. `$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/in`\n",
    "2. `$HADOOP_PREFIX/bin/hdfs dfs -put book.txt wc/in/book.txt`\n",
    "\n",
    "The following screenshot displays the result of running these two\n",
    "commands, as well as the `dfs -ls` command to display the contents of\n",
    "our new HDFS directory, and the `dfs -count` command to show the size of\n",
    "the directory contents.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: yarn [--config confdir] COMMAND\r\n",
      "where COMMAND is one of:\r\n",
      "  resourcemanager -format-state-store   deletes the RMStateStore\r\n",
      "  resourcemanager                       run the ResourceManager\r\n",
      "  nodemanager                           run a nodemanager on each slave\r\n",
      "  timelineserver                        run the timeline server\r\n",
      "  rmadmin                               admin tools\r\n",
      "  version                               print the version\r\n",
      "  jar <jar>                             run a jar file\r\n",
      "  application                           prints application(s)\r\n",
      "                                        report/kill application\r\n",
      "  applicationattempt                    prints applicationattempt(s)\r\n",
      "                                        report\r\n",
      "  container                             prints container(s) report\r\n",
      "  node                                  prints node report(s)\r\n",
      "  queue                                 prints queue information\r\n",
      "  logs                                  dump container logs\r\n",
      "  classpath                             prints the class path needed to\r\n",
      "                                        get the Hadoop jar and the\r\n",
      "                                        required libraries\r\n",
      "  daemonlog                             get/set the log level for each\r\n",
      "                                        daemon\r\n",
      " or\r\n",
      "  CLASSNAME                             run the class named CLASSNAME\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/yarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 items\n",
      "-rw-r--r--   1 data_scientist supergroup       4436 2016-04-05 20:43 hadoop/input/capacity-scheduler.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       1335 2016-04-05 20:43 hadoop/input/configuration.xsl\n",
      "-rw-r--r--   1 data_scientist supergroup        318 2016-04-05 20:43 hadoop/input/container-executor.cfg\n",
      "-rw-r--r--   1 data_scientist supergroup        158 2016-04-05 20:43 hadoop/input/core-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        154 2016-04-05 20:43 hadoop/input/core-site.xml.template\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-05 22:40 hadoop/input/hadoop\n",
      "-rw-r--r--   1 data_scientist supergroup       3670 2016-04-05 20:43 hadoop/input/hadoop-env.cmd\n",
      "-rw-r--r--   1 data_scientist supergroup       4302 2016-04-05 20:43 hadoop/input/hadoop-env.sh\n",
      "-rw-r--r--   1 data_scientist supergroup       2490 2016-04-05 20:43 hadoop/input/hadoop-metrics.properties\n",
      "-rw-r--r--   1 data_scientist supergroup       2598 2016-04-05 20:43 hadoop/input/hadoop-metrics2.properties\n",
      "-rw-r--r--   1 data_scientist supergroup       9683 2016-04-05 20:43 hadoop/input/hadoop-policy.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        354 2016-04-05 20:43 hadoop/input/hdfs-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       1449 2016-04-05 20:43 hadoop/input/httpfs-env.sh\n",
      "-rw-r--r--   1 data_scientist supergroup       1657 2016-04-05 20:43 hadoop/input/httpfs-log4j.properties\n",
      "-rw-r--r--   1 data_scientist supergroup         21 2016-04-05 20:43 hadoop/input/httpfs-signature.secret\n",
      "-rw-r--r--   1 data_scientist supergroup        620 2016-04-05 20:43 hadoop/input/httpfs-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       3523 2016-04-05 20:43 hadoop/input/kms-acls.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       1325 2016-04-05 20:43 hadoop/input/kms-env.sh\n",
      "-rw-r--r--   1 data_scientist supergroup       1631 2016-04-05 20:43 hadoop/input/kms-log4j.properties\n",
      "-rw-r--r--   1 data_scientist supergroup       5511 2016-04-05 20:43 hadoop/input/kms-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup      11291 2016-04-05 20:43 hadoop/input/log4j.properties\n",
      "-rw-r--r--   1 data_scientist supergroup        938 2016-04-05 20:43 hadoop/input/mapred-env.cmd\n",
      "-rw-r--r--   1 data_scientist supergroup       1383 2016-04-05 20:43 hadoop/input/mapred-env.sh\n",
      "-rw-r--r--   1 data_scientist supergroup       4113 2016-04-05 20:43 hadoop/input/mapred-queues.xml.template\n",
      "-rw-r--r--   1 data_scientist supergroup        138 2016-04-05 20:43 hadoop/input/mapred-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        758 2016-04-05 20:43 hadoop/input/mapred-site.xml.template\n",
      "-rw-r--r--   1 data_scientist supergroup         10 2016-04-05 20:43 hadoop/input/slaves\n",
      "-rw-r--r--   1 data_scientist supergroup       2316 2016-04-05 20:43 hadoop/input/ssl-client.xml.example\n",
      "-rw-r--r--   1 data_scientist supergroup       2268 2016-04-05 20:43 hadoop/input/ssl-server.xml.example\n",
      "-rw-r--r--   1 data_scientist supergroup       2237 2016-04-05 20:43 hadoop/input/yarn-env.cmd\n",
      "-rw-r--r--   1 data_scientist supergroup       4567 2016-04-05 20:43 hadoop/input/yarn-env.sh\n",
      "-rw-r--r--   1 data_scientist supergroup       1525 2016-04-05 20:43 hadoop/input/yarn-site.xml\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in thread \"main\" java.lang.NoSuchMethodException: org.apache.hadoop.hdfs.TestDFSShell.main([Ljava.lang.String;)\r\n",
      "\tat java.lang.Class.getMethod(Class.java:1665)\r\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:215)\r\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/yarn jar $HADOOP_PREFIX/share/hadoop/hdfs/hadoop-*test*.jar org.apache.hadoop.hdfs.TestDFSShell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `share/hadoop/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls share/hadoop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/data_scientist/rppdm/info490-sp16/Week12/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
