{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Hadoop\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "----- \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this Notebook, we will demonstrate how to run a Hadoop Streaming\n",
    "Map/Reduce job in a docker container. Our setup will be using a single\n",
    "Hadoop node, which will not be very fast, especially when compared to\n",
    "simply running the map/reduce Python code directly. However, the full\n",
    "Hadoop process will be demonstrated, including the use of the Hadoop\n",
    "file system (HDFS) and the Hadoop Streaming process model. Before\n",
    "proceeding with this Notebook, be sure to (at least start to) download\n",
    "the SequenceIQ Hadoop Docker container.\n",
    "\n",
    "Typically, basic Hadoop is operated on a large cluster that runs both\n",
    "Hadoop and HDFS, although with the development of Yarn, more diverse\n",
    "workflows are now possible. In this Notebook, we only explore the basic\n",
    "Hadoop components of Hadoop and HDFS, which work together to run code on\n",
    "the nodes that hold the relevant data in order to maximize throughput.\n",
    "[Other resources][hort] exist to learn more about Yarn and other Hadoop\n",
    "workflows. The basic Hadoop task is a map/reduce process, where a map\n",
    "process analyzes data and creates a sequential list of key-value pairs\n",
    "(like a Python dictionary). The Hadoop process model sorts the output of\n",
    "the mappers before passing the results to a reduce process. The reduce\n",
    "process combines the key-value pairs to generate final output. The\n",
    "prototype map/reduce example is the [word-count problem][wcp], where a\n",
    "large corpus is analyzed to quantify how many times each word appears\n",
    "(one can quickly see how this model can be extended to analyze website\n",
    "as opposed to texts).\n",
    "\n",
    "Thus to complete a map/reduce task in Hadoop we need to complete the\n",
    "following tasks:\n",
    "\n",
    "1. create a Map program\n",
    "2. create a Reduce program\n",
    "3. obtain a data set to analyze\n",
    "4. load our data into HDFS\n",
    "5. execute our map/reduce program by using Hadoop\n",
    "\n",
    "The rest of this Notebook will demonstrate how to perform each of these\n",
    "tasks. We first will create the two Python programs, download a sample\n",
    "text, and also download the hadoop-streaming jar file into a shared\n",
    "local directory from within our course4 Docker container. Once these\n",
    "steps are complete, we will start our Hadoop Docker container to\n",
    "complete the rest of the process.\n",
    "\n",
    "In the next code cell, we start the process by running a shell script\n",
    "that creates (and deletes first if it exists) the shared directory that\n",
    "will hold the Python codes and data for our Map/Reduce Hadoop project.\n",
    "\n",
    "-----\n",
    "[hort]: http://hortonworks.com/hadoop-tutorial/introducing-apache-hadoop-developers/\n",
    "[wcp]: https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\n",
    "\n",
    "-----\n",
    "\n",
    "https://github.com/sequenceiq/hadoop-docker\n",
    "\n",
    "https://github.com/UI-DataScience/docker-info490/blob/master/hadoop/example.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Out File #####\n",
      "ulimit -a for user data_scientist\n",
      "core file size          (blocks, -c) 0\n",
      "data seg size           (kbytes, -d) unlimited\n",
      "scheduling priority             (-e) 0\n",
      "file size               (blocks, -f) unlimited\n",
      "pending signals                 (-i) 7902\n",
      "max locked memory       (kbytes, -l) 64\n",
      "max memory size         (kbytes, -m) unlimited\n",
      "open files                      (-n) 1048576\n",
      "pipe size            (512 bytes, -p) 8\n",
      "POSIX message queues     (bytes, -q) 819200\n",
      "real-time priority              (-r) 0\n",
      "stack size              (kbytes, -s) 8192\n",
      "cpu time               (seconds, -t) unlimited\n",
      "max user processes              (-u) 1048576\n",
      "virtual memory          (kbytes, -v) unlimited\n",
      "file locks                      (-x) unlimited\n",
      "\n",
      "##### Log File #####\n",
      "2016-04-07 17:10:57,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742005_1181 file /tmp/hadoop-data_scientist/dfs/data/current/BP-629745585-172.17.0.3-1459931584833/current/finalized/subdir0/subdir0/blk_1073742005 for deletion\n",
      "2016-04-07 17:10:57,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-629745585-172.17.0.3-1459931584833 blk_1073742005_1181 file /tmp/hadoop-data_scientist/dfs/data/current/BP-629745585-172.17.0.3-1459931584833/current/finalized/subdir0/subdir0/blk_1073742005\n",
      "2016-04-07 17:57:48,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-629745585-172.17.0.3-1459931584833:blk_1073742008_1184 src: /172.17.0.1:45805 dest: /172.17.0.1:50010\n",
      "2016-04-07 17:57:48,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /172.17.0.1:45805, dest: /172.17.0.1:50010, bytes: 1573151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1819339290_1, offset: 0, srvID: 5df2a193-dec4-4f93-82b4-3262cda64f72, blockid: BP-629745585-172.17.0.3-1459931584833:blk_1073742008_1184, duration: 73672156\n",
      "2016-04-07 17:57:48,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-629745585-172.17.0.3-1459931584833:blk_1073742008_1184, type=LAST_IN_PIPELINE, downstreams=0:[] terminating\n",
      "2016-04-07 18:00:10,116 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742008_1184 file /tmp/hadoop-data_scientist/dfs/data/current/BP-629745585-172.17.0.3-1459931584833/current/finalized/subdir0/subdir0/blk_1073742008 for deletion\n",
      "2016-04-07 18:00:10,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-629745585-172.17.0.3-1459931584833 blk_1073742008_1184 file /tmp/hadoop-data_scientist/dfs/data/current/BP-629745585-172.17.0.3-1459931584833/current/finalized/subdir0/subdir0/blk_1073742008\n",
      "2016-04-07 18:00:13,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-629745585-172.17.0.3-1459931584833:blk_1073742009_1185 src: /172.17.0.1:45813 dest: /172.17.0.1:50010\n",
      "2016-04-07 18:00:13,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /172.17.0.1:45813, dest: /172.17.0.1:50010, bytes: 1573151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-807847967_1, offset: 0, srvID: 5df2a193-dec4-4f93-82b4-3262cda64f72, blockid: BP-629745585-172.17.0.3-1459931584833:blk_1073742009_1185, duration: 81468606\n",
      "2016-04-07 18:00:13,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-629745585-172.17.0.3-1459931584833:blk_1073742009_1185, type=LAST_IN_PIPELINE, downstreams=0:[] terminating\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "echo '##### Out File #####'\n",
    "out_file=$(ls -la /usr/local/hadoop/logs/hadoop-data*.out | head -1 | awk '{print $9}')\n",
    "cat  $out_file\n",
    "\n",
    "echo\n",
    "echo '##### Log File #####'\n",
    "log_file=$(ls -la /usr/local/hadoop/logs/hadoop-data*.log | head -1 | awk '{print $9}')\n",
    "tail -10  $log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HDFS\n",
    "\n",
    "At this point, we need to move our data to process into the Hadoop\n",
    "Distributed File system, or HDFS. HDFS is a a file system that is designed\n",
    "to work effectively with the Hadoop environment. In a typical Hadoop\n",
    "cluster, files would be broken up and distributed to different Hadoop\n",
    "nodes. The processing is moved to the data in this model, which can\n",
    "produce high throughput, especially for map/reduce programming tasks.\n",
    "However, this means you can not simply move around the HDFS file system\n",
    "in the same manner as a traditional Unix file system, since the\n",
    "components of a particular file are not all col-located. Instead, we\n",
    "must use the [HDFS file system interface][hdfs], which is invoked by\n",
    "using `$HADOOP_PREFIX/bin/hdfs`. Running this command by itself in your\n",
    "Hadoop Docker container will list the available commands, as shown in\n",
    "the following code cell.\n",
    "\n",
    "-----\n",
    "\n",
    "[hdfs]: https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND\r\n",
      "       where COMMAND is one of:\r\n",
      "  dfs                  run a filesystem command on the file systems supported in Hadoop.\r\n",
      "  classpath            prints the classpath\r\n",
      "  namenode -format     format the DFS filesystem\r\n",
      "  secondarynamenode    run the DFS secondary namenode\r\n",
      "  namenode             run the DFS namenode\r\n",
      "  journalnode          run the DFS journalnode\r\n",
      "  zkfc                 run the ZK Failover Controller daemon\r\n",
      "  datanode             run a DFS datanode\r\n",
      "  dfsadmin             run a DFS admin client\r\n",
      "  haadmin              run a DFS HA admin client\r\n",
      "  fsck                 run a DFS filesystem checking utility\r\n",
      "  balancer             run a cluster balancing utility\r\n",
      "  jmxget               get JMX exported values from NameNode or DataNode.\r\n",
      "  mover                run a utility to move block replicas across\r\n",
      "                       storage types\r\n",
      "  oiv                  apply the offline fsimage viewer to an fsimage\r\n",
      "  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage\r\n",
      "  oev                  apply the offline edits viewer to an edits file\r\n",
      "  fetchdt              fetch a delegation token from the NameNode\r\n",
      "  getconf              get config values from configuration\r\n",
      "  groups               get the groups which users belong to\r\n",
      "  snapshotDiff         diff two snapshots of a directory or diff the\r\n",
      "                       current directory contents with a snapshot\r\n",
      "  lsSnapshottableDir   list all snapshottable dirs owned by the current user\r\n",
      "\t\t\t\t\t\tUse -help to see options\r\n",
      "  portmap              run a portmap service\r\n",
      "  nfs3                 run an NFS version 3 gateway\r\n",
      "  cacheadmin           configure the HDFS cache\r\n",
      "  crypto               configure HDFS encryption zones\r\n",
      "  storagepolicies      list/get/set block storage policies\r\n",
      "  version              print the version\r\n",
      "\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The standard command we will use is `dfs` which runs a filesystem\n",
    "command on the HDFS file system that is supported by Hadoop. The [list\n",
    "of supported `dfs` commands][dfsl] is extensive, and mirrors many of the\n",
    "traditional Unix file systems commands. The full listing can be obtained\n",
    "by entering `$HADOOP_PREFIX/bin/hdfs dfs` at the prompt in our Hadoop\n",
    "Docker container.\n",
    "\n",
    "-----\n",
    "\n",
    "[dfsl]: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    " Some of the more useful commands for this class\n",
    "include:\n",
    "\n",
    "- `-cat`: copies the source path to STDOUT.\n",
    "\n",
    "- `-count -h`: counts the number of directories, files and byts under the\n",
    "path specified. With the `-h` flag, the output is displayed in a\n",
    "human-readable format.\n",
    "\n",
    "- `-expunge`: empties the trash. By default, files and directories are\n",
    "not removed from HDFS with the `rm` command, they are simply moved to the\n",
    "trash. This can be useful when HDFS supplies a `Name node is in safe\n",
    "mode.` message. \n",
    "\n",
    "- `-ls`: lists the contents of the indicated directory in HDFS.\n",
    "\n",
    "- `-mkdir -p`: creates a new directory in HDFS at the specified\n",
    "location. With the `-p` flag any parent directory specified in the full\n",
    "path will also be created as necessary.\n",
    "\n",
    "- `-put`: copies indicated file(s) from local host file system into the\n",
    "specified path in HDFS.\n",
    "\n",
    "- `-rm -f -r`: delete the indicated file or directory. With the `-r -f`\n",
    "flags, the command will not display any message and any will delete any\n",
    "files or directories under the indicated directory. The `-skipTrash`\n",
    "flag should be used to delete the indicated resource immediately.\n",
    "\n",
    "- `-tail`: display the last kilobyte of the indicated file to STDOUT.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxrwx---   - data_scientist supergroup          0 2016-04-06 19:34 /tmp\r\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-06 08:34 /user\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-07 17:57 /user/data_scientist\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls /user/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /user: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                  Size   Used  Available  Use%\r\n",
      "hdfs://e7d89fb87de4:9000  18.2 G  4.3 M     11.5 G    0%\r\n"
     ]
    }
   ],
   "source": [
    "# Free Space\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Testing Hadoop\n",
    "\n",
    "Run simple example. First make directories, coopy in data, and run a grep search.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hadoop\n",
      "Found 2 items\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-07 18:16 hadoop/input\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-07 18:17 hadoop/output\n",
      "Found 9 items\n",
      "-rw-r--r--   1 data_scientist supergroup       4436 2016-04-07 18:16 hadoop/input/capacity-scheduler.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        158 2016-04-07 18:16 hadoop/input/core-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       9683 2016-04-07 18:16 hadoop/input/hadoop-policy.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        354 2016-04-07 18:16 hadoop/input/hdfs-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        620 2016-04-07 18:16 hadoop/input/httpfs-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       3518 2016-04-07 18:16 hadoop/input/kms-acls.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       5511 2016-04-07 18:16 hadoop/input/kms-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        357 2016-04-07 18:16 hadoop/input/mapred-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       1525 2016-04-07 18:16 hadoop/input/yarn-site.xml\n",
      "Found 2 items\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2016-04-07 18:17 hadoop/output/_SUCCESS\n",
      "-rw-r--r--   1 data_scientist supergroup         74 2016-04-07 18:17 hadoop/output/part-r-00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/04/07 18:16:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "16/04/07 18:16:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/07 18:16:19 INFO input.FileInputFormat: Total input paths to process : 9\n",
      "16/04/07 18:16:19 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "16/04/07 18:16:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1459971273803_0015\n",
      "16/04/07 18:16:19 INFO impl.YarnClientImpl: Submitted application application_1459971273803_0015\n",
      "16/04/07 18:16:19 INFO mapreduce.Job: The url to track the job: http://e7d89fb87de4:8088/proxy/application_1459971273803_0015/\n",
      "16/04/07 18:16:19 INFO mapreduce.Job: Running job: job_1459971273803_0015\n",
      "16/04/07 18:16:27 INFO mapreduce.Job: Job job_1459971273803_0015 running in uber mode : false\n",
      "16/04/07 18:16:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/07 18:16:50 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "16/04/07 18:16:51 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/04/07 18:17:04 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "16/04/07 18:17:06 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/04/07 18:17:07 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/07 18:17:07 INFO mapreduce.Job: Job job_1459971273803_0015 completed successfully\n",
      "16/04/07 18:17:08 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=112\n",
      "\t\tFILE: Number of bytes written=1187337\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=27401\n",
      "\t\tHDFS: Number of bytes written=216\n",
      "\t\tHDFS: Number of read operations=30\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=168735\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13764\n",
      "\t\tTotal time spent by all map tasks (ms)=168735\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13764\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=168735\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13764\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=172784640\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14094336\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=749\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=98\n",
      "\t\tMap output materialized bytes=160\n",
      "\t\tInput split bytes=1239\n",
      "\t\tCombine input records=4\n",
      "\t\tCombine output records=4\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=160\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=2904\n",
      "\t\tCPU time spent (ms)=3790\n",
      "\t\tPhysical memory (bytes) snapshot=1817427968\n",
      "\t\tVirtual memory (bytes) snapshot=19209400320\n",
      "\t\tTotal committed heap usage (bytes)=1269469184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=26162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=216\n",
      "16/04/07 18:17:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/07 18:17:08 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "16/04/07 18:17:08 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/04/07 18:17:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1459971273803_0016\n",
      "16/04/07 18:17:08 INFO impl.YarnClientImpl: Submitted application application_1459971273803_0016\n",
      "16/04/07 18:17:08 INFO mapreduce.Job: The url to track the job: http://e7d89fb87de4:8088/proxy/application_1459971273803_0016/\n",
      "16/04/07 18:17:08 INFO mapreduce.Job: Running job: job_1459971273803_0016\n",
      "16/04/07 18:17:22 INFO mapreduce.Job: Job job_1459971273803_0016 running in uber mode : false\n",
      "16/04/07 18:17:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/07 18:17:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/04/07 18:17:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/07 18:17:34 INFO mapreduce.Job: Job job_1459971273803_0016 completed successfully\n",
      "16/04/07 18:17:34 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=112\n",
      "\t\tFILE: Number of bytes written=236571\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=359\n",
      "\t\tHDFS: Number of bytes written=74\n",
      "\t\tHDFS: Number of read operations=7\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2962\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4055\n",
      "\t\tTotal time spent by all map tasks (ms)=2962\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4055\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2962\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4055\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3033088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4152320\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=98\n",
      "\t\tMap output materialized bytes=112\n",
      "\t\tInput split bytes=143\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=112\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=114\n",
      "\t\tCPU time spent (ms)=780\n",
      "\t\tPhysical memory (bytes) snapshot=310075392\n",
      "\t\tVirtual memory (bytes) snapshot=3848744960\n",
      "\t\tTotal committed heap usage (bytes)=170004480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=74\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HADOOP_PREFIX\n",
    "\n",
    "# Remove old directory (if it exsits) to have clean example\n",
    "bin/hdfs dfs -rm -r -f hadoop\n",
    "\n",
    "# Make directorties for example application\n",
    "bin/hdfs dfs -mkdir -p hadoop\n",
    "bin/hdfs dfs -mkdir -p hadoop/input\n",
    "\n",
    "# Copy data into example input directory\n",
    "bin/hdfs dfs -put etc/hadoop/*.xml hadoop/input\n",
    "\n",
    "# Running Hadoop example to test installation\n",
    "example_file=$(ls share/hadoop/mapreduce/hadoop-mapreduce-examples*)\n",
    "bin/hadoop jar $example_file grep hadoop/input hadoop/output 'dfs[a-z.]+'\n",
    "\n",
    "# Display directory heirarchy\n",
    "bin/hdfs dfs -ls hadoop/\n",
    "bin/hdfs dfs -ls hadoop/input\n",
    "bin/hdfs dfs -ls hadoop/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Examine output\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "-rw-r--r--   1 data_scientist supergroup       4436 2016-04-07 18:16 hadoop/input/capacity-scheduler.xml\r\n",
      "-rw-r--r--   1 data_scientist supergroup        158 2016-04-07 18:16 hadoop/input/core-site.xml\r\n",
      "-rw-r--r--   1 data_scientist supergroup       9683 2016-04-07 18:16 hadoop/input/hadoop-policy.xml\r\n",
      "-rw-r--r--   1 data_scientist supergroup        354 2016-04-07 18:16 hadoop/input/hdfs-site.xml\r\n",
      "-rw-r--r--   1 data_scientist supergroup        620 2016-04-07 18:16 hadoop/input/httpfs-site.xml\r\n",
      "-rw-r--r--   1 data_scientist supergroup       3518 2016-04-07 18:16 hadoop/input/kms-acls.xml\r\n",
      "-rw-r--r--   1 data_scientist supergroup       5511 2016-04-07 18:16 hadoop/input/kms-site.xml\r\n",
      "-rw-r--r--   1 data_scientist supergroup        357 2016-04-07 18:16 hadoop/input/mapred-site.xml\r\n",
      "-rw-r--r--   1 data_scientist supergroup       1525 2016-04-07 18:16 hadoop/input/yarn-site.xml\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2016-04-07 18:17 hadoop/output/_SUCCESS\r\n",
      "-rw-r--r--   1 data_scientist supergroup         74 2016-04-07 18:17 hadoop/output/part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls hadoop/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tdfsadmin\r\n",
      "1\tdfs.replication\r\n",
      "1\tdfs.namenode.servicerpc\r\n",
      "1\tdfs.namenode.rpc\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -cat hadoop/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Compare with unix grep\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/usr/local/hadoop/etc/hadoop/hadoop-policy.xml\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdfsa\u001b[m\u001b[Kdmin and mradmin commands to refresh the security policy in-effect.\r\n",
      "\u001b[35m\u001b[K/usr/local/hadoop/etc/hadoop/hdfs-site.xml\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        <name>\u001b[01;31m\u001b[Kdfs.\u001b[m\u001b[Kreplication</name>\r\n",
      "\u001b[35m\u001b[K/usr/local/hadoop/etc/hadoop/hdfs-site.xml\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        <name>\u001b[01;31m\u001b[Kdfs.\u001b[m\u001b[Knamenode.rpc-bind-host</name>\r\n",
      "\u001b[35m\u001b[K/usr/local/hadoop/etc/hadoop/hdfs-site.xml\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        <name>\u001b[01;31m\u001b[Kdfs.\u001b[m\u001b[Knamenode.servicerpc-bind-host</name>\r\n"
     ]
    }
   ],
   "source": [
    "!grep --color 'dfs[a-z.]' $HADOOP_PREFIX/etc/hadoop/*.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Data \n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxr-xr-x  2 data_scientist users 4096 Apr  7 18:17 .\n",
      "drwxr-xr-x 18 data_scientist users 4096 Apr  7 18:17 ..\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "# A Bash Shell Script to delete the Hadoop diorectory if it exists, afterwhich\n",
    "# make a new Hadoop directory\n",
    "\n",
    "# Our directory name\n",
    "DIR=$HOME/hadoop\n",
    "\n",
    "# Delete if exists\n",
    "if [ -d \"$DIR\" ]; then\n",
    "    rm -rf \"$DIR\"\n",
    "fi\n",
    "\n",
    "# Now make the directory\n",
    "mkdir \"$DIR\"\n",
    "\n",
    "ls -la $DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Acquiring Data\n",
    "\n",
    "To perform data analysis by using Hadoop, we will need a data set. In the\n",
    "Notebook for the second lesson this week, we will perform a simple\n",
    "map/reduce operation that will require text data to operate. While there\n",
    "are a number of possible options, for this example we can grab a free\n",
    "book from [Project Gutenberg][pg]:\n",
    "\n",
    "    wget --directory-prefix=$HOME/hadoop/ --output-document=book.txt \\\n",
    "        http://www.gutenberg.org/cache/epub/4300/pg4300.txt`\n",
    "\n",
    "In this case, we have grabbed the full text of the novel _Ulysses_, by\n",
    "James Joyce, and placed the text in the `hadoop` subdirectory of our\n",
    "_home_ directory.\n",
    "\n",
    "-----\n",
    "[pg]: http://www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-04-07 18:17:51--  http://www.gutenberg.org/cache/epub/4300/pg4300.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1573151 (1.5M) [text/plain]\n",
      "Saving to: ‘/home/data_scientist/hadoop/book.txt’\n",
      "\n",
      "100%[======================================>] 1,573,151   1.11MB/s   in 1.3s   \n",
      "\n",
      "2016-04-07 18:17:52 (1.11 MB/s) - ‘/home/data_scientist/hadoop/book.txt’ saved [1573151/1573151]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grab a book to process\n",
    "!wget --output-document=$HOME/hadoop/book.txt \\\n",
    "http://www.gutenberg.org/cache/epub/4300/pg4300.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "At this point, we first need to create an directory to hold the input\n",
    "and output of our Hadoop task. We will create a new directory called\n",
    "`wc` with a subdirectory called `in` to hold the input data for our\n",
    "Hadoop task. Second, we will need to copy the book text file into this\n",
    "new HDFS directory. This means we will need to run the following two\n",
    "commands at the prompt in our Hadoop Docker container:\n",
    "\n",
    "1. `$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/in`\n",
    "2. `$HADOOP_PREFIX/bin/hdfs dfs -put book.txt wc/in/book.txt`\n",
    "\n",
    "The following screenshot displays the result of running these two\n",
    "commands, as well as the `dfs -ls` command to display the contents of\n",
    "our new HDFS directory, and the `dfs -count` command to show the size of\n",
    "the directory contents.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc\n",
      "Found 1 items\n",
      "-rw-r--r--   1 data_scientist supergroup    1573151 2016-04-07 23:37 wc/in/book.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/04/07 23:37:53 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HADOOP_PREFIX\n",
    "\n",
    "bin/hdfs dfs -rm -r -f wc\n",
    "\n",
    "bin/hdfs dfs -mkdir -p wc/in\n",
    "bin/hdfs dfs -put $HOME/hadoop/book.txt wc/in/book.txt\n",
    "\n",
    "bin/hdfs dfs -ls wc/in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Bayesian Hierarchical modeling.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the model parameters, rerun the Notebook and see how the\n",
    "different (unpooled versus pooled) modeling approaches perform.\n",
    "2. Change the number of points in each bin, rerun the Notebook and see\n",
    "how the different (unpooled versus pooled) modeling approaches perform.\n",
    "3. Change the model for the prior distributions, e.g., try a uniform\n",
    "distribution for the intercept. How do the different (unpooled versus\n",
    "pooled) modeling approaches perform?\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
