{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Pig\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this IPython Notebook, we introduce pig.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## PIG\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Apache Pig version 0.15.0 (r1682971) \r\n",
      "compiled Jun 01 2015, 11:44:35\r\n",
      "\r\n",
      "USAGE: Pig [options] [-] : Run interactively in grunt shell.\r\n",
      "       Pig [options] -e[xecute] cmd [cmd ...] : Run cmd(s).\r\n",
      "       Pig [options] [-f[ile]] file : Run cmds found in file.\r\n",
      "  options include:\r\n",
      "    -4, -log4jconf - Log4j configuration file, overrides log conf\r\n",
      "    -b, -brief - Brief logging (no timestamps)\r\n",
      "    -c, -check - Syntax check\r\n",
      "    -d, -debug - Debug level, INFO is default\r\n",
      "    -e, -execute - Commands to execute (within quotes)\r\n",
      "    -f, -file - Path to the script to execute\r\n",
      "    -g, -embedded - ScriptEngine classname or keyword for the ScriptEngine\r\n",
      "    -h, -help - Display this message. You can specify topic to get help for that topic.\r\n",
      "        properties is the only topic currently supported: -h properties.\r\n",
      "    -i, -version - Display version information\r\n",
      "    -l, -logfile - Path to client side log file; default is current working directory.\r\n",
      "    -m, -param_file - Path to the parameter file\r\n",
      "    -p, -param - Key value pair of the form param=val\r\n",
      "    -r, -dryrun - Produces script with substituted parameters. Script is not executed.\r\n",
      "    -t, -optimizer_off - Turn optimizations off. The following values are supported:\r\n",
      "            ConstantCalculator - Calculate constants at compile time\r\n",
      "            SplitFilter - Split filter conditions\r\n",
      "            PushUpFilter - Filter as early as possible\r\n",
      "            MergeFilter - Merge filter conditions\r\n",
      "            PushDownForeachFlatten - Join or explode as late as possible\r\n",
      "            LimitOptimizer - Limit as early as possible\r\n",
      "            ColumnMapKeyPrune - Remove unused data\r\n",
      "            AddForEach - Add ForEach to remove unneeded columns\r\n",
      "            MergeForEach - Merge adjacent ForEach\r\n",
      "            GroupByConstParallelSetter - Force parallel 1 for \"group all\" statement\r\n",
      "            PartitionFilterOptimizer - Pushdown partition filter conditions to loader implementing LoadMetaData\r\n",
      "            PredicatePushdownOptimizer - Pushdown filter predicates to loader implementing LoadPredicatePushDown\r\n",
      "            All - Disable all optimizations\r\n",
      "        All optimizations listed here are enabled by default. Optimization values are case insensitive.\r\n",
      "    -v, -verbose - Print all error messages to screen\r\n",
      "    -w, -warning - Turn warning logging on; also turns warning aggregation off\r\n",
      "    -x, -exectype - Set execution mode: local|mapreduce|tez, default is mapreduce.\r\n",
      "    -F, -stop_on_failure - Aborts execution on the first failed job; default is off\r\n",
      "    -M, -no_multiquery - Turn multiquery optimization off; default is on\r\n",
      "    -N, -no_fetch - Turn fetch optimization off; default is on\r\n",
      "    -P, -propertyFile - Path to property file\r\n",
      "    -printCmdDebug - Overrides anything else and prints the actual command used to run Pig, including\r\n",
      "                     any environment variables that are set by the pig command.\r\n",
      "16/04/08 02:09:03 INFO pig.Main: Pig script completed in 183 milliseconds (183 ms)\r\n"
     ]
    }
   ],
   "source": [
    "!pig -help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Local Pig\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/wordcount-local.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/wordcount-local.pig\n",
    "\n",
    "Lines = LOAD 'book.txt' AS (Line:chararray) ;\n",
    "Words = FOREACH Lines GENERATE FLATTEN (TOKENIZE (Line)) AS Word ;\n",
    "Groups = GROUP Words BY Word ;\n",
    "Counts = FOREACH Groups GENERATE group, COUNT (Words) ;\n",
    "Results = ORDER Counts BY $1 DESC ;\n",
    "Top_Results = LIMIT Results 10 ;\n",
    "STORE Results INTO 'top_words' ;\n",
    "DUMP Top_Results ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(the,13626)\n",
      "(of,8133)\n",
      "(and,6681)\n",
      "(a,5869)\n",
      "(to,4817)\n",
      "(in,4651)\n",
      "(his,3051)\n",
      "(he,2792)\n",
      "(I,2455)\n",
      "(with,2401)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# We run locally, and send pig/hadoop messages to nowhere\n",
    "pig -x local -f wordcount-local.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The output of out Python based map-reduce was as follows:\n",
    "\n",
    "  Word | Count\n",
    " :----: | ----:\n",
    "the |  13600\n",
    "of | 8127\n",
    "and | 6542\n",
    "a  | 5842\n",
    "to | 4787\n",
    "in | 4606\n",
    "his\t|  3035\n",
    "he  | 2712\n",
    "I  | 2432\n",
    "with | 2391\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 468\n",
      "drwxr-xr-x 2 data_scientist users   4096 Apr  8 02:09 .\n",
      "drwxr-xr-x 3 data_scientist users   4096 Apr  8 02:09 ..\n",
      "-rw-r--r-- 1 data_scientist users 461372 Apr  8 02:09 part-r-00000\n",
      "-rw-r--r-- 1 data_scientist users   3616 Apr  8 02:09 .part-r-00000.crc\n",
      "-rw-r--r-- 1 data_scientist users      0 Apr  8 02:09 _SUCCESS\n",
      "-rw-r--r-- 1 data_scientist users      8 Apr  8 02:09 ._SUCCESS.crc\n",
      "\n",
      "Top Words\n",
      "the\t13626\n",
      "of\t8133\n",
      "and\t6681\n",
      "a\t5869\n",
      "to\t4817\n",
      "in\t4651\n",
      "his\t3051\n",
      "he\t2792\n",
      "I\t2455\n",
      "with\t2401\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# Show the contents of the local output\n",
    "ls -la top_words\n",
    "\n",
    "# Now display top words from output\n",
    "echo\n",
    "echo 'Top Words'\n",
    "head -10 top_words/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Hadoop Pig\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/wordcount.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/wordcount.pig\n",
    "\n",
    "Lines = LOAD 'wc/in/book.txt' AS (Line:chararray) ;\n",
    "Words = FOREACH Lines GENERATE FLATTEN (TOKENIZE (Line)) AS Word ;\n",
    "Groups = GROUP Words BY Word ;\n",
    "Counts = FOREACH Groups GENERATE group, COUNT (Words) ;\n",
    "Results = ORDER Counts BY $1 DESC ;\n",
    "Top_Results = LIMIT Results 10 ;\n",
    "STORE Results INTO 'wc/out/top_words' ;\n",
    "DUMP Top_Results ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc/out\n",
      "(the,13626)\n",
      "(of,8133)\n",
      "(and,6681)\n",
      "(a,5869)\n",
      "(to,4817)\n",
      "(in,4651)\n",
      "(his,3051)\n",
      "(he,2792)\n",
      "(I,2455)\n",
      "(with,2401)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# We remove old output if it exists and create output directory\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f wc/out 2> /dev/null\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -mkdir wc/out 2> /dev/null\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# We run remotely, and send pig/hadoop messages to nowhere\n",
    "pig -f wordcount.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 data_scientist supergroup    1573151 2016-04-07 23:37 wc/in/book.txt\n",
      "Found 2 items\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2016-04-08 02:10 wc/out/top_words/_SUCCESS\n",
      "-rw-r--r--   1 data_scientist supergroup     461372 2016-04-08 02:10 wc/out/top_words/part-r-00000\n",
      "the\t13626\n",
      "of\t8133\n",
      "and\t6681\n",
      "a\t5869\n",
      "to\t4817\n",
      "in\t4651\n",
      "his\t3051\n",
      "he\t2792\n",
      "I\t2455\n",
      "with\t2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HADOOP_PREFIX\n",
    "# Display directory contents\n",
    "bin/hdfs dfs -ls wc/in\n",
    "bin/hdfs dfs -ls wc/out/top_words\n",
    "\n",
    "# Write output\n",
    "\n",
    "echo\n",
    "echo 'Top Words'\n",
    "\n",
    "bin/hdfs dfs -cat wc/out/top_words/part-r-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Movie Lens Data Analysis\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name of the directory holding the Small MovieLens data\n",
    "data_dir = '/home/data_scientist/hadoop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-04-08 02:12:50--  http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.34.146\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.34.146|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1040425 (1016K) [application/zip]\n",
      "Saving to: ‘/home/data_scientist/hadoop/ml-latest-small.zip’\n",
      "\n",
      "100%[======================================>] 1,040,425   2.37MB/s   in 0.4s   \n",
      "\n",
      "2016-04-08 02:12:51 (2.37 MB/s) - ‘/home/data_scientist/hadoop/ml-latest-small.zip’ saved [1040425/1040425]\n",
      "\n",
      "Archive:  /home/data_scientist/hadoop/ml-latest-small.zip\n",
      "   creating: /home/data_scientist/hadoop/ml-latest-small/\n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/links.csv  \n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/movies.csv  \n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/ratings.csv  \n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/README.txt  \n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/tags.csv  \n"
     ]
    }
   ],
   "source": [
    "# Grab a book to process\n",
    "!wget --output-document=$data_dir/ml-latest-small.zip \\\n",
    "    http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "\n",
    "!unzip -o $data_dir/ml-latest-small.zip -d $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r",
      "\r\n",
      "1,16,4.0,1217897793\r",
      "\r\n",
      "1,24,1.5,1217895807\r",
      "\r\n",
      "1,32,4.0,1217896246\r",
      "\r\n",
      "1,47,4.0,1217896556\r",
      "\r\n",
      "1,50,4.0,1217896523\r",
      "\r\n",
      "1,110,4.0,1217896150\r",
      "\r\n",
      "1,150,3.0,1217895940\r",
      "\r\n",
      "1,161,4.0,1217897864\r",
      "\r\n",
      "1,165,3.0,1217897135\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 $data_dir/ml-latest-small/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/head.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/head.pig\n",
    "\n",
    "ratings = LOAD 'ml-latest-small/ratings.csv' USING PigStorage(',') ;\n",
    "tr = STREAM ratings THROUGH `head -10` AS (userID, mnovieID, rating, timestamp) ;\n",
    "DUMP tr ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(userId,movieId,rating,timestamp)\n",
      "(1,16,4.0,1217897793)\n",
      "(1,24,1.5,1217895807)\n",
      "(1,32,4.0,1217896246)\n",
      "(1,47,4.0,1217896556)\n",
      "(1,50,4.0,1217896523)\n",
      "(1,110,4.0,1217896150)\n",
      "(1,150,3.0,1217895940)\n",
      "(1,161,4.0,1217897864)\n",
      "(1,165,3.0,1217897135)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "pig -x local -b -f head.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 data_scientist users  207997 Jan 11 10:55 ml-latest-small/links.csv\n",
      "-rw-r--r-- 1 data_scientist users  515678 Apr  8 02:12 ml-latest-small/movies.csv\n",
      "-rw-r--r-- 1 data_scientist users  515700 Apr  8 02:12 ml-latest-small/original-movies.csv\n",
      "-rw-r--r-- 1 data_scientist users 2580392 Apr  8 02:12 ml-latest-small/original-ratings.csv\n",
      "-rw-r--r-- 1 data_scientist users 2580359 Apr  8 02:12 ml-latest-small/ratings.csv\n",
      "-rw-r--r-- 1 data_scientist users  199073 Jan 11 10:54 ml-latest-small/tags.csv\n",
      "\n",
      "***** Ratings File *****\n",
      "1,16,4.0,1217897793\r\n",
      "1,24,1.5,1217895807\r\n",
      "\n",
      "***** Movies File *****\n",
      "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\r\n",
      "2,Jumanji (1995),Adventure|Children|Fantasy\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# Copy original files to new name\n",
    "cp ml-latest-small/ratings.csv ml-latest-small/original-ratings.csv\n",
    "cp ml-latest-small/movies.csv ml-latest-small/original-movies.csv\n",
    "\n",
    "# GNU SED allows inline editing, here we delete the first line from the file\n",
    "sed -i '1d' ml-latest-small/ratings.csv\n",
    "sed -i '1d' ml-latest-small/movies.csv\n",
    "\n",
    "# List CSV files\n",
    "ls -la ml-latest-small/*.csv\n",
    "\n",
    "echo\n",
    "echo '***** Ratings File *****'\n",
    "head -2 ml-latest-small/ratings.csv\n",
    "\n",
    "echo\n",
    "echo '***** Movies File *****'\n",
    "head -2 ml-latest-small/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/ratings.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/ratings.pig\n",
    "\n",
    "ratings = LOAD 'ml-latest-small/ratings.csv' USING PigStorage(',')\n",
    "    AS (userID:int, mnovieID:int, rating:double, timestamp:int) ;\n",
    "DESCRIBE ratings ;\n",
    "ILLUSTRATE ratings ;\n",
    "top_rows = LIMIT ratings 10 ;\n",
    "DUMP top_rows ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings: {userID: int,mnovieID: int,rating: double,timestamp: int}\n",
      "(6,6711,4.0,1348881409)\n",
      "-----------------------------------------------------------------------------------\n",
      "| ratings     | userID:int   | mnovieID:int   | rating:double   | timestamp:int   | \n",
      "-----------------------------------------------------------------------------------\n",
      "|             | 6            | 6711           | 4.0             | 1348881409      | \n",
      "-----------------------------------------------------------------------------------\n",
      "\n",
      "(1,16,4.0,1217897793)\n",
      "(1,24,1.5,1217895807)\n",
      "(1,32,4.0,1217896246)\n",
      "(1,47,4.0,1217896556)\n",
      "(1,50,4.0,1217896523)\n",
      "(1,110,4.0,1217896150)\n",
      "(1,150,3.0,1217895940)\n",
      "(1,161,4.0,1217897864)\n",
      "(1,165,3.0,1217897135)\n",
      "(1,204,0.5,1217895786)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "pig -x local -f ratings.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/join.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/join.pig\n",
    "\n",
    "ratings = LOAD 'ml-latest-small/ratings.csv' USING PigStorage(',')\n",
    "    AS (userID:int, movieID:int, rating:double, timestamp:int) ;\n",
    "\n",
    "movies = LOAD 'ml-latest-small/movies.csv' USING PigStorage(',')\n",
    "    AS (movieID:int, title:chararray, genre:chararray) ;\n",
    "\n",
    "movie_ratings = JOIN ratings by movieID, movies by movieID ;\n",
    "\n",
    "DESCRIBE movie_ratings ;\n",
    "top_rows = LIMIT movie_ratings 10 ;\n",
    "DUMP top_rows ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_ratings: {ratings::userID: int,ratings::movieID: int,ratings::rating: double,ratings::timestamp: int,movies::movieID: int,movies::title: chararray,movies::genre: chararray}\n",
      "(151,1,5.0,864684243,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(176,1,4.0,965402628,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(215,1,3.5,1433873781,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(218,1,3.5,1255817134,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(347,1,5.0,1274980200,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(450,1,3.0,835226407,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(650,1,5.0,965433049,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(661,1,4.0,866409965,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(29,1,4.0,846942580,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(122,1,5.0,1024806364,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "pig -x local -b -f join.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/join-group.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/join-group.pig\n",
    "\n",
    "ratings = LOAD 'ml-latest-small/ratings.csv' USING PigStorage(',')\n",
    "    AS (userID:int, movieID:int, rating:double, timestamp:int) ;\n",
    "\n",
    "high_ratings = FILTER ratings BY rating > 3 ;\n",
    "\n",
    "hr_group = GROUP high_ratings BY movieID ;\n",
    "\n",
    "hr_count = FOREACH hr_group GENERATE group AS mvID, COUNT(high_ratings) AS cnt ;\n",
    "\n",
    "movies = LOAD 'ml-latest-small/movies.csv' USING PigStorage(',')\n",
    "    AS (movieID:int, title:chararray, genre:chararray) ;\n",
    "\n",
    "movie_ratings = JOIN hr_count by mvID, movies by movieID ;\n",
    "\n",
    "ordered_movies = ORDER movie_ratings BY cnt DESC ;\n",
    "\n",
    "top_movies = LIMIT ordered_movies 10 ;\n",
    "\n",
    "DESCRIBE top_movies ;\n",
    "\n",
    "DUMP top_movies ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_movies: {hr_count::mvID: int,hr_count::cnt: long,movies::movieID: int,movies::title: chararray,movies::genre: chararray}\n",
      "(318,282,318,\"Shawshank Redemption, The (1994)\")\n",
      "(296,268,296,Pulp Fiction (1994),Comedy|Crime|Drama|Thriller)\n",
      "(356,258,356,Forrest Gump (1994),Comedy|Drama|Romance|War)\n",
      "(593,253,593,\"Silence of the Lambs, The (1991)\")\n",
      "(2571,231,2571,\"Matrix, The (1999)\")\n",
      "(260,230,260,Star Wars: Episode IV - A New Hope (1977),Action|Adventure|Sci-Fi)\n",
      "(527,220,527,Schindler's List (1993),Drama|War)\n",
      "(110,204,110,Braveheart (1995),Action|Drama|War)\n",
      "(50,202,50,\"Usual Suspects, The (1995)\")\n",
      "(589,198,589,Terminator 2: Judgment Day (1991),Action|Sci-Fi)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "pig -x local -b -f join-group.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1556\n",
      "drwxr-xr-x  2 data_scientist users    4096 Apr  8 02:13 .\n",
      "drwxr-xr-x 18 data_scientist users    4096 Apr  7 21:26 ..\n",
      "-rw-r--r--  1 data_scientist users 1573151 Apr  7 18:17 book.txt\n",
      "-rwxr--r--  1 data_scientist users     694 Apr  7 20:07 mapper.py\n",
      "-rwxr--r--  1 data_scientist users    1481 Apr  7 20:07 reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Clean up the working directory (Don't run to save data for later analysis)\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# Remove pig log files\n",
    "rm -f pig*.log\n",
    "\n",
    "# Remove our pig scripts\n",
    "rm -f *.pig\n",
    "\n",
    "# Remove the movieLens Data\n",
    "rm -f ml-latest-small.zip\n",
    "rm -rf ml-latest-small\n",
    "\n",
    "# Remove our output file.\n",
    "rm -rf top_words\n",
    "\n",
    "# Display cleaned directory contents\n",
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced bayesian modeling. Now that you\n",
    "have run the Notebook, go back and run it a second time. Notice how the\n",
    "data and thus model fits have changed.\n",
    "\n",
    "1. Change the number of model points (by default there are 50 model\n",
    "points). How does increasing or decreasing the number of points affect\n",
    "the model accuracy?\n",
    "2. Try changing the model parameters, does the resulting fits replicate\n",
    "the true model?\n",
    "3. Compare the accuracy of the linear regression methods introduced\n",
    "earlier in the corse with the Bayesian approach. What are the benefits\n",
    "of the different techniques?\n",
    "4. Do the distribution we use to model our priors affect the fitting?\n",
    "Try changing the distributions and see what changes.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
