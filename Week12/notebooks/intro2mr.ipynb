{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Map/Reduce\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we \n",
    "\n",
    "-----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced general liner models by using\n",
    "pymc3. Now that you have run the Notebook, go back and make the\n",
    "following changes to see how the results change.\n",
    "\n",
    "1. Change the number of sample points both up and down, how does the glm\n",
    "fit change?\n",
    "2. Replace the existing model by a higher order model (include third and\n",
    "possibly higher order terms). How well does the corresponding GLM fit\n",
    "the data?\n",
    "3. One can use the Bayes factor to compare model fits (simply the ratio\n",
    "of the posteriors of the two models). Compute the Bayes factor for the\n",
    "linear and quadratic model fits to the original data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Mapper: Word Count\n",
    "\n",
    "The first Python code we will write is the map Python program. This\n",
    "program simply reads data from STDIN, tokenizes each line into words and\n",
    "outputs each word on a separate line along with a count of one. Thus our\n",
    "map program generates a list of word tokens as the keys and the value is\n",
    "always one.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/data_scientist/rppdm/hadoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/rppdm/hadoop/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/rppdm/hadoop/mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# We explicitly define the word/count separator token.\n",
    "sep = '\\t'\n",
    "\n",
    "# We open STDIN and STDOUT\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "    \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "        \n",
    "            # Strip off leading and trailing whitespace\n",
    "            line = line.strip()\n",
    "            \n",
    "            # We split the line into word tokens. Use whitespace to split.\n",
    "            # Note we don't deal with punctuation.\n",
    "            \n",
    "            words = line.split()\n",
    "            \n",
    "            # Now loop through all words in the line and output\n",
    "\n",
    "            for word in words:\n",
    "                fout.write(\"{0}{1}1\\n\".format(word, sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Reducer: Word Count\n",
    "\n",
    "The second Python program we write is our reduce program. In this code,\n",
    "we read key-value pairs from STDIN and use the fact that the Hadoop\n",
    "process first sorts all key-value pairs before sending the map output to\n",
    "the reduce process to accumulate the cumulative count of each word. The\n",
    "following code could easily be made more sophisticated by using `yield`\n",
    "statements and iterators, but for clarity we use the simple approach of\n",
    "tracking when the current word becomes different than the previous word\n",
    "to output the key-cumulative count pairs.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/rppdm/hadoop/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/rppdm/hadoop/reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# We explicitly define the word/count separator token.\n",
    "sep = '\\t'\n",
    "\n",
    "# We open STDIN and STDOUT\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "    \n",
    "        # Keep track of current word and count\n",
    "        cword = None\n",
    "        ccount = 0\n",
    "        word = None\n",
    "   \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "        \n",
    "            # Strip off leading and trailing whitespace\n",
    "            # Note by construction, we should have no leading white space\n",
    "            line = line.strip()\n",
    "            \n",
    "            # We split the line into a word and count, based on predefined\n",
    "            # separator token.\n",
    "            # Note we haven't dealt with punctuation.\n",
    "            \n",
    "            word, scount = line.split('\\t', 1)\n",
    "            \n",
    "            # We wil assume count is always an integer value\n",
    "            \n",
    "            count = int(scount)\n",
    "            \n",
    "            # word is either repeated or new\n",
    "            \n",
    "            if cword == word:\n",
    "                ccount += count\n",
    "            else:\n",
    "                # We have to handle first word explicitly\n",
    "                if cword != None:\n",
    "                    fout.write(\"{0:s}{1:s}{2:d}\\n\".format(cword, sep, ccount))\n",
    "                \n",
    "                # New word, so reset variables\n",
    "                cword = word\n",
    "                ccount = count\n",
    "        else:\n",
    "            # Output final word count\n",
    "            if cword == word:\n",
    "                fout.write(\"{0:s}{1:s}{2:d}\\n\".format(word, sep, ccount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Testing Python Map-Reduce\n",
    "\n",
    "Before we begin using Hadoop, we should first test our Python codes out\n",
    "to ensure they work as expected. First, we should change the permissions\n",
    "of the two programs to be executable, which we can do with the Unix\n",
    "`chmod` command.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1548\n",
      "drwxr-xr-x 1 data_scientist staff     170 Apr  5 23:56 .\n",
      "drwxr-xr-x 1 data_scientist staff    2482 Apr  5 23:46 ..\n",
      "-rw-r--r-- 1 data_scientist staff 1573151 Apr  5 23:46 book.txt\n",
      "-rwxr--r-- 1 data_scientist staff     694 Apr  5 23:55 mapper.py\n",
      "-rwxr--r-- 1 data_scientist staff    1481 Apr  5 23:56 reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "chmod u+x /home/data_scientist/rppdm/hadoop/mapper.py\n",
    "chmod u+x /home/data_scientist/rppdm/hadoop/reducer.py\n",
    "\n",
    "ls -la /home/data_scientist/rppdm/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Testing Mapper.py\n",
    "\n",
    "To test out the map Python code, we can run the Python `mapper.py` code\n",
    "and specify that the code should redirect STDIN to read the book text\n",
    "data. This is done in the following code cell, we pipe the output into\n",
    "the Unix `head` command in order to restrict the output, which would be\n",
    "one line per word found in the book text file. In the second code cell,\n",
    "we next pipe the output of  `mapper.py` into the Unix `sort` command,\n",
    "which is done automatically by Hadoop. To see the result of this\n",
    "operation, we next pipe the result into the Unix `uniq` command to count\n",
    "duplicates, pipe this result into a new sort routine to sort the output\n",
    "by the number of occurrences of a word, and finally display the last few\n",
    "lines with the Unix `tail` command to verify the program is operating\n",
    "correctly.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267976\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/rppdm/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2391 with\t1\n",
      "   2432 I\t1\n",
      "   2712 he\t1\n",
      "   3035 his\t1\n",
      "   4606 in\t1\n",
      "   4787 to\t1\n",
      "   5842 a\t1\n",
      "   6542 and\t1\n",
      "   8127 of\t1\n",
      "  13600 the\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/rppdm/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    " uniq -c -d | sort -n -k 1 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Testing Reducer.py\n",
    "\n",
    "To test out the reduce Python code, we run the previous code cell, but\n",
    "rather than piping the result into the Unix `tail` command, we pipe the\n",
    "result of the sort command into the Python `reducer.py` code. This\n",
    "simulates the Hadoop model, where the map output is key sorted before\n",
    "being passed into the reduce process. First, we will simply count the\n",
    "number of lines displayed by the reduce process, which will indicate the\n",
    "number of  unique _word tokens_ in the book. Next, we will sort the\n",
    "output by the number of times each word token appears and display the\n",
    "last few lines to compare with the previous results.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50106\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/rppdm/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    "./reducer.py | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\t2391\n",
      "I\t2432\n",
      "he\t2712\n",
      "his\t3035\n",
      "in\t4606\n",
      "to\t4787\n",
      "a\t5842\n",
      "and\t6542\n",
      "of\t8127\n",
      "the\t13600\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/rppdm/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    "./reducer.py | sort -n -k 2 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Map/Reduce\n",
    "\n",
    "At this point, we first need to change into the directory where we\n",
    "created our Python mapper and reducer programs, and where we downloaded\n",
    "the hadoop-streaming jar file and the sample book to analyze. In the\n",
    "Hadoop Docker container, enter `cd rppds/hadoop`, which will change our\n",
    "current working directory to the appropriate location, which is\n",
    "indicated by a change in the shell prompt to `/rppds/hadoop#`. \n",
    "\n",
    "Before proceeding, we should test our Python codes, but now within the\n",
    "Hadoop Docker container, which will have a different python environment\n",
    "than our class container. We can easily do this by modifying our earlier\n",
    "test to now use the correct path in the Hadoop Docker container:\n",
    "\n",
    "    /rppds/hadoop/mapper.py <  book.txt | sort -n -k 1 |  \\\n",
    "        /rppds/hadoop/reducer.py | sort -n -k 2 | tail -10\n",
    "\n",
    "Doing this, however, now gives an `UnicodeDecodeError`. The simplest\n",
    "solution is to explicitly state that the Python interpreter should use\n",
    "`utf-8` for all IO operations, which we can do by setting the Python\n",
    "environment variable `PYTHONIOENCODING` to `utf-8`. We do this by\n",
    "entering the following command at the container prompt:\n",
    "\n",
    "    export PYTHONIOENCODING=utf-8\n",
    "\n",
    "After setting this environment variable, the previous Unix command\n",
    "string will now produce the correct output.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Hadoop Streaming\n",
    "\n",
    "We are now ready tio actually run our Python codes via Hadoop Streaming.\n",
    "The main command to perform this task is `$HADOOP_PREFIX/bin/hadoop jar\n",
    "hs.jar`, where `hs.jar` is the hadoop-streaming jar file we downloaded\n",
    "earlier in this Notebook. Running this command will display a usage\n",
    "message that is not extremely useful, supplying the `-help` flag will\n",
    "provide more a more useful summary. For our map/reduce Python example to\n",
    "run successfully, we will need to specify six flags:\n",
    "\n",
    "1. `-files`: a comma separated list of files to be copied to the Hadoop cluster.\n",
    "2. `-input`: the HDFS input file(s) to be used for the map task.\n",
    "3. `-output`: the HDFS output directory, used for the reduce task.\n",
    "4. `-mapper`: the command to run for the map task.\n",
    "5. `-reducer`: the command to run for the reduce task.\n",
    "6. `-cmdenv`: set environment variables for a Hadoop streaming task.\n",
    "\n",
    "Given our previous setup, we will run the full command as follows:\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hadoop jar hs.jar -files mapper.py,reducer.py -input wc/in \\\n",
    "        -output wc/out -mapper mapper.py -reducer reducer.py -cmdenv PYTHONIOENCODING=utf-8\n",
    "\n",
    "When this command is run, a series of messages will be displayed to the\n",
    "screen (STDOUT) showing the progress of our Hadoop Streaming task. At\n",
    "the end of the stream of information messages will be a statement\n",
    "indicating the location of the output directory as shown below:\n",
    "\n",
    "![Hadoop Success](images/hadoop-success.png)\n",
    "\n",
    "In order to view the results of our Hadoop Streaming task, we must use\n",
    "HDFS DFS commands to examine the directory and files generated by our\n",
    "Python Map/Reduce programs. The following list of DFS commands might\n",
    "prove useful to view the results of this map/reduce job.\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hdfs dfs -ls wc\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hdfs dfs -ls wc/out\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hdfs dfs -count -h wc/out/part-00000\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hdfs dfs -tail wc/out/part-00000\n",
    "\n",
    "To compare this map/reduce Hadoop Streaming task output to our previous\n",
    "output, we can use the `$HADOOP_PREFIX/bin/hdfs dfs -cat\n",
    "wc/out/part-00000 | sort -n -k 2 | tail -10`, which should be executed at\n",
    "a Hadoop Docker container shell prompt. This code listing provides the\n",
    "succesful output of this command, following a succesful map/reduce\n",
    "processing task.\n",
    "\n",
    "```\n",
    "/rppds/hadoop# $HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000 | \\\n",
    "    sort -n -k 2 | tail -10\n",
    "\n",
    "with\t2391\n",
    "I\t2432\n",
    "he\t2712\n",
    "his\t3035\n",
    "in\t4606\n",
    "to\t4787\n",
    "a\t5842\n",
    "and\t6542\n",
    "of\t8127\n",
    "the\t13600\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Cleanup\n",
    "\n",
    "Following the succesful run of our map/reduce Python programs, we have\n",
    "created a new directory `wc/out`, which contains two files. If we wish\n",
    "to rerun this Hadoop Streaming map/reduce task, we must either specify a\n",
    "different output directory, or else we must clean up the results of the\n",
    "previous run. To remove the output directory, we can simply use the DFS\n",
    "`-rm -r -f -skipTrash wc/out` command, which will immediately delete the\n",
    "`wc/out` directory. The successful completion of this command is\n",
    "indicated by Hadoop, and this can also be verified by listing the\n",
    "contents of the `wc` directory as shown in the following screenshot.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in thread \"main\" java.io.FileNotFoundException: File mapper.py does not exist.\r\n",
      "\tat org.apache.hadoop.util.GenericOptionsParser.validateFiles(GenericOptionsParser.java:403)\r\n",
      "\tat org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:301)\r\n",
      "\tat org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:485)\r\n",
      "\tat org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:170)\r\n",
      "\tat org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:153)\r\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:64)\r\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\r\n",
      "\tat org.apache.hadoop.streaming.HadoopStreaming.main(HadoopStreaming.java:50)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\r\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -files mapper.py,reducer.py -input wc/in \\\n",
    "        -output wc/out -mapper mapper.py -reducer reducer.py -cmdenv PYTHONIOENCODING=utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 34600\r\n",
      "drwxr-xr-x 2 data_scientist hadoop     4096 Mar 29 09:29 .\r\n",
      "drwxr-xr-x 8 data_scientist hadoop     4096 Mar 29 09:29 ..\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    62983 Nov 13  2014 activation-1.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    44925 Nov 13  2014 apacheds-i18n-2.0.0-M15.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   691479 Nov 13  2014 apacheds-kerberos-codec-2.0.0-M15.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    16560 Nov 13  2014 api-asn1-api-1.0.0-M20.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    79912 Nov 13  2014 api-util-1.0.0-M20.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    43398 Nov 13  2014 asm-3.2.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   303139 Nov 13  2014 avro-1.7.4.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop 11948376 Nov 13  2014 aws-java-sdk-1.7.4.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   188671 Nov 13  2014 commons-beanutils-1.7.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   206035 Nov 13  2014 commons-beanutils-core-1.8.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    41123 Nov 13  2014 commons-cli-1.2.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    58160 Nov 13  2014 commons-codec-1.4.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   575389 Nov 13  2014 commons-collections-3.2.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   241367 Nov 13  2014 commons-compress-1.4.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   298829 Nov 13  2014 commons-configuration-1.6.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   143602 Nov 13  2014 commons-digester-1.8.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   112341 Nov 13  2014 commons-el-1.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   305001 Nov 13  2014 commons-httpclient-3.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   185140 Nov 13  2014 commons-io-2.4.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   284220 Nov 13  2014 commons-lang-2.6.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    62050 Nov 13  2014 commons-logging-1.1.3.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop  1599627 Nov 13  2014 commons-math3-3.1.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   273370 Nov 13  2014 commons-net-3.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    68866 Nov 13  2014 curator-client-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   185245 Nov 13  2014 curator-framework-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   248171 Nov 13  2014 curator-recipes-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   190432 Nov 13  2014 gson-2.2.4.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop  1648200 Nov 13  2014 guava-11.0.2.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop     9731 Nov 13  2014 hadoop-ant-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    21609 Nov 13  2014 hadoop-archives-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    67166 Nov 13  2014 hadoop-auth-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    86999 Nov 13  2014 hadoop-aws-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    14571 Nov 13  2014 hadoop-datajoin-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    94034 Nov 13  2014 hadoop-distcp-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    62122 Nov 13  2014 hadoop-extras-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   215398 Nov 13  2014 hadoop-gridmix-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   113680 Nov 13  2014 hadoop-openstack-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   277856 Nov 13  2014 hadoop-rumen-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   113776 Nov 13  2014 hadoop-sls-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   104977 Nov 13  2014 hadoop-streaming-2.6.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    45024 Nov 13  2014 hamcrest-core-1.3.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    31212 Nov 13  2014 htrace-core-3.0.4.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   433368 Nov 13  2014 httpclient-4.2.5.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   227708 Nov 13  2014 httpcore-4.2.5.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    33483 Nov 13  2014 jackson-annotations-2.2.3.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   192699 Nov 13  2014 jackson-core-2.2.3.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   232248 Nov 13  2014 jackson-core-asl-1.9.13.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   865838 Nov 13  2014 jackson-databind-2.2.3.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    18336 Nov 13  2014 jackson-jaxrs-1.9.13.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   780664 Nov 13  2014 jackson-mapper-asl-1.9.13.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    27084 Nov 13  2014 jackson-xc-1.9.13.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   408133 Nov 13  2014 jasper-compiler-5.5.23.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    76844 Nov 13  2014 jasper-runtime-5.5.23.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    18490 Nov 13  2014 java-xmlbuilder-0.4.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   105134 Nov 13  2014 jaxb-api-2.2.2.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   890168 Nov 13  2014 jaxb-impl-2.2.3-1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   458739 Nov 13  2014 jersey-core-1.9.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   147952 Nov 13  2014 jersey-json-1.9.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   713089 Nov 13  2014 jersey-server-1.9.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   539735 Nov 13  2014 jets3t-0.9.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    67758 Nov 13  2014 jettison-1.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   539912 Nov 13  2014 jetty-6.1.26.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   177131 Nov 13  2014 jetty-util-6.1.26.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   588001 Nov 13  2014 joda-time-2.5.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   185746 Nov 13  2014 jsch-0.1.42.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   100636 Nov 13  2014 jsp-api-2.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    33015 Nov 13  2014 jsr305-1.3.9.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   245039 Nov 13  2014 junit-4.11.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   489884 Nov 13  2014 log4j-1.2.17.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    85449 Nov 13  2014 metrics-core-3.0.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop  1419869 Nov 13  2014 mockito-all-1.8.5.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop  1199572 Nov 13  2014 netty-3.6.2.Final.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    29555 Nov 13  2014 paranamer-2.3.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   533455 Nov 13  2014 protobuf-java-2.5.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   105112 Nov 13  2014 servlet-api-2.5.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   995968 Nov 13  2014 snappy-java-1.0.4.1.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    23346 Nov 13  2014 stax-api-1.0-2.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    15010 Nov 13  2014 xmlenc-0.52.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop    94672 Nov 13  2014 xz-1.0.jar\r\n",
      "-rw-r--r-- 1 data_scientist hadoop   792964 Nov 13  2014 zookeeper-3.4.6.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la $HADOOP_PREFIX/share/hadoop/tools/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
