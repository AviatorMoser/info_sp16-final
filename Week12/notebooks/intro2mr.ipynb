{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Map/Reduce\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we \n",
    "\n",
    "-----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Mapper: Word Count\n",
    "\n",
    "The first Python code we will write is the map Python program. This\n",
    "program simply reads data from STDIN, tokenizes each line into words and\n",
    "outputs each word on a separate line along with a count of one. Thus our\n",
    "map program generates a list of word tokens as the keys and the value is\n",
    "always one.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/data_scientist/hadoop/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# We explicitly define the word/count separator token.\n",
    "sep = '\\t'\n",
    "\n",
    "# We open STDIN and STDOUT\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "    \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "        \n",
    "            # Strip off leading and trailing whitespace\n",
    "            line = line.strip()\n",
    "            \n",
    "            # We split the line into word tokens. Use whitespace to split.\n",
    "            # Note we don't deal with punctuation.\n",
    "            \n",
    "            words = line.split()\n",
    "            \n",
    "            # Now loop through all words in the line and output\n",
    "\n",
    "            for word in words:\n",
    "                fout.write(\"{0}{1}1\\n\".format(word, sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Reducer: Word Count\n",
    "\n",
    "The second Python program we write is our reduce program. In this code,\n",
    "we read key-value pairs from STDIN and use the fact that the Hadoop\n",
    "process first sorts all key-value pairs before sending the map output to\n",
    "the reduce process to accumulate the cumulative count of each word. The\n",
    "following code could easily be made more sophisticated by using `yield`\n",
    "statements and iterators, but for clarity we use the simple approach of\n",
    "tracking when the current word becomes different than the previous word\n",
    "to output the key-cumulative count pairs.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/data_scientist/hadoop/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# We explicitly define the word/count separator token.\n",
    "sep = '\\t'\n",
    "\n",
    "# We open STDIN and STDOUT\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "    \n",
    "        # Keep track of current word and count\n",
    "        cword = None\n",
    "        ccount = 0\n",
    "        word = None\n",
    "   \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "        \n",
    "            # Strip off leading and trailing whitespace\n",
    "            # Note by construction, we should have no leading white space\n",
    "            line = line.strip()\n",
    "            \n",
    "            # We split the line into a word and count, based on predefined\n",
    "            # separator token.\n",
    "            # Note we haven't dealt with punctuation.\n",
    "            \n",
    "            word, scount = line.split('\\t', 1)\n",
    "            \n",
    "            # We wil assume count is always an integer value\n",
    "            \n",
    "            count = int(scount)\n",
    "            \n",
    "            # word is either repeated or new\n",
    "            \n",
    "            if cword == word:\n",
    "                ccount += count\n",
    "            else:\n",
    "                # We have to handle first word explicitly\n",
    "                if cword != None:\n",
    "                    fout.write(\"{0:s}{1:s}{2:d}\\n\".format(cword, sep, ccount))\n",
    "                \n",
    "                # New word, so reset variables\n",
    "                cword = word\n",
    "                ccount = count\n",
    "        else:\n",
    "            # Output final word count\n",
    "            if cword == word:\n",
    "                fout.write(\"{0:s}{1:s}{2:d}\\n\".format(word, sep, ccount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Testing Python Map-Reduce\n",
    "\n",
    "Before we begin using Hadoop, we should first test our Python codes out\n",
    "to ensure they work as expected. First, we should change the permissions\n",
    "of the two programs to be executable, which we can do with the Unix\n",
    "`chmod` command.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1556\n",
      "drwxr-xr-x  2 data_scientist users    4096 Apr  7 20:03 .\n",
      "drwxr-xr-x 18 data_scientist users    4096 Apr  7 18:17 ..\n",
      "-rw-r--r--  1 data_scientist users 1573151 Apr  7 18:17 book.txt\n",
      "-rwxr--r--  1 data_scientist users     694 Apr  7 20:07 mapper.py\n",
      "-rwxr--r--  1 data_scientist users    1481 Apr  7 20:07 reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "chmod u+x /home/data_scientist/hadoop/mapper.py\n",
    "chmod u+x /home/data_scientist/hadoop/reducer.py\n",
    "\n",
    "ls -la /home/data_scientist/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Testing Mapper.py\n",
    "\n",
    "To test out the map Python code, we can run the Python `mapper.py` code\n",
    "and specify that the code should redirect STDIN to read the book text\n",
    "data. This is done in the following code cell, we pipe the output into\n",
    "the Unix `head` command in order to restrict the output, which would be\n",
    "one line per word found in the book text file. In the second code cell,\n",
    "we next pipe the output of  `mapper.py` into the Unix `sort` command,\n",
    "which is done automatically by Hadoop. To see the result of this\n",
    "operation, we next pipe the result into the Unix `uniq` command to count\n",
    "duplicates, pipe this result into a new sort routine to sort the output\n",
    "by the number of occurrences of a word, and finally display the last few\n",
    "lines with the Unix `tail` command to verify the program is operating\n",
    "correctly.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267976\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2391 with\t1\n",
      "   2432 I\t1\n",
      "   2712 he\t1\n",
      "   3035 his\t1\n",
      "   4606 in\t1\n",
      "   4787 to\t1\n",
      "   5842 a\t1\n",
      "   6542 and\t1\n",
      "   8127 of\t1\n",
      "  13600 the\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    " uniq -c -d | sort -n -k 1 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Testing Reducer.py\n",
    "\n",
    "To test out the reduce Python code, we run the previous code cell, but\n",
    "rather than piping the result into the Unix `tail` command, we pipe the\n",
    "result of the sort command into the Python `reducer.py` code. This\n",
    "simulates the Hadoop model, where the map output is key sorted before\n",
    "being passed into the reduce process. First, we will simply count the\n",
    "number of lines displayed by the reduce process, which will indicate the\n",
    "number of  unique _word tokens_ in the book. Next, we will sort the\n",
    "output by the number of times each word token appears and display the\n",
    "last few lines to compare with the previous results.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50106\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    "./reducer.py | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\t2391\n",
      "I\t2432\n",
      "he\t2712\n",
      "his\t3035\n",
      "in\t4606\n",
      "to\t4787\n",
      "a\t5842\n",
      "and\t6542\n",
      "of\t8127\n",
      "the\t13600\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    "./reducer.py | sort -n -k 2 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Map/Reduce\n",
    "\n",
    "At this point, we first need to change into the directory where we\n",
    "created our Python mapper and reducer programs, and where we downloaded\n",
    "the hadoop-streaming jar file and the sample book to analyze. In the\n",
    "Hadoop Docker container, enter `cd rppds/hadoop`, which will change our\n",
    "current working directory to the appropriate location, which is\n",
    "indicated by a change in the shell prompt to `/rppds/hadoop#`. \n",
    "\n",
    "Before proceeding, we should test our Python codes, but now within the\n",
    "Hadoop Docker container, which will have a different python environment\n",
    "than our class container. We can easily do this by modifying our earlier\n",
    "test to now use the correct path in the Hadoop Docker container:\n",
    "\n",
    "    /rppds/hadoop/mapper.py <  book.txt | sort -n -k 1 |  \\\n",
    "        /rppds/hadoop/reducer.py | sort -n -k 2 | tail -10\n",
    "\n",
    "Doing this, however, now gives an `UnicodeDecodeError`. The simplest\n",
    "solution is to explicitly state that the Python interpreter should use\n",
    "`utf-8` for all IO operations, which we can do by setting the Python\n",
    "environment variable `PYTHONIOENCODING` to `utf-8`. We do this by\n",
    "entering the following command at the container prompt:\n",
    "\n",
    "    export PYTHONIOENCODING=utf-8\n",
    "\n",
    "After setting this environment variable, the previous Unix command\n",
    "string will now produce the correct output.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Hadoop Streaming\n",
    "\n",
    "We are now ready tio actually run our Python codes via Hadoop Streaming.\n",
    "The main command to perform this task is `$HADOOP_PREFIX/bin/hadoop jar\n",
    "hs.jar`, where `hs.jar` is the hadoop-streaming jar file we downloaded\n",
    "earlier in this Notebook. Running this command will display a usage\n",
    "message that is not extremely useful, supplying the `-help` flag will\n",
    "provide more a more useful summary. For our map/reduce Python example to\n",
    "run successfully, we will need to specify six flags:\n",
    "\n",
    "1. `-files`: a comma separated list of files to be copied to the Hadoop cluster.\n",
    "2. `-input`: the HDFS input file(s) to be used for the map task.\n",
    "3. `-output`: the HDFS output directory, used for the reduce task.\n",
    "4. `-mapper`: the command to run for the map task.\n",
    "5. `-reducer`: the command to run for the reduce task.\n",
    "6. `-cmdenv`: set environment variables for a Hadoop streaming task.\n",
    "\n",
    "Given our previous setup, we will run the full command as follows:\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hadoop jar hs.jar -files mapper.py,reducer.py -input wc/in \\\n",
    "        -output wc/out -mapper mapper.py -reducer reducer.py -cmdenv PYTHONIOENCODING=utf-8\n",
    "\n",
    "When this command is run, a series of messages will be displayed to the\n",
    "screen (STDOUT) showing the progress of our Hadoop Streaming task. At\n",
    "the end of the stream of information messages will be a statement\n",
    "indicating the location of the output directory as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc/out\n",
      "packageJobJar: [/tmp/hadoop-unjar1976082841072322793/] [] /tmp/streamjob5549391610319238472.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/04/07 20:08:03 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "16/04/07 20:08:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/07 20:08:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/07 20:08:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/04/07 20:08:06 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/04/07 20:08:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1459971273803_0021\n",
      "16/04/07 20:08:07 INFO impl.YarnClientImpl: Submitted application application_1459971273803_0021\n",
      "16/04/07 20:08:07 INFO mapreduce.Job: The url to track the job: http://e7d89fb87de4:8088/proxy/application_1459971273803_0021/\n",
      "16/04/07 20:08:07 INFO mapreduce.Job: Running job: job_1459971273803_0021\n",
      "16/04/07 20:08:15 INFO mapreduce.Job: Job job_1459971273803_0021 running in uber mode : false\n",
      "16/04/07 20:08:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/07 20:08:25 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/04/07 20:08:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/04/07 20:08:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/07 20:08:33 INFO mapreduce.Job: Job job_1459971273803_0021 completed successfully\n",
      "16/04/07 20:08:33 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2601887\n",
      "\t\tFILE: Number of bytes written=5568567\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1577469\n",
      "\t\tHDFS: Number of bytes written=527718\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16395\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4571\n",
      "\t\tTotal time spent by all map tasks (ms)=16395\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4571\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16395\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4571\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16788480\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4680704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=33055\n",
      "\t\tMap output records=267976\n",
      "\t\tMap output bytes=2065929\n",
      "\t\tMap output materialized bytes=2601893\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=50105\n",
      "\t\tReduce shuffle bytes=2601893\n",
      "\t\tReduce input records=267976\n",
      "\t\tReduce output records=50105\n",
      "\t\tSpilled Records=535952\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=349\n",
      "\t\tCPU time spent (ms)=3410\n",
      "\t\tPhysical memory (bytes) snapshot=511086592\n",
      "\t\tVirtual memory (bytes) snapshot=5773438976\n",
      "\t\tTotal committed heap usage (bytes)=307437568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1577247\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=527718\n",
      "16/04/07 20:08:33 INFO streaming.StreamJob: Output directory: wc/out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Change into correct working directory\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# Delete output directory (if it exists)\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f wc/out\n",
    "\n",
    "# Grab current streaming lib jar filename\n",
    "streaming_file=$(ls $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming*)\n",
    "\n",
    "# Run the Map Reduce task within Hadoop\n",
    "$HADOOP_PREFIX/bin/hadoop jar $streaming_file \\\n",
    "    -files mapper.py,reducer.py -input wc/in \\\n",
    "    -output wc/out -mapper mapper.py -reducer reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Hadoop Results\n",
    "\n",
    "In order to view the results of our Hadoop Streaming task, we must use\n",
    "HDFS DFS commands to examine the directory and files generated by our\n",
    "Python Map/Reduce programs. The following list of DFS commands might\n",
    "prove useful to view the results of this map/reduce job.\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hdfs dfs -ls wc\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hdfs dfs -ls wc/out\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hdfs dfs -count -h wc/out/part-00000\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hdfs dfs -tail wc/out/part-00000\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2016-04-07 20:08 wc/out/_SUCCESS\n",
      "-rw-r--r--   1 data_scientist supergroup     527718 2016-04-07 20:08 wc/out/part-00000\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls wc/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0            1            515.3 K wc/out/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -count -h wc/out/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoo.\t1\r\n",
      "zoological\t1\r\n",
      "zouave's\t1\r\n",
      "zrads,\t2\r\n",
      "zrads.\t1\r\n",
      "É\t1\r\n",
      "Élus,_\t1\r\n",
      "à\t3\r\n",
      "è\t3\r\n",
      "état_.\t1\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -tail wc/out/part-00000 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To compare this map/reduce Hadoop Streaming task output to our previous\n",
    "output, we can use the `$HADOOP_PREFIX/bin/hdfs dfs -cat\n",
    "wc/out/part-00000 | sort -n -k 2 | tail -10`, which should be executed at\n",
    "a Hadoop Docker container shell prompt. This code listing provides the\n",
    "succesful output of this command, following a succesful map/reduce\n",
    "processing task.\n",
    "\n",
    "```\n",
    "/rppds/hadoop# $HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000 | \\\n",
    "    sort -n -k 2 | tail -10\n",
    "\n",
    "with\t2391\n",
    "I\t2432\n",
    "he\t2712\n",
    "his\t3035\n",
    "in\t4606\n",
    "to\t4787\n",
    "a\t5842\n",
    "and\t6542\n",
    "of\t8127\n",
    "the\t13600\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\t2391\r\n",
      "I\t2432\r\n",
      "he\t2712\r\n",
      "his\t3035\r\n",
      "in\t4606\r\n",
      "to\t4787\r\n",
      "a\t5842\r\n",
      "and\t6542\r\n",
      "of\t8127\r\n",
      "the\t13600\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000 | sort -n -k 2 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Cleanup\n",
    "\n",
    "Following the succesful run of our map/reduce Python programs, we have\n",
    "created a new directory `wc/out`, which contains two files. If we wish\n",
    "to rerun this Hadoop Streaming map/reduce task, we must either specify a\n",
    "different output directory, or else we must clean up the results of the\n",
    "previous run. To remove the output directory, we can simply use the DFS\n",
    "`-rm -r -f -skipTrash wc/out` command, which will immediately delete the\n",
    "`wc/out` directory. The successful completion of this command is\n",
    "indicated by Hadoop, and this can also be verified by listing the\n",
    "contents of the `wc` directory as shown in the following screenshot.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !$HADOOP_PREFIX/bin/hdfs dfs -r -f wc/out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced general liner models by using\n",
    "pymc3. Now that you have run the Notebook, go back and make the\n",
    "following changes to see how the results change.\n",
    "\n",
    "1. Change the number of sample points both up and down, how does the glm\n",
    "fit change?\n",
    "2. Replace the existing model by a higher order model (include third and\n",
    "possibly higher order terms). How well does the corresponding GLM fit\n",
    "the data?\n",
    "3. One can use the Bayes factor to compare model fits (simply the ratio\n",
    "of the posteriors of the two models). Compute the Bayes factor for the\n",
    "linear and quadratic model fits to the original data.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
