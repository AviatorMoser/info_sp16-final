{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to NLP: Basic Concepts\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore basic concepts in NLP:\n",
    "\n",
    "Tokenization\n",
    "Chunking\n",
    "POS\n",
    "NER\n",
    "\n",
    "Example application?\n",
    "\n",
    "NLTK and spacy.\n",
    "\n",
    "-----\n",
    "\n",
    "Need to nltk download:\n",
    "punkt\n",
    "averaged_perceptron_tagger\n",
    "maxent_ne_chunker\n",
    "words\n",
    "universal_tagset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 sentances in course description\n",
      "----------------------------------------\n",
      "Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.\n"
     ]
    }
   ],
   "source": [
    "# As a text example, we use the course description for INFO490  SP16.\n",
    "info_course = ['Advanced Data Science: This class is an asynchronous, online course.', \n",
    "               'This course will introduce advanced data science concepts by building on the foundational concepts presented in INFO 490: Foundations of Data Science.', \n",
    "               'Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.', \n",
    "               'Next, students will learn machine learning techniques including supervised and unsupervised learning, dimensional reduction, and cluster finding.', \n",
    "               'An emphasis will be placed on the practical application of these techniques to high-dimensional numerical data, time series data, image data, and text data.', \n",
    "               'Finally, students will learn to use relational databases and cloud computing software components such as Hadoop, Spark, and NoSQL data stores.', \n",
    "               'Students must have access to a fairly modern computer, ideally that supports hardware virtualization, on which they can install software.', \n",
    "               'This class is open to sophomores, juniors, seniors and graduate students in any discipline who have either taken a previous INFO 490 data science course or have received instructor permission.']\n",
    "\n",
    "text = \" \".join(info_course)\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "snts = sent_tokenize(text)\n",
    "print('{0} sentances in course description'.format(len(snts)))\n",
    "print(40*'-')\n",
    "print(snts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 words in course description\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science', ':', 'This', 'class', 'is', 'an',\n",
      "  'asynchronous', ',', 'online', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "wtks = word_tokenize(text)\n",
    "\n",
    "print('{0} words in course description'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "# Display the tokens\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "pp.pprint(wtks[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 words in course description (WS Tokenizer)\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science:', 'This', 'class', 'is', 'an', 'asynchronous,',\n",
      "  'online', 'course.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "wtks = tokenizer.tokenize(text)\n",
    "\n",
    "print('{0} words in course description (WS Tokenizer)'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "pp.pprint(wtks[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 words in course description (WP Tokenizer)\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science', ':', 'This', 'class', 'is', 'an',\n",
      "  'asynchronous', ',', 'online', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "wtks = tokenizer.tokenize(text)\n",
    "\n",
    "print('{0} words in course description (WP Tokenizer)'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "pp.pprint(wtks[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Collocations\n",
    "\n",
    "PMI = pointwise mutual information\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 bi-grams in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('An', 'emphasis'),\n",
      "  ('an', 'asynchronous'),\n",
      "  ('any', 'discipline'),\n",
      "  ('as', 'Hadoop'),\n",
      "  ('be', 'placed'),\n",
      "  ('by', 'building'),\n",
      "  ('can', 'install'),\n",
      "  ('cloud', 'computing'),\n",
      "  ('cluster', 'finding'),\n",
      "  ('components', 'such')]\n"
     ]
    }
   ],
   "source": [
    "top_bgs = 10\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wtks)\n",
    "bgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "\n",
    "print('Best {0} bi-grams in course description (WP Tokenizer)'.format(top_bgs))\n",
    "print(50*'-')\n",
    "\n",
    "ppf = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=False)\n",
    "ppf.pprint(bgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 tri-grams in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('any', 'discipline', 'who'),\n",
      "  ('components', 'such', 'as'),\n",
      "  ('fairly', 'modern', 'computer'),\n",
      "  ('ideally', 'that', 'supports'),\n",
      "  ('received', 'instructor', 'permission'),\n",
      "  ('such', 'as', 'Hadoop'),\n",
      "  ('supports', 'hardware', 'virtualization'),\n",
      "  ('that', 'supports', 'hardware'),\n",
      "  ('they', 'can', 'install'),\n",
      "  ('use', 'relational', 'databases')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(wtks)\n",
    "tgs = finder.nbest(trigram_measures.pmi, top_bgs)\n",
    "\n",
    "print('Best {0} tri-grams in course description (WP Tokenizer)'.format(top_bgs))\n",
    "print(50*'-')\n",
    "\n",
    "ppf = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=False)\n",
    "ppf.pprint(tgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Tagging\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('Advanced', 'INFO'), ('Data', 'INFO'), ('Science', 'INFO'), (':', 'INFO'),\n",
      "  ('This', 'INFO'), ('class', 'INFO'), ('is', 'INFO'), ('an', 'INFO'),\n",
      "  ('asynchronous', 'INFO'), (',', 'INFO'), ('online', 'INFO'),\n",
      "  ('course', 'INFO'), ('.', 'INFO')]\n"
     ]
    }
   ],
   "source": [
    "a_tag = 'INFO'\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "default_tagger = DefaultTagger(a_tag)\n",
    "tgs = default_tagger.tag(wtks)\n",
    "\n",
    "print('Tagged course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "pp.pprint(tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Part of Speech Tagging\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer/Univesal Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NOUN'),\n",
      "  ('Data', 'NOUN'),\n",
      "  ('Science', 'NOUN'),\n",
      "  (':', '.'),\n",
      "  ('This', 'DET'),\n",
      "  ('class', 'NOUN'),\n",
      "  ('is', 'VERB'),\n",
      "  ('an', 'DET'),\n",
      "  ('asynchronous', 'ADJ'),\n",
      "  (',', '.'),\n",
      "  ('online', 'ADJ'),\n",
      "  ('course', 'NOUN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "ptgs = pos_tag(wtks, tagset='universal')\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer/Univesal Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(ptgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "More detaile tagging possible, embed webpage?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer/Default Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'),\n",
      "  ('Data', 'NNP'),\n",
      "  ('Science', 'NN'),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'VBZ'),\n",
      "  ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'),\n",
      "  (',', ','),\n",
      "  ('online', 'JJ'),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "ptgs = pos_tag(wtks)\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer/Default Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(ptgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Named Entity Recognition\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "NER tagged course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ Tree('PERSON', [('Advanced', 'NNP')]),\n",
      "  Tree('ORGANIZATION', [('Data', 'NNP'), ('Science', 'NN')]),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'VBZ'),\n",
      "  ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'),\n",
      "  (',', ','),\n",
      "  ('online', 'JJ'),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.'),\n",
      "  ('This', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "nrcs = ne_chunk(pos_tag(wtks))\n",
    "\n",
    "print(50*'-')\n",
    "print('NER tagged course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "ppf.pprint(nrcs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Corpus\n",
    "\n",
    "- Penn Treebank\n",
    "- Brown\n",
    "- Wordnet\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank tagged text.\n",
      "--------------------------------------------------------------------------------\n",
      "Words:     [ 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n",
      "  'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "--------------------------------------------------------------------------------\n",
      "Setnences: [ 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n",
      "  'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "--------------------------------------------------------------------------------\n",
      "Tagged Words: \n",
      "[ ('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'),\n",
      "  ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'),\n",
      "  ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'),\n",
      "  ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'),\n",
      "  ('.', '.')]\n",
      "--------------------------------------------------------------------------------\n",
      "Tagged Sentances: \n",
      "[ ('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'),\n",
      "  ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'),\n",
      "  ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'),\n",
      "  ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'),\n",
      "  ('.', '.')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "print('Penn Treebank tagged text.')\n",
    "print(80*'-')\n",
    "\n",
    "print('Words:     ', end='')\n",
    "pp.pprint(treebank.words()[:18])\n",
    "print(80*'-')\n",
    "\n",
    "print('Setnences: ', end='')\n",
    "pp.pprint(treebank.sents()[0])\n",
    "print(80*'-')\n",
    "\n",
    "print('Tagged Words: ')\n",
    "pp.pprint(treebank.tagged_words()[:18])\n",
    "print(80*'-')\n",
    "\n",
    "print('Tagged Sentances: ')\n",
    "pp.pprint(treebank.tagged_sents()[0])\n",
    "print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "pt_tagger = UnigramTagger(treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'),\n",
      "  ('Data', 'NNP'),\n",
      "  ('Science', 'NN'),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'VBZ'),\n",
      "  ('an', 'DT'),\n",
      "  ('asynchronous', None),\n",
      "  (',', ','),\n",
      "  ('online', None),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pt_tgs = pt_tagger.tag(wtks)\n",
    "\n",
    "print('Penn Treebank tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(pt_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Brown corpus has over 1 million tagged words\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "b_tagger = UnigramTagger(brown.tagged_sents(brown.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'JJ-TL'),\n",
      "  ('Data', 'NNS-TL'),\n",
      "  ('Science', 'NN-TL'),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'BEZ'),\n",
      "  ('an', 'AT'),\n",
      "  ('asynchronous', None),\n",
      "  (',', ','),\n",
      "  ('online', None),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "b_tgs = b_tagger.tag(wtks)\n",
    "\n",
    "print('Brown tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(b_tgs[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown tagged course description (WP Tokenizer/Linked Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'JJ-TL'),\n",
      "  ('Data', 'NNS-TL'),\n",
      "  ('Science', 'NN-TL'),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'BEZ'),\n",
      "  ('an', 'AT'),\n",
      "  ('asynchronous', 'INFO'),\n",
      "  (',', ','),\n",
      "  ('online', 'INFO'),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# We can link taggers\n",
    "\n",
    "b_tagger._taggers = [b_tagger, default_tagger]\n",
    "\n",
    "b_tgs = b_tagger.tag(wtks)\n",
    "\n",
    "print('Brown tagged course description (WP Tokenizer/Linked Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(b_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Extracting Specififc terms\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'), ('Data', 'NNP'), ('Science', 'NN'), (':', ':'),\n",
      "  ('This', 'DT'), ('class', 'NN'), ('is', 'VBZ'), ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'), (',', ','), ('online', 'JJ'), ('course', 'NN'),\n",
      "  ('.', '.')]\n",
      "------------------------------------------------------------\n",
      "POS tagged course description (WP Tokenizer/RegEx applied)\n",
      "------------------------------------------------------------\n",
      "['Advanced', 'Data', 'Science', 'class', 'asynchronous', 'online', 'course']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# NN matchs NN|NNS|NNP|NNPS\n",
    "rgxs = re.compile(r\"(JJ|NN|VBN|VBG)\")\n",
    "\n",
    "ptgs = pos_tag(wtks)\n",
    "trms = [tkn[0] for tkn in ptgs if re.match(rgxs, tkn[1])]\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "pp.pprint(ptgs[:13])\n",
    "print(60*'-')\n",
    "print('POS tagged course description (WP Tokenizer/RegEx applied)')\n",
    "print(60*'-')\n",
    "pp.pprint(trms[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we . Now that you have run the Notebook, go back and make\n",
    "the following changes to see how the results change.\n",
    "\n",
    "1. Change\n",
    "2. Change \n",
    "3. Try \n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
